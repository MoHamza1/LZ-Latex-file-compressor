 \end \end \a \a \a \aa \acute \addcontentsline \contentsline \address \addtocontents \addtocounter \addtolength \nl \nl \setlength \newlength \settowidth \ae \aleph \alph \alpha \amalg \and \maketitle \angle \appendix \approx \arabic \arccos \arcsin \arctan \arg \arraycolsep \arrayrulewidth \hline \vline \arraystretch \ast \asymp \author \maketitle \b \backslash \bar \baselineskip \baselinestretch \baselineskip \end \item \nonumber \raggedright \raggedleft \item \item \unitlength \pushtabs \poptabs \kill \newtheorem \beta \bf \bibitem \bibliography \bibliographystyle \documentstyle \bigcap \bigcirc \bigcup \bigodot \bigoplus \bigotimes \bigtriangledown \bigtriangleup \bigskip \bigskipamount \bigskip \bigsqcup \biguplus \bigvee \bigwedge \bmod \boldmath \bot \bottomfraction \bowtie \breve \bullet \c \cal \cap \caption \cc \cdot \cdots \ldots \centering \chapter \chapter \chapter \check \chi \circ \circle \put \circle \circle \cite \bibitem \cleardoublepage \clearpage \cline \closing \clubsuit \columnsep \columnseprule \columnwidth \textwidth \cong \coprod \copyright \cos \cosh \cot \coth \csc \cup \d \dag \dagger \dashbox \dashv \date \maketitle \today \day \dblfloatpagefraction \dblfloatsep \dbltextfloatsep \dbltopfraction \ddag \ddagger \ddot \ddots \deg \delta \det \diamond \diamondsuit \dim \displaystyle \div \documentstyle \dot \doteq \dotfill \doublerulesep \downarrow \ell \em \emptyset \encl \end \begin \epsilon \equiv \eta \evensidemargin \exists \exp \fbox \fboxrule \fbox \framebox \fboxsep \fbox \framebox \fill \flat \floatpagefraction \floatsep \flushbottom \textheight \fnsymbol \footheight \footnote \footnotemark \footnotesep \footnotesize \footskip \footnotetext \footnotemark \forall \frac \frame \framebox \framebox \frown \fussy \sloppy \gamma \gcd \ge \geq \gets \gg \glossary \glossaryentry \glossaryentry \glossary \grave \hat \hbar \headheight \headsep \heartsuit \hfill \hspace \fill \fill \hline \hom \hookleftarrow \hookrightarrow \hrulefill \hspace \hspace \hspace \huge \hyphenation \i \iff \imath \in \include \includeonly \include \index \indexentry \indexentry \index \indexspace \inf \infty \input \int \intextsep \iota \it \item \itemindent \itemsep \j \jmath \kappa \ker \kill \tabbing \l \label \ref \pageref \labelwidth \labelsep \lambda \land \langle \large \normalsize \lbrace \lbrack \lceil \ldots \cdots \le \leadsto \left \right \leftarrow \lefteqn \leftharpoondown \leftharpoonup \leftmargin \leftmargini \leftmarginvi \leftrightarrow \leq \lfloor \lg \lhd \lim \liminf \limsup \line \put \put \linebreak \linethickness \linewidth \listoffigures \listoftables \listparindent \ll \ln \lnot \log \longleftarrow \longleftrightarrow \longmapsto \longrightarrow \lor \lq \makebox \makebox \makeglossary \glossaryentry \makeindex \indexentry \maketitle \title \author \date \mapsto \marginpar \marginparpush \marginparsep \marginparwidth \markboth \markright \max \mbox \medskip \medskipamount \medskip \mho \mid \min \mit \models \month \mp \mu \multicolumn \multiput \put \put \put \nabla \natural \ne \nearrow \neg \neq \newcommand \cs \cs \newcounter \newenvironment \newfont \cs \newlength \nl \nl \setlength \addtolength \settowidth \newline \linebreak \newpage \clearpage \newsavebox \binname \savebox \newtheorem \ni \nofiles \noindent \nolinebreak \linebreak \nonumber \nopagebreak \linebreak \normalmarginpar \reversemarginpar \normalsize \not \not \notin \nu \nwarrow \o \obeycr \oddsidemargin \odot \oe \oint \omega \ominus \onecolumn \twocolumn \opening \oplus \oslash \otimes \oval \put \overbrace \overline \owns \pagebreak \linebreak \pagenumbering \pageref \label \pagestyle \paragraph \paragraph \parallel \parbox \parindent \parsep \parskip \part \part \partial \partopsep \perp \phi \pi \pm \pmod \poptabs \pushtabs \pounds \prec \preceq \prime \prod \propto \protect \caption \ps \closing \psi \pushtabs \poptabs \put \mbox \raggedbottom \raggedleft \raggedright \raisebox \rangle \rbrace \rbrack \rceil \ref \label \renewcommand \cs \cs \renewenvironment \newenvironment \restorecr \obeycr \reversemarginpar \rfloor \rhd \rho \right \left \rightarrow \rightharpoondown \rightharpoonup \rightleftharpoons \rightmargin \rm \roman \rq \rule \savebox \binname \makebox \binname \usebox \binname \sbox \binname \binname \savebox \sc \scriptsize \scriptstyle \scriptscriptstyle \searrow \sec \section \section \setcounter \setlength \nl \nl \addtolength \newlength \settowidth \setminus \settowidth \nl \nl \setlength \newlength \addtolength \sf \sharp \shortstack \yy \zzz \sigma \signature \sim \simeq \sin \sinh \sl \sloppy \fussy \small \smallint \smallskip \smallskipamount \smallskip \smile \spadesuit \sqcap \sqcup \sqrt \sqsubset \sqsubseteq \sqsupset \sqsupseteq \ss \stackrel \stackrel \longrightarrow \star \stop \subparagraph \subparagraph \subsection \subsubsection \subsection \subsubsection \subset \subseteq \succ \succeq \sum \sup \supset \supseteq \surd \swarrow \symbol \t \tabbingsep \tabcolsep \tableofcontents \tan \tanh \tau \textfloatsep \textfraction \textheight \textstyle \textwidth \thanks \maketitle \theta \thicklines \thinlines \thicklines \thinspace \thispagestyle \pagestyle \tilde \times \tiny \title \maketitle \to \today \top \topfraction \topmargin \topsep \topskip \triangle \triangleleft \triangleright \tt \twocolumn \typein \cs \cs \typeout \u \unboldmath \underbrace \underline \unitlength \unlhd \unrhd \uparrow \updownarrow \uplus \upsilon \usebox \binname \binname \usecounter \v \value \varepsilon \varphi \varpi \varrho \varsigma \vartheta \vdash \vdots \vec \vector \put \put \vee \verb \verb \verb \vert \vfill \vspace \fill \fill \vspace \vspace \vspace \wedge \widehat \widetilde \wp \wr \xi \year \zeta \rm \it \bf \sl \sf \sc \tt \t \c \u \d \v \b \oe \aa \l \ae \o \ss \dag \copyright \ddag \pounds \hat \dot \check \ddot \tilde \breve \acute \bar \grave \vec \alpha \nu \beta \xi \gamma \delta \pi \epsilon \rho \zeta \sigma \eta \tau \theta \upsilon \iota \phi \kappa \chi \lambda \psi \mu \omega \varepsilon \varsigma \vartheta \varphi \varrho \pm \cap \mp \cup \setminus \uplus \cdot \sqcap \times \sqcup \ast \triangleleft \star \triangleright \diamond \wr \circ \bigcirc \bullet \bigtriangleup \div \bigtriangledown \lhd \rhd \vee \odot \wedge \dagger \oplus \ddagger \ominus \amalg \otimes \unlhd \oslash \unrhd \leq \geq \prec \succ \preceq \succeq \ll \gg \subset \supset \subseteq \supseteq \sqsubset \sqsupset \sqsubseteq \sqsupseteq \in \ni \vdash \dashv \smile \mid \frown \parallel \neq \perp \equiv \cong \sim \bowtie \simeq \propto \asymp \models \approx \doteq \sum \bigcap \prod \bigcup \coprod \bigsqcup \int \bigvee \oint \bigwedge \bigodot \bigotimes \bigoplus \biguplus \lfloor \rfloor \lceil \rceil \langle \rangle \backslash \vert \uparrow \downarrow \updownarrow \arccos \csc \ker \min \arcsin \deg \lg \arctan \det \lim \sec \arg \dim \liminf \sin \cos \exp \limsup \sinh \cosh \gcd \ln \sup \cot \hom \log \tan \coth \inf \max \tanh \leftarrow \longleftarrow \rightarrow \longrightarrow \leftrightarrow \longleftrightarrow \mapsto \longmapsto \hookleftarrow \hookrightarrow \leftharpoonup \rightharpoonup \leftharpoondown \rightharpoondown \rightleftharpoons \leadsto \uparrow \nearrow \downarrow \searrow \swarrow \updownarrow \nwarrow \aleph \prime \hbar \emptyset \imath \nabla \jmath \surd \ell \top \wp \bot \angle \partial \triangle \infty \backslash \forall \sharp \exists \clubsuit \neg \diamondsuit \flat \heartsuit \natural \spadesuit \mho


% \section{Lattice-based cryptography II}
% \label{sec:20}

% This lecture will be designed and delivered by Karl Southern.


\section{Lattice-based cryptography II}
\label{sec:20}

\subsection{Lattices}
The \Define{basis} of a lattice is a set of $n$ linearly independent vectors in $\mathbb{R}^d$, and is denoted $\textbf{B} = \begin{pmatrix}\textbf{b}_1,...,\textbf{b}_n\end{pmatrix} \in \mathbb{R}^{d\times n}$.
\\
A \Define{lattice} is set of points generated by taking all integer linear combinations of a basis. The lattice generated by the basis $\textbf{B}$ is: $$\mathcal{L}(\textbf{B}) = \left\{\sum_{i = 1}^{n}x_{i}\cdot\textbf{b}_{i}:\forall x_{i}\in\mathbb{Z}\right\}.$$
\\
A lattice is said to be \Define{full rank} if $d = n$. We will only be looking at full rank lattices, and so can treat the basis $\textbf{B}$ as being a square matrix of dimension $n\times n$. Treating the basis as a matrix, we can also write the definition of a lattice as $$\mathcal{L}(\textbf{B}) = \left\{ \textbf{B}\cdot\textbf{x}:\forall\textbf{x}\in\mathbb{Z}^{n}\right\}.$$
\\
Some examples:
\\
The basis $\textbf{B} = \begin{pmatrix}1 & 0 \\ 0 & 1 \end{pmatrix}$ gives the integer lattice $\mathbb{Z}^2$, since every vector can be created by some integer combination of $\textbf{B}$, a visualisation of this can be seen in Figure~\ref{fg:zn}.
\\
A visualisation of the lattice generated from the basis $\textbf{B} = \begin{pmatrix}1 & 3 \\ 2 & 1\end{pmatrix}$ is given in Figure~\ref{fg:b1-b2}.
\\
\begin{figure}[ht]
		\begin{minipage}{.5\textwidth}
		\centering
		\framebox{
			\resizebox{.9\columnwidth}{.9\columnwidth}{%
			\begin{tikzpicture}
				\coordinate (Origin)  at (0,0);
				\node  at (0,0)  [below]{$O$};

				\clip (-3.25,-2.25) rectangle (8.2cm,8.2cm); % Clips the picture...
				\coordinate (Bone) at (0,1);
				\coordinate (Btwo) at (1,0);
				\foreach \x in {-10,-9,...,10}{% Two indices running over each
					\foreach \y in {-10,-9,...,10}{% node on the grid we have drawn
						\node[draw,circle,inner sep=2pt,fill] at (\x,\y) {};
						\node[draw,circle,inner sep=1pt] at (\x,\y) {};
						% Places a dot at those points
					}
				}
				\draw [ultra thick,-latex,red] (Origin)
				-- (Bone) node [above left] {$\textbf{b}_2$};
				\draw [ultra thick,-latex,red] (Origin)
				-- (Btwo) node [below right] {$\textbf{b}_1$};


		\end{tikzpicture}}}
		\caption[A lattice]{The lattice formed from $\textbf{B} = \begin{pmatrix}1 & 0 \\ 0 & 1 \end{pmatrix}$} %$\textbf{B} = \begin{pmatrix}1 & 0 \\ 0 & 1 \end{pmatrix}$
		\label{fg:zn}
	\end{minipage}
	\begin{minipage}{.5\textwidth}
	\centering
	\framebox{
		\resizebox{.9\columnwidth}{.9\columnwidth}{%
	\begin{tikzpicture}
		\coordinate (Origin)  at (0,0);
		\node  at (0,0)  [below]{$O$};

		\clip (-3.25,-2.25) rectangle (8.2cm,8.2cm); % Clips the picture...
		\coordinate (Bone) at (1,2);
		\coordinate (Btwo) at (3,1);
		\foreach \x in {-10,-9,...,10}{% Two indices running over each
			\foreach \y in {-10,-9,...,10}{% node on the grid we have drawn
				\node[draw,circle,inner sep=2pt,fill] at (\x+3*\y,2*\x+\y) {};
				\node[draw,circle,inner sep=1pt] at (\x,\y) {};
				% Places a dot at those points
			}
		}
		\draw [ultra thick,-latex,red] (Origin)
		-- (Bone) node [above left] {$\textbf{b}_1$};
		\draw [ultra thick,-latex,red] (Origin)
		-- (Btwo) node [below right] {$\textbf{b}_2$};


	\end{tikzpicture}}}
	\caption[A lattice]{The lattice formed from $\textbf{B} = \begin{pmatrix}1 & 3 \\ 2 & 1\end{pmatrix}$} %$
	\label{fg:b1-b2}
	\end{minipage}
\end{figure}

\paragraph{Fundamental parallelepiped} The \Define{fundamental parallelepiped} is the area bounded by all possible 0,1 combinations of the basis vectors, and is defined as the set of points $$\mathcal{P}(\textbf{B}) = \textbf{B}\cdot[0,1)^{n} = \left\{ \sum_{i=0}^{n} x_{i}\cdot\textbf{b}_{i}:\forall 0 \le x_{i} < 1 \right\}.$$
In Figure~\ref{fg:pb} you can see $\mathcal{P}(\textbf{B})$ highlighted.
If you think of the lattice as a wall, you can consider the fundamental parallelepiped as being a brick. The entire lattice is made up of an infinite amount of shifted copies of the fundamental parallelepiped, also known as a tiling. A visualisation of this can be seen in Figure~\ref{fg:tile}.
\\
The volume of a lattice, denoted $\mathtt{det}(\mathcal{L})$ is the defined as the volume of the fundamental parallelepiped, $\text{vol}(\mathcal{P}(\textbf{B}))$.
The volume of the fundamental parallelepiped is equal to the determinant of the basis:
$$\text{det}(\mathcal{L}(\textbf{B})) = \text{vol}(\mathcal{P}(\textbf{B})) = |\text{det}(\textbf{B})|. $$

\begin{figure}[ht]
	\begin{minipage}{.5\textwidth}
		\centering
		\framebox{
			\resizebox{.9\columnwidth}{.9\columnwidth}{%
				\begin{tikzpicture}
					\coordinate (Origin)  at (0,0);
					\node  at (0,0)  [below]{$O$};

					\clip (-3.25,-2.25) rectangle (8.2cm,8.2cm); % Clips the picture...
					\coordinate (Bone) at (1,2);
					\coordinate (Btwo) at (3,1);
					\coordinate (Bonetwo) at (4,3);
					\foreach \x in {-10,-9,...,10}{% Two indices running over each
						\foreach \y in {-10,-9,...,10}{% node on the grid we have drawn
							\node[draw,circle,inner sep=2pt,fill] at (\x+3*\y,2*\x+\y) {};
							\node[draw,circle,inner sep=1pt] at (\x,\y) {};
							% Places a dot at those points
						}
					}
					\draw [ultra thick,-latex,red] (Origin)
					-- (Bone) node [above left] {$\textbf{b}_1$};
					\draw [ultra thick,-latex,red] (Origin)
					-- (Btwo) node [below right] {$\textbf{b}_2$};
					\draw [dotted,red] (Bone)
					-- (Bonetwo) node {};
					\draw [dotted,red] (Btwo)
					-- (Bonetwo) node {};
					\fill[gray,opacity=0.2] (Origin) -- (Bone) -- (Bonetwo) -- (Btwo) -- (Origin);

					\node at (2,1.5) {$\mathcal{P}(\textbf{B})$};

		\end{tikzpicture}}}
		\caption{The fundamental parallelepiped $\mathcal{P}(\textbf{B})$}
		\label{fg:pb}
	\end{minipage}
	\begin{minipage}{.5\textwidth}
	\centering
	\framebox{
		\resizebox{.9\columnwidth}{.9\columnwidth}{%
			\begin{tikzpicture}

			 \clip (-3.25,-2.25) rectangle (8.2cm,8.2cm); % Clips the picture...
				%\fill[gray,opacity=0.2] (Origin) -- (Bone) -- (Bonetwo) -- (Btwo) -- (Origin);
				\foreach \x in {-10,-9,...,10}{% Two indices running over each
					\foreach \y in {-10,-9,...,10}{% node on the grid we have drawn
						%\vara = \x+3*\y
						%\varb = 2*\x+\y
						\fill[gray,opacity=0.2] (0+\x+3*\y,0+2*\x+\y) -- (1 + \x+3*\y,2+2*\x+\y) -- (4 +\x+3*\y,3+2*\x+\y) -- (3 +\x+3*\y,1+2*\x+\y) -- (0+\x+3*\y,0+2*\x+\y);
					}
				}
			  \foreach \x in {-10,-9,...,10}{% Two indices running over each
					  	\foreach \y in {-10,-9,...,10}{% node on the grid we have drawn
					  		%\vara = \x+3*\y
					  		%\varb = 2*\x+\y
					  		\draw[ultra thick,white] (0+\x+3*\y,0+2*\x+\y) -- (1 + \x+3*\y,2+2*\x+\y) -- (4 +\x+3*\y,3+2*\x+\y) -- (3 +\x+3*\y,1+2*\x+\y) -- (0+\x+3*\y,0+2*\x+\y);
					  	}
			  	}
		  				\coordinate (Origin)  at (0,0);
		  \node  at (0,0)  [below]{$O$};


		  \coordinate (Bone) at (1,2);
		  \coordinate (Btwo) at (3,1);
		  \coordinate (Bonetwo) at (4,3);
		  \foreach \x in {-10,-9,...,10}{% Two indices running over each
		  	\foreach \y in {-10,-9,...,10}{% node on the grid we have drawn
		  		\node[draw,circle,inner sep=2pt,fill] at (\x+3*\y,2*\x+\y) {};
		  		\node[draw,circle,inner sep=1pt] at (\x,\y) {};
		  		% Places a dot at those points
		  	}
		  }
		  \draw [ultra thick,-latex,red] (Origin)
		  -- (Bone) node [above left] {$\textbf{b}_1$};
		  \draw [ultra thick,-latex,red] (Origin)
		  -- (Btwo) node [below right] {$\textbf{b}_2$};
		  \draw [dotted,red] (Bone)
		  -- (Bonetwo) node {};
		  \draw [dotted,red] (Btwo)
		  -- (Bonetwo) node {};


	\end{tikzpicture}}}
	\caption{Tiling the lattice with $\mathcal{P}(\textbf{B})$}
	\label{fg:tile}
\end{minipage}
\end{figure}

\paragraph{Minimum distance} The \Define{minimum distance} ($\lambda_1$) on a lattice is the smallest distance between any two lattice points: $$\lambda_1 = \text{min}\{||\textbf{x} - \textbf{y}|| \forall \textbf{x},\textbf{y}\in\mathcal{L}, \textbf{x}\neq\textbf{y}\}.$$
The minimum distance of the lattice is also the length of the shortest vector in the lattice.
\\
Minkowski's theorem states that for any lattice,
\[
    \lambda_1(\mathcal{L}) \leq \sqrt{n}\cdot\text{det}(\mathcal{L})^{1/n}.
\]
This gives us an upper bound for the length of the shortest vector.
\subsection{Shortest Vector Problem}
\Define{Shortest Vector Problem}.
\\
Instance: A lattice $\mathcal{L}(\textbf{B})$.
\\
Question: What is the shortest non-zero vector in the lattice? (i.e. find $\textbf{v} \in \mathcal{L}$ s.t. $||\textbf{v}|| = \lambda_1(\mathcal{L})$).
\\
~\\
There also exists an approximate version of this problem, where we are asked to find a vector that is at most $\gamma$ times the size of the longest vector. Where $\gamma = \gamma(n) \geq 1$.
\\
\Define{Approximate-Shortest Vector Problem}.
\\
Instance: A lattice $\mathcal{L}(\textbf{B})$.
\\
Question: Find a non-zero vector in the lattice that is at most $\gamma$ times longer than the shortest vector. (i.e. find $\textbf{v} \in \mathcal{L}$ s.t. $||\textbf{v}|| \leq \gamma(n)\cdot\lambda_1(\mathcal{L})$).
\\
~\\
SVP has been shown to be NP-Hard, for approximate-SVP the hardness depends on the approximation factor ($\gamma$). For $\gamma = O(1)$, approximate-SVP is "hard", for $\gamma = O(\sqrt{n})$ it's in NP $\cap$ co-NP, for $\gamma = 2^{\tilde{O}(n)}$ it's in P. For crypto we tend to rely on the hardness of approximate-SVP for $\gamma$ = poly($n$).
\\
There are a number of approaches for solving SVP, we'll discuss two of them.
\paragraph{Enumeration} The `brute force' method of solving SVP is enumeration, given a lattice we enumerate through every lattice point in some bounded region, typically the fundamental parallelepiped. Whilst very slow, this method is still often used as it has very low memory requirements.
\paragraph{Sieving} Sieving algorithms are randomised algorithms that, in some cases, run very efficiently but use an exponential amount of space. They work by generating some random lattice points and then taking the difference between these points (Figure~\ref{fg:sieve1}). The difference between any two lattice points is also a valid lattice point, and so this is then added to the list (Figure~\ref{fg:sieve2}). Hopefully as this is repeated a short vector is found.

\begin{figure}[ht]
	\begin{minipage}{.5\textwidth}
		\centering
		\framebox{
			\resizebox{.9\columnwidth}{.9\columnwidth}{%
				\begin{tikzpicture}
					\coordinate (Origin)  at (0,0);
					\node  at (0,0)  [below]{\Huge O};

					%\clip (-30,-30) rectangle (40,40); % Clips the picture...
					\coordinate (Bone) at (1,2);
					\coordinate (Btwo) at (3,1);
					\coordinate (Bonetwo) at (4,3);
					\foreach \x in {-30,-29,...,30}{% Two indices running over each
						\foreach \y in {-30,-29,...,30}{% node on the grid we have drawn
							\pgfmathparse{\x+3*\y}
							\ifnum -20 < \pgfmathresult
								\pgfmathparse{\x+3*\y}
								\ifnum  20 > \pgfmathresult
								\pgfmathparse{2*\x+\y}
								\ifnum -20 < \pgfmathresult
								\pgfmathparse{2*\x+\y}
								\ifnum 20 > \pgfmathresult
							%\ifthenelse{-5 < (\x+3*\y) \AND (\x+3*\y< 5) \AND -5 < 2*\x+\y \AND 2*\x+\y<5}{
									\node[draw,circle,inner sep=2pt,fill,black] at (\x+3*\y,2*\x+\y) {};
								\fi
							\fi
							\fi
							\fi
							%\node[draw,circle,inner sep=1pt] at (\x,\y) {};
							% Places a dot at those points
						}
					}
					\coordinate (r1) at (9,-2);
					\coordinate (r2) at (-18,-6);
					\coordinate (r3) at (7,4);
					\coordinate (r4) at (6,12);
					\coordinate (r5) at (-6,-2);
					\coordinate (r6) at (10,10);
					\coordinate (r7) at (-6,-12);
					\coordinate (r8) at (-6,8);
					\coordinate (r9) at (-16,-12);
					\coordinate (r10) at (12,-6);

					\node[draw,circle,inner sep=5pt,fill,red] at (Origin) {};

					\node[draw,circle,inner sep=5pt,fill,red] at (r1) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r2) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r3) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r4) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r5) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r6) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r7) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r8) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r9) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r10) {};

				\draw [ultra thick,-latex,black] (r4)
				-- (r6);

				\draw [ultra thick,-latex,black] (r1)
				-- (r10);

				\draw [ultra thick,-latex,black] (r2)
				-- (r9);

	\end{tikzpicture}}}
		\caption{Choosing some random points and finding the differences.}
		\label{fg:sieve1}
	\end{minipage}
	\begin{minipage}{.5\textwidth}
		\centering
		\framebox{
			\resizebox{.9\columnwidth}{.9\columnwidth}{%
				\begin{tikzpicture}
					\coordinate (Origin)  at (0,0);
					\node  at (0,0)  [below]{\Huge O};

					%\clip (-30,-30) rectangle (40,40); % Clips the picture...
					\coordinate (Bone) at (1,2);
					\coordinate (Btwo) at (3,1);
					\coordinate (Bonetwo) at (4,3);
					\foreach \x in {-30,-29,...,30}{% Two indices running over each
						\foreach \y in {-30,-29,...,30}{% node on the grid we have drawn
							\pgfmathparse{\x+3*\y}
							\ifnum -20 < \pgfmathresult
							\pgfmathparse{\x+3*\y}
							\ifnum  20 > \pgfmathresult
							\pgfmathparse{2*\x+\y}
							\ifnum -20 < \pgfmathresult
							\pgfmathparse{2*\x+\y}
							\ifnum 20 > \pgfmathresult
							%\ifthenelse{-5 < (\x+3*\y) \AND (\x+3*\y< 5) \AND -5 < 2*\x+\y \AND 2*\x+\y<5}{
							\node[draw,circle,inner sep=2pt,fill,black] at (\x+3*\y,2*\x+\y) {};
							\fi
							\fi
							\fi
							\fi
							%\node[draw,circle,inner sep=1pt] at (\x,\y) {};
							% Places a dot at those points
						}
					}
					\coordinate (r1) at (9,-2);
					\coordinate (r2) at (-18,-6);
					\coordinate (r3) at (7,4);
					\coordinate (r4) at (6,12);
					\coordinate (r5) at (-6,-2);
					\coordinate (r6) at (10,10);
					\coordinate (r7) at (-6,-12);
					\coordinate (r8) at (-6,8);
					\coordinate (r9) at (-16,-12);
					\coordinate (r10) at (12,-6);

					\node[draw,circle,inner sep=5pt,fill,red] at (Origin) {};

					\node[draw,circle,inner sep=5pt,fill,red] at (r1) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r2) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r3) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r4) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r5) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r6) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r7) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r8) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r9) {};
					\node[draw,circle,inner sep=5pt,fill,red] at (r10) {};

					\draw [ultra thick,-latex,black] (r4)
					-- (r6);

					\draw [ultra thick,-latex,black] (r1)
					-- (r10);

					\draw [ultra thick,-latex,black] (r2)
					-- (r9);

					\coordinate (n1) at (4,-2);
					\coordinate (n2) at (3,-4);
					\coordinate (n3) at (-2,6);

					\node[draw,circle,inner sep=5pt,fill,blue] at (n1) {};
					\node[draw,circle,inner sep=5pt,fill,blue] at (n2) {};
					\node[draw,circle,inner sep=5pt,fill,blue] at (n3) {};

					\draw [ultra thick,-latex,black] (n1)
					-- (Origin);

					\draw [ultra thick,-latex,black] (n2)
					-- (Origin);

					\draw [ultra thick,-latex,black] (n3)
					-- (Origin);

		\end{tikzpicture}}}
		\caption{Adding the new vectors to the list, ready to repeat.}
		\label{fg:sieve2}
	\end{minipage}
\end{figure}

\subsection{Breaking LWE}
The main method for breaking LWE involves turning it into a SVP instance. Given an LWE instance $(\textbf{A},\textbf{b}),~ \textbf{b} = \textbf{A}\cdot\textbf{s} + \textbf{e}$, we can construct an SVP instance by creating a lattice basis $$\textbf{B} = \begin{pmatrix}
	\textbf{A}^{\top} & 0 \\
	\textbf{b}^{\top} & 1
\end{pmatrix} \in \mathbb{Z}^{(n+1)\times(m+1)}_{q}.$$ Every point on this lattice can be defined by $\begin{pmatrix}\textbf{x}^{\top}|y\end{pmatrix}\cdot\textbf{B} = \begin{pmatrix}\textbf{x}^\top\cdot\textbf{A}^{\top} + y\cdot\textbf{b}^{\top}\\ y\end{pmatrix}$, where $\textbf{x} \in \mathbb{Z}^{n}$ and $y\in\mathbb{Z}$.
The length of the vector represented by this point is minimised when $\textbf{x}^\top\cdot\textbf{A}^{\top} + y\cdot\textbf{b}^{\top}$ is at its minimum, in fact the length of this vector is at its shortest when $\textbf{x} = \textbf{s}$ and $y = -1$, giving us $\begin{pmatrix}\textbf{s}^{\top}\cdot\textbf{A}^{\top} -\textbf{b}^{\top}|-1\end{pmatrix} = \begin{pmatrix}(\textbf{A}\cdot\textbf{s}-\textbf{b})^\top|-1\end{pmatrix}= \begin{pmatrix}\textbf{e}^\top|-1\end{pmatrix}$.
\\
If we can find the shortest vector in this lattice $\begin{pmatrix}\textbf{e}^\top|-1\end{pmatrix}$, then we can calculate $\begin{pmatrix}\textbf{e}^\top|-1\end{pmatrix}\cdot\textbf{B}^{-1} =\begin{pmatrix}\textbf{s}^\top|-1\end{pmatrix}$ and therefore have solved the LWE instance.
\subsection{See further}
\paragraph{Homomorphic encryption} LWE is very versatile, and has opened up new branches of cryptography. It has allowed cryptographers to create the first homomorphic encryption system, where the exists some addition (or multiplication) such that applying that function to two ciphertexts and decrypting would give the same result as just adding (or multiplying) the initial plaintexts, i.e. $d(Add(e(p1),e(p2))) = p1 + p2$, where $e(),~d()$ are the encryption and decryption functions.

\paragraph{What other crypto is there?} Cryptography is an ever expanding field of research, other interesting areas include:
\begin{itemize}
	\item Multi-party computation - multiple users want to perform a computation on all their inputs, without anyone revealing their inputs.
	\item Threshold cryptography - a group of $n$ users want to encrypt some data, such that for some $t<n$, any subgroup of these users of size at least $t$ can decrypt the data while any subgroup of these users of size less than $t$ are unable to.
	\item Zero knowledge proofs - prove to someone that you know some value without giving away the value.
	\item Random number generation.
\end{itemize}



% \section{Lattice-based cryptography I}
% \label{sec:19}

% This lecture will be designed and delivered by Karl Southern.



\section{Lattice-based cryptography I}
\label{sec:19}
Lattice-based cryptography is one of the two main branches of post-quantum cryptography, the other being code-based cryptography (which will be covered in Lectures~\ref{sec:29} and \ref{sec:30}). Lattice-based cryptography covers all cryptosystems that rely on the hardness of lattice problems for their security.

\subsection{Post-Quantum Cryptography}

\paragraph{Quantum algorithms}
A quantum algorithm is any algorithm that utilises quantum computation in solving a problem, and requires a quantum computer to be run.
\\
There are two quantum algorithms that are important to cryptography, Grover's algorithm and Shor's algorithm. \Define{Grover's algorithm} is used as a search algorithm, and can find an item in a list in $O(\sqrt{N})$ queries, compared to standard classical search algorithms that take $O(N)$ queries. \Define{Shor's algorithm} is a polynomial time algorithm for solving integer factorisation, compared to the current classical algorithm which has an exponential run time.
\paragraph{What does this mean for cryptography?}
A brute force search for an AES(-256) key currently takes roughly $2^{256}$ time, as there are $2^{256}$ keys. Using Grover's algorithm for the brute force search has a run time of $\sqrt{2^{256}} = 2^{128}$. This can be easily remedied by doubling the size of the key. For virtually all public key cryptosystems (RSA, Diffie-Hellman, Elliptic Curve), Shor's algorithm can break the cryptosystem in polynomial time, the only solution to this is to create new cryptosystems that are secure against quantum attacks.
\\
A \Define{Post-Quantum cryptosystem} is any cryptosystem that runs on a classical computer and is secure against an attacker who has access to a quantum computer.
Quantum cryptosystems, where the cryptosystem requires a quantum computer to run and is secure against an attacker who has access to a quantum computer, do exist and are in use today, albeit in a limited capacity.
\subsection{Learning With Errors (LWE)}
\paragraph{LWE}
Learning with Errors is parametrised by two integers, $n$ and $q$, and an arbitrary error distribution over $\mathbb{Z}_{q}$, $\chi$. Quite often $\chi$ is based on the Gaussian distribution or the Binomial distribution. Given a secret vector $\textbf{s} \in \mathbb{Z}_{q}^{n}$ we create a single Learning With Errors instance by choosing a random vector $\textbf{a}$ from $\mathbb{Z}_{q}^{n}$, sampling a random integer from  our error distribution $\chi$ and calculating $b = \textbf{a}^{\top}\cdot\textbf{s} + e$, we then output the pair $(\textbf{a},b)$.

For example with $n = 4$, $q = 17$, a secret vector of $\textbf{s} = \begin{pmatrix}0\\1\\2\\3\end{pmatrix}$, a random vector $\textbf{a}$ of $\begin{pmatrix}0\\12\\16\\13\end{pmatrix}$ and a random error $e$ of $-1$, we calculate
\[
    b = \textbf{a}^{\top}\cdot\textbf{s} + e = (10\cdot0 +12\cdot1 +16\cdot2 +13\cdot3) - 1 = 82 ~\text{mod}~ 17 = 14,
\]
giving us a pair $([10,12,16,13],14)$.
As we often want to calculate multiple pairs using the same secret we often write this as $\textbf{b} = \textbf{A}\cdot \textbf{s} + \textbf{e}$, where $$\textbf{b} = \begin{pmatrix}b_{1}\\ \vdots\\  b_{m}\end{pmatrix}\in \mathbb{Z}_{q}^{m},~ \textbf{A} = \begin{pmatrix}\textbf{a}_{1} \\ \vdots \\ \textbf{a}_{m}\end{pmatrix} \in \mathbb{Z}_{q}^{m \times n},~ \textbf{s} \in \mathbb{Z}_{q}^{n} ~\text{and}~ \textbf{e} =\begin{pmatrix}e_{1}\\ \vdots\\  e_{m}\end{pmatrix} \in \mathbb{Z}_{q}^{m}$$ with each $e_{i}$ being drawn independently at random from $\chi$. This gives us $m$ equations, where each one satisfies $b_{i} =  \textbf{a}_{i}^{\top}\cdot\textbf{s} + e_{i}$.


\paragraph{LWE problems} There are two LWE problems, Decision-LWE and Search-LWE.
\\
\Define{Decision-LWE}.
\\
Instance: A set of $m$ pairs $(\textbf{a}_{i},b_{i})$, where $\textbf{a}_{i} \in \mathbb{Z}_{q}^{n}, b_{i} \in \mathbb{Z}_{q}$.
\\
Question: Are the pairs LWE instances or are they random?
\\
~\\
\Define{Search-LWE}.
\\
Instance: A set of $m$ pairs $(\textbf{a}_{i},b_{i})$, where $\textbf{a}_{i} \in \mathbb{Z}_{q}^{n}, b_{i} \in \mathbb{Z}_{q}$.
\\
Question: Find the secret vector $s$ that was used to create the pairs.
\\
~\\
Solving either of these problems (either classically or using a quantum computer) is at least as hard as solving several NP-hard lattice problems using a quantum computer.

\subsection{The Learning With Errors Cryptosystem}
\paragraph{Key Generation} Key Generation is done by creating a set of LWE equations, with $(\textbf{A},\textbf{b})$ being the public key and $\textbf{s}$ being the private key.
\\
\begin{enumerate}
	\item Choose $\textbf{A}$ uniformly at random from $\mathbb{Z}^{m\times n}_{q}$.
	\item Choose $\textbf{s}$ uniformly at random from $\mathbb{Z}^{n}_{q}$.
	\item Sample $\textbf{e}$ from $\chi^{m}$.
	\item Calculate $\textbf{b} = \textbf{A}\cdot \textbf{s}+\textbf{e}$
	\item Return $(\textbf{A},\textbf{b})$ as the public key.
	\item Return $\textbf{s}$ as the private key.
\end{enumerate}
\paragraph{Encryption} Encryption is done by turning the public key into two more sets of equations using a new secret vector, we can only encrypt one bit, $pt$, at a time.
\begin{enumerate}
	\item Choose $\textbf{r}$ uniformly at random from $\mathbb{Z}^{m}_{2}$.
	\item Sample $\textbf{e}'$ from $\chi^{n}$.
	\item Sample $e''$ from $\chi$.
	\item Calculate $\textbf{a}'^{\top} = \textbf{r}^{\top}\cdot\textbf{A} + \textbf{e}'^{\top}$
	\item Calculate $b' = \textbf{r}^{\top}\cdot\textbf{b} + e'' + pt\cdot\frac{q}{2}$
	\item Return $(\textbf{a}',b')$ as the ciphertext.
\end{enumerate}
\paragraph{Decryption} Decryption is done by using the private key to calculate $\textbf{a}'^{\top}\cdot\textbf{s} \approx b'$
\begin{enumerate}
	\item $v = \textbf{a}'^{\top}\cdot\textbf{s}$
	\item $v' = b'$
	\item $m' = v' - v$
	\item If $m'$ is closer to 0 than $\frac{q}{2}$ (mod $q$) then $pt = 0$ else $pt = 1$.
\end{enumerate}
\paragraph{Correctness} In order to prove that the cryptosystem is correct, we need to show that $m' \approx pt\cdot\frac{q}{2}$.
\begin{align*}
m'- m\frac{q}{2} & = v' - v - m\frac{q}{2} \\
& = b' - \textbf{a}'^{\top}\cdot\textbf{s} \\
& = \textbf{r}'^{\top}\cdot\textbf{b} + e'' - \textbf{a}'^{\top}\cdot\textbf{s} \\
& = \textbf{r}'^{\top}\cdot(\textbf{A}\cdot \textbf{s}+\textbf{e}) + e'' - (\textbf{r}^{\top}\cdot\textbf{A} + \textbf{e}'^{\top})\cdot\textbf{s} \\
& = \textbf{r}'^{\top}\cdot\textbf{A}\cdot \textbf{s}+\textbf{r}'^{\top}\textbf{e}+ e'' -\textbf{r}^{\top}\cdot\textbf{A}\textbf{s} - \textbf{e}'^{\top}\cdot\textbf{s} \\
& = \textbf{r}'^{\top}\cdot\textbf{e} + e'' - \textbf{e}'^{\top}\cdot\textbf{s} \\
\end{align*}
The cryptosystem is correct when $|\textbf{r}'^{\top}\cdot\textbf{e} + e'' - \textbf{e}'^{\top}\cdot\textbf{s}| < \frac{q}{4}$.
\\
In order to ensure that the system is `hard' to break, $q \approx n^2$ and $m \approx n\text{log}(q)$. Typically we use parameters $n \approx 8$, $q\approx 2^{13}$, $m\approx 512$, with $\textbf{s}, \textbf{r}$ being taken from the range $[-5, \ldots ,5]$ rather than from $[0, \ldots ,q]$.
\paragraph{Security} Like the LWE problem, the LWE cryptosystem is as hard to break as solving some NP-hard lattice problems. In the next section we discuss these lattice problems in more detail.
\subsection{Example}
For the following examples, we're going to be using the following parameters: $q = 13,~ n=4,~ m=14$.
\paragraph{Key Generation}
Our randomly chosen $\textbf{A},\textbf{s}$ and $\textbf{e}$ are:
$$\textbf{A} = \begin{pmatrix}
4& 11&  5&  7\\
7&  3&  3& 10\\
9&  8&  9&  3\\
12&  7&  1&  9\\
0&  8&  2&  9\\
7& 11&  4&  5\\
11&  0& 11&  7\\
0&  3&  5&  1\\
6&  5&  4&  1\\
3&  8& 12&  9\\
6&  0& 12&  1\\
7&  3&  8&  9\\
5&  0&  1&  1\\
11&  5&  0&  8
\end{pmatrix},~
\textbf{s} = \begin{pmatrix} 7 \\ 3 \\ 11 \\ 12 \end{pmatrix},~
\textbf{e} = \begin{pmatrix}0\\0\\0\\-1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\0\end{pmatrix}.
$$
Giving:
$$
\textbf{b} = \textbf{A}\cdot \textbf{s} + \textbf{e}
 =  \begin{pmatrix}
 4& 11&  5&  7\\
 7&  3&  3& 10\\
 9&  8&  9&  3\\
 12&  7&  1&  9\\
 0&  8&  2&  9\\
 7& 11&  4&  5\\
 11&  0& 11&  7\\
 0&  3&  5&  1\\
 6&  5&  4&  1\\
 3&  8& 12&  9\\
 6&  0& 12&  1\\
 7&  3&  8&  9\\
 5&  0&  1&  1\\
 11&  5&  0&  8
 \end{pmatrix} \cdot
  \begin{pmatrix} 7 \\ 3 \\ 11 \\ 12\end{pmatrix}
  +
  \begin{pmatrix}0\\0\\0\\-1\\0\\0\\1\\0\\0\\0\\0\\0\\0\\0\end{pmatrix}
  \text{mod}~13 =\begin{pmatrix}
  	5\\3\\1\\2\\11\\4\\10\\11\\9\\12\\4\\7\\6\\6
  \end{pmatrix}
$$
$(\textbf{A},\textbf{b})$ is returned as the public key and $\textbf{s}$ is returned as the private key.
\paragraph{Encryption}
We're going to encrypt a message of $`1'$. First we generate our random values:
$$ \textbf{r}^{\top} = \begin{pmatrix}
	1&1&0&1&0&1&1&0&1&0&1&0&1&0
\end{pmatrix},~
\textbf{e}'^{\top}=\begin{pmatrix}
	0&-1&0&0
\end{pmatrix},~
e'' = -1
$$
\\
This gives us:
\begin{align*}
\textbf{a}'^{\top} = \textbf{r}'^{\top}\cdot \textbf{A}+\textbf{e}'^{\top}  &=
\begin{pmatrix}
	1&1&0&1&0&1&1&0&1&0&1&0&1&0
\end{pmatrix} \cdot
\begin{pmatrix}
	4& 11&  5&  7\\
	7&  3&  3& 10\\
	9&  8&  9&  3\\
	12&  7&  1&  9\\
	0&  8&  2&  9\\
	7& 11&  4&  5\\
	11&  0& 11&  7\\
	0&  3&  5&  1\\
	6&  5&  4&  1\\
	3&  8& 12&  9\\
	6&  0& 12&  1\\
	7&  3&  8&  9\\
	5&  0&  1&  1\\
	11&  5&  0&  8
\end{pmatrix} + \begin{pmatrix}
0&-1&0&0
\end{pmatrix}\\
&=
\begin{pmatrix}
6&10&2&2
\end{pmatrix}\\
b'  = \textbf{r}^{\top}\cdot\textbf{b} + e'' + \text{pt}\cdot\frac{q}{2} &=  \begin{pmatrix}
	1&1&0&1&0&1&1&0&1&0&1&0&1&0
\end{pmatrix}\cdot
\begin{pmatrix}
	5\\3\\1\\2\\11\\4\\10\\11\\9\\12\\4\\7\\6\\6
\end{pmatrix} -1 + 6 = 9.
\\
\end{align*}
\\
Giving us a ciphertext of $(\textbf{a}',b')$
\paragraph{Decryption}
Given the ciphertext and the private key, find the corresponding plaintext.
\\
\begin{align*}
v &= \textbf{a}'^{\top}\cdot\textbf{s} = \begin{pmatrix}
	6&10&2&2
\end{pmatrix} \cdot \begin{pmatrix} 7 \\ 3 \\ 11 \\ 12 \end{pmatrix}~\text{mod} 13 =  1. \\
v' - v &= 9 - 1 = 8\\
\end{align*}
\\
As $8$ is closer to $6$ than $13$, we return $1$. We've correctly decrypted the ciphertext.
\\
\paragraph{Non-deterministic} In all the cryptosystems you've seen so far, if you encrypt the same message with the same key, you will always get the same result. We call cryptosystems where this happens \Define{deterministic}. The LWE cryptosystem is non-deterministic, and so every time you encrypt the same message with the same key the ciphertext will be different (with high probability). For example another valid encryption of the message `$1$' using the above public key is the ciphertext $\textbf{a}'^{\top} = \begin{pmatrix}
	0&6&12&2
\end{pmatrix},~ b' = 9$, or even $\textbf{a}'^{\top} = \begin{pmatrix}
3&10&6&2
\end{pmatrix},~ b' = 10$
\paragraph{A second decryption} Since there are multiple valid encryptions of the message `$1$', we now try to decrypt one of the other ciphertexts.
\begin{align*}
\textbf{a}'^{\top}& = \begin{pmatrix}
	3&10&6&2
\end{pmatrix},~ b' = 10
\\
v &= \textbf{a}'^{\top}\cdot\textbf{s} = \begin{pmatrix}
	3&10&6&2
\end{pmatrix} \cdot \begin{pmatrix} 7 \\ 3 \\ 11 \\ 12 \end{pmatrix}~\text{mod} 13 =  11.
\\
v' - v &= 10 - 11 = 12\\
\end{align*}
\\
As $12$ is closer to $13$ than $6$, we return $0$. We've got a decryption failure!
\subsection{See further}
\paragraph{Learning with Rounding} The LWE problem has a large number of variants, a common one is Learning with Rounding (LWR) where instead of adding errors, we round from $q$ to $p$ (for $p \ll q$). The equation changes from $\textbf{b} = \textbf{A}\cdot\textbf{s} + \textbf{e}$ to $\textbf{b} = \lfloor\textbf{A}\cdot\textbf{s}\rceil_{q\rightarrow p}$.
\\
$\lfloor x \rceil_{q\rightarrow p} = \left\lfloor\frac{p}{q}x\right\rceil~\text{mod}~p$  e.g. rounding $\begin{pmatrix}5\\7\\15\\13\end{pmatrix}$ from  $\mathbb{Z}_{16}$ to  $\mathbb{Z}_{4}$ would give $\begin{pmatrix}1\\2\\0\\3\end{pmatrix}$.
\paragraph{NIST-PQC standardisation}
After DES was broken, NIST (The US National Institute of Standards and Technology) launched a competition to find an algorithm to replace it in 1997, and the process ended in 2001 with Rijndael selected as the `winner'. We are currently going through a similar standardisation process for post-quantum cryptography, although we expect multiple winners. Leading candidates include Kyber (based on a LWE variant), Saber (based on a LWR variant), NTRU (another hard lattice problem) and Classic McEliece (based on code based crypto, covered in Lecture~\ref{sec:29}); alternate candidates include Frodo (based on LWE), NTRU-Prime, BIKE (code-based), HQC (code-based) and SIKE (based on Supersingular isogenies).
\subsection{Exercises}
\begin{exercise}
	In the paragraph on non-determinism, two alternate encryptions of the message were given, we decrypted one of them and had a decryption failure. Decrypt the other and check if it decrypts correctly.
\end{exercise}
\begin{exercise}
	A realistic parameter set for a 128-bit secure LWE cryptosystem, is $q = 2^{15}, n = 8, m = 640$. Assuming we're sending a 128-bit message, what is the size of the message after it has been encrypted? How does this compare to other public key encryption schemes you've met so far?
\end{exercise}

\section{Signature schemes}
\label{sec:18}

\subsection{Introduction to digital signatures}

\paragraph{Man-in-the-middle}

Suppose Alice and Bob are using Diffie-Hellman to exchange a key. Suppose Eve acts as the \Important{``man-in-the-middle''}, i.e. she intercepts the messages between Alice and Bob (unbeknownst to either of them). Then Eve can fool them as follows.
\begin{enumerate}
    \item Eve forms her own secret key $e$, and public key $E = g^e \mod p$.

    \item Eve intercepts $A = g^a \mod p$, and sends $E$ on to Bob.

    \item Bob responds with $B = g^b \mod p$, which Eve intercepts and sends $E$ back to Alice.

    \item Eve computes $K_A = A^e = g^{ae} \mod p$ to communicate with Alice and $K_B = B^e = g^{eb} \mod p$ to communicate with Bob.

    \item Alice computes $K_A = E^a = g^{ae} \mod p$.

    \item Bob computes $K_B = g^{be} \mod p$.
\end{enumerate}
That way, Eve can continue to intercept messages, decrypt, re-encrypt and forward without Alice or Bob realising it. Eve can even alter messages without Alice or Bob being aware.



\paragraph{Digital signatures}

A digital signature is an addendum to a digital document or message to verify who wrote or sent it. If $M$ is a message Alice wants to send to Bob, she creates a signature $S$, and sends the pair $(M,S)$ to Bob. Bob must have some way of verifying that Alice created the signature and that the message has not been altered since she sent it.

A digital signature should provide the same guarantees as a traditional hand-written signature. Namely:
\begin{description}
\item[Unforgeability] Only Alice should be able to sign her name to a message.

\item[Undeniability] Alice should not be able to later deny that she signed a signed message.

\item[Authentication] The signature should confirm that the contents of the message are as intended.
\end{description}

Clearly $S$ must be a function of $M$ and of some private key $d_A$ known only to Alice. $S = f(M,d_A)$


\subsection{Some digital signature schemes}

\paragraph{Public key based schemes}


Any public key cryptosystem in which encryption and decryption are commutative, i.e. if
\[
    e(d(M)) = d(e(M)) = M,
\]
can be used as a signature scheme.

RSA signature: Recall Alice has a public key $(n,e)$ and private key $d$, such that
\[
    C^d = M^{ed} = M \mod n.
\]
\begin{enumerate}
    \item Alice creates the signature by ‘decrypting’ her message
    \[
    	S = M^d \mod n.
    \]

    \item Alice send the pair $(M,S)$ to Bob.

    \item Bob ‘encrypts’ $S$, and verifies that
    \[
        S^e = M^{de} = M \mod n.
    \]
\end{enumerate}

Being able to sign general messages requires knowledge of the secret key $d$, so Bob trusts the message is from Alice.



\paragraph{Example}

Alice’s public key is $(n = 65,e = 19)$. Her private key is $d = 43$. Alice sends the message ``Hi'' using conversion from Exercise \ref{exercise:rsa}:
\begin{align*}
    Hi -> M &= (11, 38)\\
    S &= (1143 \mod 65, 3843 \mod 65) = (41 , 12).
\end{align*}
Bob receives the message/signature pair $((11,38),(41,12))$.
He converts $(11,38)$ into ``Hi''.
He checks $S$ by checking $(4119 \mod 65,1219 \mod 65) = (11,38)$
It does - he knows only someone with knowledge of Alice’s private key could have created the message! I.e. it is from Alice.



\paragraph{Problems}

Firstly, Eve can \Important{sign random messages}:
\begin{enumerate}
    \item Eve chooses any $R < n$.

    \item Eve computes $M = R^e \mod n$.

    \item Eve can send $(M,R)$ as a valid message/signature pair from Alice.
\end{enumerate}

Secondly, Eve may be able to \Important{dupe Alice} into signing unseen messages:
\begin{enumerate}
    \item Eve chooses $R < n$ and computes $R^{-1} \mod n$.

    \item Eve asks Alice to sign $M_1 = MR \mod n$ and $M_2 = R^{-1} \mod n$.

    \item Eve can use
    \[
        S_1S_2 = M_1^d M_2^d = M^d R^d R^{-d} = M^d \mod n,
    \]
    to sign $M$, which Alice has never seen.
\end{enumerate}

%If the message is long, the signature will take a long time to compute. Typically public key cryptosystems are used for key exchange, rather than long message transmission, due to their lack of speed. E.g. RSA is approximately 1000 times slower than DES.





\subsection{Hashing and signing}

\paragraph{‘Hash then sign’ schemes}

\begin{description}
\item [Setup] Alice and Bob need to setup the following:
\begin{itemize}
    \item Alice has a public key (e.g. $(n,e)$), and private key (e.g. $d$);

    \item Alice and Bob agree on some public hash function $h$ to use.
\end{itemize}

\item [Signing] To sign a message $M$, Alice:
\begin{enumerate}
    \item computes the hash of the message $H = h(M)$;

    \item uses her private key to sign the hashed message, e.g. $S = H^d$;

    \item sends the pair $(M,S)$ to Bob.
\end{enumerate}

\item [Verification] Bob verifies that the signature is correct as follows:
\begin{enumerate}
    \item Bob computes the hash of the message $H = h(M)$;

    \item Bob checks that the signature matches the hash, i.e. $S^e = H$.
\end{enumerate}
\end{description}

Much quicker than signing, transmitting and checking the full message.



\paragraph{Signed, encrypted messages}

So far all signed messages have been sent in the clear: $(M, S(h(M) )$. To send a secure message Alice may:
\begin{enumerate}
    \item Compute the hash $H$ of the message $M$.

    \item Compute the signature $S$ for $H$ using her private key.

    \item Encrypt the full message $(M,S)$ using Bob’s public key, obtaining $C$.

    \item Send $C$ to Bob.
\end{enumerate}

To verify, Bob then:
\begin{enumerate}
    \item Decrypts $C$ to obtain $(M,S)$.

    \item Computes the hash $H$ of $M$.

    \item Encrypts the signature $S$ using Alice’s public key, obtaining $H'$.

    \item Checks that $H' = H$.
\end{enumerate}

\subsection{Certificate Authorities and chains of trust}

\paragraph{Trusted Authorities}

All this can still be foiled if Eve intercepts every message from the beginning, and forges replies.
We need a method of knowing we really have Bob’s public key, and not one Eve has advertised falsely as Bob’s.

This is overcome by have a trusted authority, or \Important{certificate authority} (CA), who verifies people’s identities and public keys, and signs a certificate to confirm that they are who they say they are.

For example, imagine Bob wishes to set up an internet shop. He generates private and public keys, and approaches a CA (e.g. Verisign). The CA checks him out and signs a certificate saying he is Bob and his public key is correct. If Alice finds Bob’s shop and wishes to send a secure message, she can check that the public key advertised does belong to Bob, because it is signed by the CA, and Alice already knows the CAs public key.

Browsers come pre-configured with the public keys of major certificate authorities. You have to trust the CA to do its homework and check the identity. In 2001 someone convinced Verisign that they were a representative of Microsoft, and got a certificate signed by Verisign in the name of Microsoft Corporation. It later transpired that they were unconnected, and Microsoft and Verisign acted quickly to try and stop them.



\paragraph{Chains of trust}

How do I know to trust the CA? There are a few major CAs whose details are built into modern web browsers. You can accept others, if you trust them. Other CAs have their details certified by one of the major ones. Corporations are certified by a CA. You have to trust the chain of certifications, up to the major CA, in order to trust the website.

%E.g. DUO. insert picture


\subsection{See further}

\paragraph{Discrete logarithm based signatures}
The first signature scheme based on discrete logarithm is the ElGamal scheme. This was modifiesd by Schnorr, and in turn this became the Digital Signature Algorithm (\Important{DSA}), developed by the NSA behind closed doors and proposed in 1991. Similarly to DES, it was based on a relatively small modulus $p$ of 512 bits, leading to criticism. NIST then allowed for variable modulus sizes. The DSA was then superseded in 2000 by the Elliptic Curve Discrete Signature Algorithm (\Important{ECDSA}), which is similar to DSA but as its name suggests, is based on the discrete logarithm problem on elliptic curves.

\paragraph{Lamport's scheme}
The \Define{Lamport signature scheme} is based on hash functions. It is \Important{one-time-secure}, that is it is secure as long as the private key is used to sign only a single message. Here is how it works with a cryptographic hash function $H$.

Suppose Alice wants to sign the $l$-bit long message $m = m_1 \dots m_l$. She first chooses $2l$ words $x_{0,1}, \dots, x_{0,l}; x_{1,1}, \dots, x_{1,l} \in \{0,1\}^n$ at random. She then computes their images $y_{i, j} = H(x_{i,j})$. She keeps the $x_{i,j}$ secret and releases the $y_{i,j}$ in public.

Then the signature of $m = m_1 \dots m_l$ is $\sigma = (\sigma_1, \dots, \sigma_l) = (x_{m_1,1}, \dots, x_{m_l,l})$. Bob verifies that it is genuine by verifying that $H(\sigma_j) = y_{j, m_j}$ for all $j$.

%\paragraph{Identification schemes and the Fiat-Shamir transform}

\subsection{Exercises}

\begin{exercise} \label{exercise:signature1}
Bob has RSA public key $(n,e)=(65,19)$. 'Bob' sends you the message/signature pair $(M,S) = (Yes,hoS)$. Verify whether this message is correctly signed by Bob or not. Note - you should treat each character as a separate message, i.e. $(M,S)=(M_1 M_2 M_3, S_1 S_2 S_3)$, $M_1=Y$, $S_1=h$, etc.
\end{exercise}

\begin{exercise} \label{exercise:signature2}
Generate a valid message/signature pair that appears to be signed by Bob without using his private key. (The exact message may be out of your control.)
\end{exercise}

\begin{exercise} \label{exercise:signature3}
Generate a valid signature for the messages 'hAw' and 'H' using Bob's private key ($d=43$).
\end{exercise}

\begin{exercise} \label{exercise:signature4}
Assume you have tricked Bob into providing the signatures in Exercise \ref{exercise:signature3}. Use these two signatures to generate a valid signature for the message 'Now', and verify that it is correct.
\end{exercise}




\section{Hash functions}
\label{sec:17}

\subsection{Security of hash functions}


\paragraph{Hash function}

A \Important{hash function} is simply a function that takes inputs of some length and compresses them into short, fixed-length outputs. The classic use of hash functions is in data structures, where they can be used to build hash tables that enable constant-time lookup when storing a set of elements. A good hash function for this purpose is one that yields few collisions, where a collision is a pair $x$, $x'$ such that $H(x) = H(x')$.

\paragraph{Collision resistance}

Informally, a function $H$ is \Important{collision resistant} if is intractable (say infeasible with a probabilistic polynomial time algorithm) to find a collision in $H$. We will only be interested in hash functions whose domain is larger than their range. Therefore, collisions must exist, but such collisions are hard to find.

Cryptographic hash functions used in practice generally have a fixed output length and are unkeyed, meaning that the hash function is just a function
\[
    \Define{H : \{0,1\}^* \to \{0,1\}^l}.
\]
This is problematic in practice, as there is always a constant-time algorithm to find a collision (e.g. list all possible inputs of length $l+1$, then you must find a collision there). Nonetheless, the unkeyed hash functions used in day-to-day life are collision resistant for all practical purposes.

\paragraph{Weaker notions of security}

In some applications, we only need to rely on weaker security requirements, for instance:
\begin{description}
    \item [Second-preimage resistance] Given $x$, it is hard to find an $x' \ne x$ such that $H(x') = H(x)$.

    \item [Preimage resistance] Given $y$, it is hard to find an $x$ such that $H(x) = y$. (This is equivalent to: $H$ is a one-way function.)
\end{description}

\subsection{Constructions}

Hash functions $H$ are often constructed by first designing a collision resistant compression function $h$ handling fixed-length inputs, and then using domain extension to handle arbitrary-length inputs. We now look at a way of constructing the compression function using block ciphers, and a way of extending the domain by applying the compression function recursively.

\paragraph{Hash functions from block ciphers}

Let $e(k,x)$ be a block cipher with $n$-bit key length and $l$-bit block length: for a plaintext block $x \in \{0,1\}^l$ and a key $k \in \{0,1\}^n$, it returns $e(x,k)$. We can then define the compression function $h : \{0,1\}^{n + l} \to \{0,1\}^l$ as
\[
    h(x,k) = e(x,k) \oplus x.
\]
This is called the \Define{Davies-Meyer construction}.

Collision resistance of the compression function can be proved for so-called ideal ciphers, which are a much stronger notion of security than what we needed for the design of block ciphers. In particular, in an ideal cipher, the permutations $e(\cdot,k)$ and $e(\cdot,k')$ must behave independently even if $k$ and $k'$ only differ by one bit. Also, in a ideal-cipher there can be no keys $k$ where $e(\cdot,k)$ should behave randomly even with the knowledge of $k$. As such, block ciphers used for encryption, e.g. DES and AES are not suitable for the Davies-Meyer construction.

\paragraph{The Merkle-Damg\aa{}rd transform}

The \Define{Merkle-Damg\aa{}rd transform} is a common approach for domain extension while maintaining collision resistance. It is used in practice in MD5 and in the family of hash functions SHA.

For simplicity, suppose $h : \{0,1\}^{2n} \to \{0,1\}^n$ is a collision resistant compression function (in fact, we can work with $\{0,1\}^{n+m} \to \{0,1\}^n$ for any $m \ge 1$). We will now construct a collision resistant hash function $H : \{0,1\}^* \to \{0,1\}^n$. Again for simplicity we will assume that the input $x$ has length $L = |x| < 2^n$.

The transform goes as follows.
\begin{enumerate}
    \item Split $x$ into $B = \lceil L/n \rceil$ blocks of length $n$, padding $x$ with zeros if necessary, to obtain $x_1, \dots, x_B$.

    \item Set $x_{B+1} = L$, encoded in binary as an $n$-bit string.

    \item Set $z_0 = 0^n$ (the initialisation vector IV).

    \item For $i=1, \dots, B+1$, compute
    \[
        z_i = h( z_{i-1}, x_i).
    \]

    \item Return $H(x) = z_{B+1}$.
\end{enumerate}



\subsection{Applications}

\paragraph{Fingerprinting and deduplication}
When using a hash function $H$, the hash (or digest) of a file serves as a unique identifier for that file. The hash function $H(x)$ of the file $x$ is like a fingerprint, and one can check whether two files are equal by comparing their digests. This idea can be used in different ways.

\begin{description}
    \item [Virus fingerprinting] Virus scanners identify viruses and block or quarantine them. One of the most basic steps for this is to store a database containing the hashes of known viruses and then to look up the hash of a downloaded file in this database.

    \item [Deduplication] Data deduplication is used to eliminate duplicate copies of data, especially in the context of cloud storage where multiple users rely on a single cloud service to store their data. The key observation here is that if multiple users wish to store the same file, then the file only needs to be stored once and need not be uploaded separately by each user.
\end{description}



\paragraph{Password hashing}

One of the most common and important uses of hash functions is for \Important{password protection}. Instead of storing the passwords in the clear, only the hash of the password is stored instead. That is, the hard drive stores the value $hpw = H(pw)$ in a password file; later, when the user enters their password $pw$, the system checks whether $H(pw) = hpw$ before granting access.

If the password is chosen from a relatively small space of possibilities (e.g. a dictionary of English words, with only less than $100,000$ words), then an attacker could enumerate all possible passwords and check their hash. We would like to claim that an attacker can do no better than this. However, preimage resistance only guarantees that $H(x)$ is hard to invert when $x$ is chosen uniformly from a large domain like $\{0,1\}^n$. It says nothing about the hardness of inverting $H$ when $x$ is drawn according to another distribution (or from a smaller domain altogether). Moreover, preimage resistance says nothing about the concrete time it takes to find a preimage: if there is an algorithm that finds a collision in time $2^{n/2}$ (still exponential!), then a 30-bit password would be cracked in $2^{15}$ time.

\paragraph{Mitigation of attacks against password hashing}
One technique to mitigate the threat of password cracking is the use of ``slow'' has functions, or to \Important{slow down} existing hash functions by using multiple operations (i.e. computing $H^I(pw)$ for some $I \gg 1$). this slows down legitimate users by a factor of $I$, which is OK if $I$ is not too high (say, $I = 10^3$). On the other hand, it has a significant impact on an adversary attempting to crack thousands of passwords at once.

A second method is to introduce a \Important{salt}. When a user registers their passwords, the laptop/server will generate a long random value $s$ (a ``salt'') unique to that user, and store $(s, hpww = H(s, pw))$ instead of merely sorting $H(pw)$ as before. Since $s$ is unknown to the adversary in advance, preprocessing is ineffective and the best they can do is wait to obtain the password file and perform the exhaustive search as before.




\subsection{See further}

\paragraph{Attacks against hash functions}
The main point of an attack is to find a collision. If $H : \{0,1\}^* \to \{0,1\}^l$, then the naive algorithm of trying $H(x)$ for all $x \in \{0,1\}^{l+1}$ is sure to find a collision. But one can do better by using the \Important{birthday paradox}. Indeed, it can be shown that for $X = \{x_1, \dots, x_q\} \subseteq \{0,1\}^l$ chosen uniformly at random, the probability of finding a collision is roughly $1/2$ for $q = \Theta(2^{l/2})$. That simple attack uses a lot of memory but can be adapted to use much less space (the so-called Small-space birthday attacks).

There are other attacks aimed at inverting one-way function (to attack against preimage resistance), offering clever tradeoffs between time and memory.


\paragraph{Secure Hash Algorithm}
The Secure Hash Algorithm (\Important{SHA}) refers to a series of cryptographic hash functions standardised by NIST. Perhaps the most well-known is SHA-1, introduced in 1995, and replacing the insecure SHA-0. It was then superseded by SHA-2 (which includes SHA-256 and SHA-512, referring to the respective output lengths). All those hash functions use the Davies-Meyer construction and the Merkle-Damg\aa{}rd transform. Note that the block ciphers were designed ad hoc for the SHA hash functions.

The new standard is now \Important{SHA-3} (Keccak), which was standardised in 2015. Its design is significantly different from previous SHA functions. Indeed, it does not use Davies-Meyer or Merkle-Damg\aa{}rd, but instead uses an unkeyed permutation to build the compression function and then uses the so-called sponge construction for domain extension.



\subsection{Exercises}

\begin{exercise}
We have looked at weak keys of DES in Exercise \ref{exercise:weak_keys}. It can be shown that if the key $K$ is weak, then we can easily find fixed points, i.e. $x$ such that $e(x,K) = x$. Given that fact, show that it is easy to find collisions for the Davies-Meyer construction using DES.
\end{exercise}


\begin{exercise}
Let us evaluate the performance of a birthday attack for collision. Suppose the compression function $h$ behaves randomly for inputs of size $l$ (equal to the output).
\begin{enumerate}
\item Choose $q$ elements $x_1, \dots, x_q \in \{0,1\}^l$ uniformly at random. Show that the probability of finding a collision is then equal to
\[
    P = 1 - \left( 1 - \frac{1}{2^l} \right) \left( 1 - \frac{2}{2^l} \right) \dots \left( 1 - \frac{q-1}{2^l} \right).
\]

\item Obviously, we are looking to have $q \ll 2^l$. By using the approximation $1 - \delta \approx e^{-\delta}$ for small $\delta > 0$, approximate $P$ to
\[
    P \approx e^{- \frac{ q^2 }{ 2^{l+1} } }.
\]

\item Say we are aiming at a probability of success of $P  = 1/2$, then show that we only need
\[
    q \approx \sqrt{2 \ln 2} \cdot 2^{l/2}.
\]
\end{enumerate}
\end{exercise}




\begin{exercise}
MD5 is an old hash function that was proven insecure in 2004. Nowadays, any laptop can implement an attack finding a collision for MD5 in a few seconds (we can even ask for meaningful collisions) However, MD5 is still used by some legacy code to this day. Have a look around and see if you can spot usage of MD5.

Similarly, see where SHA-256 is being used instead of SHA-3.
\end{exercise}




\section{Discrete logarithm cryptography}
\label{sec:16}

\subsection{Diffie-Hellman key exchange}

The \Define{Diffie-Hellman key exchange} (1976) is a mechanism to agree on a secret key by exchanging messages without revealing the actual key value. It is based on the difficulty of \Important{discrete logarithms}.

If $p$ is a prime, then we say $g \in \Z_p^*$ is \Define{primitive mod $p$} (or a generator) if all the elements of $\Z_p^*$ are powers of $g$:
\[
    \{g, g^2 \mod p, g^3 \mod p, \dots, g^{p-1} \mod p = 1  \} = \{1, 2, \dots, p-1 \}.
\]
It is easily shown that there are $\phi(p-1)$ primitive elements mod $p$, where $\phi(n)$ is Euler's totient function.

Here is the protocol.
\begin{enumerate}
    \item Alice and Bob publicly agree a large prime $p$ and a primitive element $g$.

    \item Alice and Bob choose private keys $a$ and $b$ (between $2$ and $p-2$).

    \item Alice computes
    \[
        A = g^a \mod p,
    \]
    and sends $A$ to Bob.

    \item Bob computes
    \[
        B = g^b \mod p,
    \]
    and sends $B$ to Alice.

    \item Alice computes the shared secret key
    \[
        K = B^a \mod p.
    \]

    \item Bob computes $K$ as
    \[
        K = A^b \mod p.
    \]
\end{enumerate}


Alice and Bob know the same $K$, since
\[
    A^b = (g^a)^b = g^{ab} = (g^b)^a = B^a.
\]
Eve only knows $A$, $B$, $p$ and $g$. If Eve could compute discrete logarithms easily, then she could obtain (for instance): $a = \log_g A$ and then $K = B^a \mod p$.


\subsection{The ElGamal cryptosystem}


Let $p$ be a large prime number, and let $g \in \Z_p^*$ be a primitive element. Let $\mathcal{M} = \Z_p^*$, $\mathcal{C} = \Z_p^* \times \Z_p^*$.

\begin{description}
\item[Key generation]
    \begin{enumerate}
        \item Let $A = g^a \mod p$ for some $a$

        \item Public key: $(p, g, A)$

        \item Private key: $a$
    \end{enumerate}

\item[Encryption] Choose a secret random number $k \in \Z_{p-1}$, then
\begin{align*}
    e(x, k) &= (y_1, y_2),\\
    y_1 &= g^k \mod p\\
    y_2 &= x A^k \mod p.
\end{align*}


\item[Decryption]
\[
    d(y_1,y_2) = y_2 (y_1^a)^{-1} \mod p.
\]
\end{description}

Again, decryption is correct since:
\[
    d(y_1, y_2) = y_2 (y_1^a)^{-1} = x A^k (g^{ak})^{-1} = x \mod p.
\]

Here is an example. Let $p = 2579$ and $g = 2$; $g$ is a primitive element mod $p$. Let $a = 765$, then
\[
    A = g^a \mod p = 2^{765} \mod 2579 = 949.
\]
Now suppose Alice wishes to send the message $x = 1299$. She randomly chooses $k = 853$, then
\begin{align*}
    y_1 &= g^k \mod p = 2^{853} \mod 2579 = 435,\\
    y_2 &= x A^k \mod p = 1299 \times 949^{853} \mod 2579 = 2396.
\end{align*}
When Bob receives the ciphertext $(435, 2396)$, he computes
\[
    x = y_2 ( y_1^a )^{-1} \mod 2579 = 2396 \times ( 435^{765} )^{-1} \mod 2579 = 1299.
\]



\subsection{Some algorithms for discrete logarithm}


\paragraph{Naive discrete log algorithms}
The problem is: given a prime $p$, a primitive element $g$ mod $p$, and $A \in \Z_p^*$, find $a$ such that $A = g^a$. The first naive algorithm is exhaustive search: compute the powers of $g$ in sequence ($1, g, g^2, g^3, \dots$) until we hit $g^a = A$. This will take $\Omega(p)$ time (for simplicity we assume operations mod $p$ are done in constant time), which is exponential in the size of $p$. Another approach is to precompute all the possible values $g^i$ and then sort the list $(i,g^i)$ according to their second coordinates. The precomputation still takes $O(p)$, but once that is done, finding $A$ on the list only takes $O(\log p)$ thanks to search by dichotomy.

\paragraph{Shank's algorithm}
\Define{Shank's algorithm} is a time-memory tradeoff between the two naive approaches we've mentioned above.

\begin{algorithm}[H]
\caption{Shank's algorithm$(p-1,g,A)$}
\begin{algorithmic}[1]
	\State{$m \gets \lceil \sqrt{p-1} \rceil$}
    \For{$j \gets 0$ to $m-1$}
        \State{Compute $g^{mj}$}
    \EndFor
    \State{$L_1 \gets$ the list of pairs $(j, g^{mj})$ sorted according to their second coordinates}
    \For{$i \gets 0$ to $m-1$}
        \State{Compute $A g^{-i}$}
    \EndFor
    \State{$L_2 \gets$ the list of pairs $(i, A g^{-i})$ sorted according to their second coordinates}
    \State{Find two pairs $(j,B) \in L_1$ and $(i,B) \in L_2$ with the same second coordinate}
    \State{\Return{$mj + i \mod p-1$}}
\end{algorithmic}
\end{algorithm}

Indeed it works:
\[
    Ag^{-i} = g^{mj} \iff A = g^{mj + i}.
\]


Shank's algorithm is also called \Important{Giant step-baby step} algorithm. Indeed, the first few lines of the algorithm that determine $L_1$ are precomputations. They are the giant steps: we take steps of size $\approx \sqrt{p}$. After that, computing $L_2$ is the baby steps (of size step $1$).

\paragraph{Pollard's rho algorithm}

We have already seen \Define{Pollard's rho} algorithm for factoring. The same basic idea can be adapted to compute the discrete logarithm as well. Once again, we form a sequence $x_0, x_1, \dots$ by iterating a random-looking function $f$. We still aim to find a collision of the form $x_i = x_{2i}$.

First, partition $\Z_p^*$ into three sets $S_1, S_2, S_3$ of roughly equal size. We work on triples $(x,s,t) \in \Z_p^* \times \Z_{p-1} \times \Z_{p-1}$ and we define
\[
    f(x, s, t) = \begin{cases}
    (Ax, s, t+1)    &\text{if } x \in S_1\\
    (x^2, 2s, 2t)   &\text{if } x \in S_2\\
    (gx, s+1, t)    &\text{if } x \in S_3.
    \end{cases}
\]
We furthermore enforce that our triples have the property
\[
    x = g^s A^t.
\]
Note that $f$ preserves that property. So we only need to start with a triple satisfying that property, say $(1,0,0)$. So we define
\[
    (x_i, s_i, t_i) = \begin{cases}
    (1,0,0) &\text{if } i=0\\
    f(x_{i-1}, s_{i-1} , t_{i-1}) &\text{otherwise.}
    \end{cases}
\]
(Note: we need to make sure $1 \notin S_2$, otherwise we would get stuck at $(1,0,0)$.)

The following lemma justifies looking for a \Important{collision}.

\begin{lemma}
Suppose $x_i = x_{2i}$ and $\gcd(t_{2i} - t_i, p-1) = 1$, then
\[
    a = \log_g A = (s_i - s_{2i})(t_{2i}- t_i)^{-1} \mod p-1.
\]
\end{lemma}

\begin{proof}
Under the hypothesis, we have
\begin{align*}
    g^{s_{2i}} A^{t_{2i}} &= g^{s_i} A^{t_i}\\
    g^{s_{2i} + at_{2i} } &= g^{ s_i + at_i }\\
    s_{2i} + at_{2i}  &=  s_i + at_i \mod p-1\\
    a &= (s_{2i} - s_i)(t_i - t_{2i})^{-1} \mod p-1.
\end{align*}
\end{proof}

We thus obtain the algorithm.

\begin{algorithm}[H]
\caption{Pollard $\rho$ discrete log algorithm$(p-1,g,A)$}
\begin{algorithmic}[1]
	\State{Partition $\Z_p^* = S_1 \cup S_2 \cup S_3$}
	\State{$(x,s,t) \gets (1,0,0)$}
	\State{$(x',s',t') \gets f(x,s,t)$}
	\While{$x \ne x'$}
	    \State{$(x,s,t) \gets f(x,s,t)$}
	    \State{$(x',s',t') \gets f^2(x',s',t')$}
    \EndWhile
    \If{$\gcd(t' - t, p-1) = 1$}
        \State{\Return{$(s - s')(t' - t)^{-1} \mod p-1$}}
    \Else{}
        \State{\Return{"failure"}}
    \EndIf
\end{algorithmic}
\end{algorithm}

If the collision is not valid, i.e. $\gcd(t' - t, n) = d > 1$, then the situation is not too bad. In fact, there are $d$ possible solutions for $a$, so if $d$ is not too large one can determine the $d$ possible solutions and try them all out to find the correct one.

Again, under reasonable assumptions about the randomness of the function $f$, we expect to compute discrete logarithms in $O(\sqrt{p})$ iterations.

\subsection{See further}

\paragraph{More algorithms for discrete logarithm}
Again, there are many more algorithms for discrete logarithms. We briefly mention \Important{Pohlig-Hellman} and the \Important{Index Calculus algorithm}; both are reviewed in \cite{Sti02}. The latter runs in subexponential running time. The same team that holds the record for factoring the largest semiprime also recently broke the record for discrete log. In fact, the best algorithms for factoring and discrete log run in similar times.

\paragraph{Elliptic curves}
In abstract terms, the discrete logarithm problem can be applied to any \Important{finite cyclic group}. A finite cyclic group is a pair $(G,\cdot)$ where $G$ is a finite set (say of order $|G| = n$) and $\cdot$ is a binary (product) operation on $G$ such that $G = \{g^0 = 1, g^1 = g, \dots, g^{n-2} \}$ for some $g \in G$. (The element $g$ is then called the generator.) So far, we have focused on the cyclic group $(\Z_p^*, \times)$. But there are many other cyclic groups out there.

Even though two cyclic groups of the same order are algebraically equivalent (isomorphic), the way they are described makes the discrete logarithm problem more or less difficult. Think of the cyclic group $(\Z_{p-1}, +)$ with generator $1$. Even though this group is equivalent to $(\Z_p^*, \times)$, its discrete log problem is trivial!

\Important{Elliptic-curve cryptography} (ECC) is based on the discrete logarithm for cyclic groups defined on points of elliptic curves, which is harder than the discrete log for $(\Z_p^*, \times)$. The algebra used there is very involved so we will not have time to talk about it. Note that ECC uses significantly \Important{smaller keys} than RSA for the same level of security (e.g. $108$ bits for ECC against $512$ bits for RSA).

\subsection{Exercises}

\begin{exercise}
Go through the following example of Diffie-Hellman key exchange:
\begin{align*}
    p &= 17,\\
    g &= 3,\\
    a &= 5,\\
    b &= 11.
\end{align*}
Compute all the messages and the agreed key.
\end{exercise}



\begin{exercise}
Here is a naive attempt at key exchange, where Alice wants to transmit an $n$-bit key $K = (k_1, \dots, k_n)$ to Bob.
\begin{enumerate}
    \item Alice generates a random string $a = (a_1, \dots, a_n)$ and sends
    \[
        A = K \oplus a.
    \]

    \item Bob receives $A$, generates another random string $b = (b_1, \dots b_n)$, and sends back
    \[
        B = A \oplus b.
    \]

    \item Alice then ``removes'' the string $a$ and sends back
    \[
        A' = B \oplus a = K \oplus b.
    \]

    \item Bob obtains the key by computing
    \[
        K = A' \oplus b.
    \]
\end{enumerate}
As you can see, the key $K$ is never sent in plaintext. However, this scheme is insecure! Why?
\end{exercise}



\section{RSA II: Cryptanalysis}
\label{sec:15}
\subsection{Vulnerabilities of RSA}

\paragraph{General vulnerabilities}

Due to its popularity, RSA has been the subject of much research trying to break the cryptosystem.

Several schemes are known for attacking the system, which perform well if special conditions are met, for instance:
\begin{itemize}
    \item Primes $p$ and $q$ are ‘too close’ (searching from $\sqrt{n}$ up to $(p+q)/2$ reveals a factor).

    \item If $d$ is too small $(< n^{0.25})$, then $d$ can be recovered from $(n,e)$.

    \item Sending the same message to different people with the same $e$ is insecure.
\end{itemize}

No scheme for cracking RSA in general, except by factoring $n$, is known.


\paragraph{Other issues}
Problem of non-random messages: If you only send a limited range of messages, Eve can try encrypting each to see which gives the intercepted cryptogram. For instance, if you always send ‘Transfer X pounds into my account’ or ‘My 3-digit card security code is X’.

This is why Alice should always pad messages with random bits.



\subsection{Factorisation algorithms}

\paragraph{Trial division}
The naive factoring algorithm, called \Define{trial division}: ``check all numbers $p$ up to $\sqrt{n}$ to see if $p \divides n-1$'' runs in exponential time. Indeed, the size of the input is $\log n$, while the algorithm would run in $\Omega(\sqrt{n}) = \Omega( c^{\log n} )$ for some constant $c$. There are no polynomial time algorithms known for factoring, but below we present three algorithms that can be much more efficient than the naive factoring algorithm.

\paragraph{Pollard's $p-1$ algorithm}

\Define{Pollard's $p-1$ algorithm} has two inputs: the odd integer $n$ to be factored, and a prescribed bound $B$.


\begin{algorithm}[H]
\caption{Pollard $p-1$ factoring algorithm$(n,B)$}
\begin{algorithmic}[1]
	\State{$a \gets 2$}
	\For{$j \gets 2$ to $B$}
	    \State{$a \gets a^j \mod n$}
    \EndFor
    \State{$d \gets \gcd(a-1, n)$}
    \If{$1 < d < n$}
        \State{\Return{$d$}}
    \Else{}
        \State{\Return{"failure"}}
    \EndIf
\end{algorithmic}
\end{algorithm}

Here's a simple explanation of why this works. Suppose $p$ is a prime divisor of $n$, and suppose that $p-1$ is \Define{$B$-smooth}: $r \le B$ for every prime power $r \divides (p-1)$. That is, we assume that $p-1$ only has \Important{small factors} in its factorisation. For example, the number $2,000$ is $B$-smooth for any $B \ge 125$, since it factors as
\[
    2000 = 2^4 \times 5^3 = 16 \times 125.
\]

\begin{theorem}\label{th:pollard_p-1}
If $p$ is a prime divisor of $n$, and $r \le B$ for every prime power $r \divides (p-1)$, then Pollard's $p-1$ algorithm$(n,B)$ returns $d$, a nontrivial factor of $n$.
\end{theorem}


\begin{proof}
The key idea in the proof is that $p \divides a-1$. Let us prove this now.

At the end of the for loop, we have
\[
    a = 2^{B!} \mod n.
\]
Since $p \divides n$, we have
\[
    a = 2^{B!} \mod p.
\]
Since all the terms in the factorisation of $p-1$ belong to $\{1, 2, 3, \dots, B\}$, we must have
\[
    p-1 \divides 1 \times 2 \times 3 \times \dots \times B = B!.
\]
Say $B! = \gamma (p-1)$. By Fermat's theorem, $a^{p-1} = 1 \mod p$, hence
\[
    a = 2^{B!} \mod p = \left( 2^{p-1} \right)^\gamma \mod p = 1^\gamma \mod p = 1 \mod p.
\]
Thus $p \divides a-1$.

We can now finish the proof. Since $p \divides n$ and $p \divides a-1$, we have $p | d$, where $d = \gcd(a-1,n)$. By definition, $d \divides n$ and, unless $a = 1$, $1 < d < n$; that is, $d$ is a nontrivial divisor of $n$.
\end{proof}

The running time is clearly of the form $O(B \text{poly}(\log n))$. So if $B$ itself is $O(\text{poly}(\log n))$, the $p-1$ algorithm runs in polynomial time. However, it may not work: if $p-1$ has a factor greater than $B$, then the algorithm may return a failure. (Note that if $B = \sqrt{n}$, then the $p-1$ algorithm is sure to work, but that's no improvement compared to the naive algorithm.)

Pollard's $p-1$ algorithm can then be thwarted by constructed $n = pq$ without small factors as follows. First, choose a large prime $p_1$ such that $p = 2p_1 + 1$ is also prime, and similarly a large prime $q_1$ such that $q = 2q_1 + 1$ is also prime. (That task is not actually onerous.) Then the RSA modulus $n = pq$ will be resistant to factorisation using Pollard's $p-1$ algorithm.


\paragraph{Pollard's rho algorithm}

Let $p$ be the smallest prime divisor of $n$. Suppose there exist two integers $x, x' \in \Z_n$ such that $x \ne x'$ and $x = x' \mod p$. Then $p \le \gcd(x-x',n) < n$, hence the gcd is a nontrivial factor of $n$. To find such a collision, one can take a random set $X$ of size $|X| \approx 1.17 \sqrt{p}$: by the birthday paradox, we would find a collision with probability $50\%$. However, we would need to compute $\Theta(|X|^2) = \Theta(p) = \Theta(\sqrt{n})$ gcds.

The \Define{Pollard rho} algorithm is a clever heuristic to find collisions without that many gcds.Let $f(x)$ be a polynomial with integer coefficients. A popular choice is
\[
    f(x) = x^2 + 1.
\]
We will presume that the mapping $x \mapsto f(x) \mod p$ behaves like a random mapping. That way we can start with a particular value of $x$, say $x_0$ and search for a collision on its \Important{orbit} $X = \{x_0, x_1, \dots, x_k\}$, where
\[
    x_1 = f(x_0) \mod n, x_2 = f(x_1) = f^2(x_0) \mod n, \dots, x_k = f^k(x_0) \mod n.
\]

There must be a collision somewhere on the orbit. Indeed, the mapping $x \mapsto f(x) \mod n$ is a self-map of $\Z_n$, so there are $i$ and $j$ such that $f^i(x_0) \mod n = f^j(x_0) \mod n$. Therefore, there exist $\lambda$ and $\pi$ such that
\[
    x_\lambda = x_{\lambda + \pi} \mod p.
\]
We say $\lambda$ is the \Define{pre-period} of $x_0$ and $\pi$ is its \Define{period}. If we draw the directed graph $D = (X \mod p, A)$ with arcs $A = \{ (x_i \mod p, x_{i+1} \mod p) \}$, we get a rho-like shape, hence the name of the algorithm.

Let $i'$ satisfy $i' \ge i$ and $i' = 0 \mod \pi$, then
\[
    x_{2i'} = x_{i'} \mod p.
\]
Consider the sequences $B = (x_0, x_1, x_2, \dots)$ and $C = (x_0, x_2, x_4, \dots)$. We then have $b_{i'} = x_{i'}$ and $c_{i'} = x_{2i'} = b_{i'} \mod p$: we can find a collision simply by following those two sequences. We expect $i'$ to be at most $\sqrt{p}$.

\begin{algorithm}[H]
\caption{Pollard $\rho$ factoring algorithm$(n,x_1)$}
\begin{algorithmic}[1]
	\State{$x \gets x_0$}
	\State{$x' \gets f(x) \mod n$}
	\State{$p \gets \gcd(x-x', n)$}
	\While{$p=1$}
	    \State{$x \gets f(x) \mod n$}
	    \State{$x' \gets f^2(x') \mod n$}
	    \State{$p \gets \gcd(x-x',n)$}
    \EndWhile
    \If{$p < n$}
        \State{\Return{$p$}}
    \Else{}
        \State{\Return{"failure"}}
    \EndIf
\end{algorithmic}
\end{algorithm}

Note that there is a small chance of failure if the collision $x = x' \mod p$ actually satisfies $x = x'$. This occurs with probability roughly $p/n$, which is very small in practice. And in any case, if this occurs, then one can simply re-run the algorithm with a different initial point $x_0$.

\paragraph{Quadratic sieve algorithm}

Given $x,y$ with \Important{$x^2 = y^2 \mod n$} and $x \ne \pm y \mod n$, we obtain
\[
    0 = x^2 - y^2 = (x-y)(x+y) \mod n,
\]
and hence $n | (x-y)(x+y)$. But $n$ neither divides $x+y$ nor $x-y$, thus $\gcd(x-y,n)$ is one of the prime factors of $n$.

The \Define{quadratic sieve} algorithm tries to generate such a pair $x$, $y$. The key idea is to select a sequence of elements $x_i \in \Z_n$ and generate the corresponding sequence of values
\[
    q_i = x_i^2 \mod n.
\]
We choose $x_i = \left\lceil \sqrt{n} \right\rceil + i$ (or some choose a double-indexed sequence $x_{i,j} = \left\lceil \sqrt{jn} \right\rceil + i$) because then $x_i^2 \mod n$ is a small number. In fact, we select a bound $B$ and we only keep the $q_i$'s that are $B$-smooth, so we can factor them efficiently.

Let $p_1, \dots, p_k$ denote the $k$ prime numbers less than or equal to $B$. We then obtain
\begin{align*}
    q_1 &= x_1^2 \mod n = \prod_{j=1}^k p_j^{e_{1,j}}\\
    q_2 &= x_2^2 \mod n = \prod_{j=1}^k p_1^{e_{2,j}}\\
    \vdots\\
    q_l &= x_l^2 \mod n = \prod_{j=1}^k p_j^{e_{l,j}}.
\end{align*}


The next step is to find a subset of the $\{q_i\}$ whose product is a square (in $\N$). If we take the subset $S$, the product is
\[
    z = \prod_{i \in S} q_i = \prod_{j=1}^k p_j^{\sum_{i \in S} e_{i,j}}.
\]
The number $z$ is a square if and only if $\sum_{i \in S} e_{i,j}$ is an even number for all $j$. Finding an $S$ that satisfies this property is actually easy! (This is based on linear algebra over the binary field $\GF(2)$, we'll look at that stuff in Lecture \ref{sec:23}.) Suppose we find the right $S$, then we obtain
\[
    z = \left( \prod_{i \in S} x_i  \right)^2 \mod n.
\]
This gives us two square roots modulo $n$ of $z$: $x^2 = y^2 = z \mod n$. This pair of square roots may not be valid (we may have $x = \pm y \mod n$), but if we use $l > k+1$, we ensure to have multiple sets $S$ and hence multiple pairs of square roots; heuristically this will yield at least one valid pair.

The running time of the quadratic sieve algorithm depends on $B$. A careful analysis of the number of $B$-smooth numbers yields a subexponential $B = O(c^{\sqrt{\log n \log \log n}})$ and an overall \Important{subexponential running time} of
\[
    O\left( \exp( (1 + o(1)) \sqrt{ \ln n \ln \ln n } ) \right).
\]

\paragraph{State-of-the-art}
The current best algorithm for factoring is the so-called Galois Field Number Sieve. It recently (February 2020) managed to factor the number RSA-250: a 250-digit (829 bits) number $n=pq$. In fact, RSA has a challenge with a list of semiprimes ($n= pq$) of increasing length that people try to factor. The list goes up to a number that's 2048 bits long.


\subsection{See further}

\paragraph{Side-channel attacks}
Another type of attack on RSA and similar systems was discovered by Paul Kocher in 1995. He showed that it is possible to discover the decryption exponent by carefully timing the computation times for a series of decryptions (see \cite[Section 6.2]{TW02} for a review). Even though Kocher's \Important{timing attack} can be thwarted, it was nonetheless a game changer as it opened the way for a new range of so-called \Important{side-channel attacks}, whereby the attacker has access to other kinds of data such as timing, power consumption, memory usage, heat diffusion, etc.


\paragraph{Schor's algorithm}
Integer factorisation can be done in polynomial time... on a quantum computer. \Important{Schor's algorithm} is beyond the scope of this course. However, the threat of quantum computers is taken very seriously. Indeed, NIST are holding a competition to standardise \Important{Post-Quantum Cryptography} (\Define{PQC}), i.e. public-key cryptosystems and signature schemes based on problems which are believed to be secure against quantum attacks. There are two main kinds of post-quantum cryptosystems left: those based on \Important{lattice problems} (Lectures \ref{sec:19} and \ref{sec:20}) and those based on \Important{error-correcting codes} (Lectures \ref{sec:29} and \ref{sec:30}).

\subsection{Exercises}

\begin{exercise}
Apply Pollard's $p-1$ algorithm for $n = 15770708441$ and $B = 175$ to obtain
\[
    n = 135979 \times 115979.
\]
(Yes, you need a computer for this one.)
\end{exercise}


\begin{exercise}
Let $n = 7171$. With $x_0 = 1$, use Pollard's rho algorithm to find the factor $71$ of $n$. (You should get $\gcd(x_{11} - x_{22}, n) = 71$.)
\end{exercise}









\section{RSA I: The system}
\label{sec:14}


\subsection{Public-key cryptography}

\paragraph{One-way functions}
A function $f$ is said to be a \Define{one-way function} if:
\begin{enumerate}
    \item For all inputs $x$ it is easy to compute $f(x)$.

    \item Given some (random) $y$, it is hard to find an $x$ such that $y = f(x)$.
\end{enumerate}

By easy, we usually mean that it may be computed in time polynomial in $\log x$. Hardness is not so easy to define. At first, you may want inverting $f$ to be an NP-hard problem. But note that NP-hardness is a worst case criterion; random cases, or the majority of cases may still be easy (Knapsack encryption suffers from that drawback). Also, hardness is time dependent: what was in feasible in 1977 is relatively easy today. The minimum requirement for hardness is that there is no known efficient algorithm that works on random instances.




\paragraph{One-way functions}

We will look at two candidate one-way functions: multiplication and exponentiation modulo a prime. In this lecture, we focus on multiplication.

\begin{description}
\item [Multiplication] Given two integers $a$, $b$, it is easy (polynomial time) to compute the product $ab$.

\item[Inverting (factoring)] Given a large integer $c$, is it difficult to find integers $a$, $b$ such that $ab = c$?
\end{description}

Not in general. If you pick $a$, $b$ randomly then each is even with probability $1/2$. So $c = a \cdot b$ is even with probability $3/4$. You can often return $c = 2 \cdot (c/2)$.
Better to pick two $k$ bit primes, $a$, $b$. Now it is usually hard to invert.
The decision problem (Does $c$ have a factor less than $m$?) is not thought to be in P. It is in both NP and co-NP (given a prime factorisation of $c$ you can check both membership and non-membership). The best known algorithm (general number field sieve), for an $n$ bit number runs in time
\[
    \exp \{ (1.92 + o(1)) (\ln n)^{1/3} (\ln \ln n)^{2/3} \}.
\]





\paragraph{Public-key cryptography}
A \Define{trapdoor function} is a one-way function that can be easily inverted with a secret key. A public-key cryptosystem uses a trapdoor function as follows:
\begin{enumerate}
    \item Alice encrypts her message $M$, using a one-way function $f$: $C = f(M)$.

    \item Bob has a secret method of opening the trapdoor and inverting $f$.

    \item Even if Eve intercepts $f(M)$, she cannot invert without knowing Bob’s secret trapdoor.
\end{enumerate}
Trapdoor functions were first proposed by James Ellis at GCHQ in 1969. The first public key cryptosystem was developed by Clifford Cocks, also at GCHQ, in 1973. Both developments remained secret until 1997.



\paragraph{RSA cryptosystem}
Rivest, Shamir and Adleman rediscovered, and published, Cocks’ scheme in 1977.
It relies on the difficulty of factoring for security.

\begin{description}
\item[Key generation]~
\begin{enumerate}
    \item Bob chooses two large primes $p$ and $q$.

    \item Bob computes the public modulus $n = pq$, and the public exponent $e$ co-prime to $\phi(n) = (p-1)(q-1)$. ($e< \phi(n)$.)

    \item Bob publishes his public key $(n,e)$.

    \item His private key is the unique integer $d$ such that $ed = 1 \mod \phi(n)$.
\end{enumerate}

\item [Encryption]~
\begin{enumerate}
    \item Alice breaks her message into blocks so that each block $M$ satisfies $M<n$.

    \item Alice encrypts $M$ as $C = M^e \mod n$.
\end{enumerate}

\item[Decryption]~
\begin{enumerate}
    \item Bob can decrypt using $C^d = M \mod n$.
\end{enumerate}
\end{description}


\paragraph{Proof that RSA works}
The correctness of decryption is nontrivial. It follows from this simple number theoretic result, which can be viewed as a simple case of Lagrange's theorem on the order of subgroups of a finite group.

\begin{theorem}
For any integer $n$ and any $a \in \Z_n^*$,
\[
    a^{\phi(n)} = 1 \mod n.
\]
\end{theorem}
Therefore, the decryption is correct:
\[
    C^d = M^{ed} = M^{ed + k \phi(n)} = M \mod n.
\]


\paragraph{Computing time of RSA}
All the operations (key generation, encryption and decryption) are done in expected polynomial time.

Primality testing is very fast using randomised algorithms, and possible using a deterministic polynomial time algorithm. If we select random odd $k$-bit numbers, we expect to test polynomially many before we find a prime.

Computing $n = pq$ and $\phi(n) = (p-1)(q-1)$ is simple multiplication.

Finding $e$ by random sampling and testing (Euclid’s Algorithm), we expect to take polynomially many tests. We can also pick $e$ to be a prime number, then we just need to check it does not divide $\phi(n)$. In practice, we often use the prime number
\[
    e = 65537 = 2^{16}+1.
\]
Finding $d$ is again \Important{Euclid’s Algorithm}. Encryption and decryption can be achieved by \Important{repeated squaring}.



\paragraph{Generating random prime numbers}
In practice, one uses the \Important{Miller-Rabin primality test}. It is a yes-biased Monte Carlo algorithm, that is it always give an answer, and never returns ``$n$ is composite'' when $n$ is actually prime. The algorithm runs very fast. The probability of getting it wrong is exponentially small.



\paragraph{TLS}
Amongst the many applications of RSA, let us mention Transport Layer Security (\Define{TLS}). This is the newer version of SSL, used as the protocol for secure websites (https). Here is a massively reduced summary:
\begin{enumerate}
	\item Client contacts server and they agree to use the highest level cryptography suite they both know.

	\item The client generates a random number and communicates it to the server using public key cryptography (RSA) and the server’s public key.

	\item The server and client both compute a secret master key from the random number.

	\item The master key is used to communicate using symmetric cryptography (AES).
\end{enumerate}
Why? Because symmetric cryptography is about 1000 times faster than RSA.




\subsection{See further}

\paragraph{The Rabin cryptosystem}
RSA is not the only publick-key cryptosystem based on the complexity of factoring. Another example is the \Define{Rabin cryptosystem}, described below.
\begin{description}
    \item[Key generation] Let $n = pq$, where $p$ and $q$ are primes and $p,q = 3 \mod 4$. Then $n$ is the public key, while $(p,q)$ is the private key.

    \item[Encryption] Let $x \in \Z_n^*$, then
    \[
        e(x) = x^2 \mod n.
    \]

    \item[Decryption] Let $y \in \Z_n^*$, then
    \[
        d(x) = \sqrt{y} \mod n.
    \]
\end{description}
(The restriction $p,q = 3 \mod n$ can actually be omitted.)

The one main drawback of the Rabin cryptosystem is that encryption is not injective. Indeed, any ciphertext $y$ has four possible square roots, and hence corresponds to four possible plaintexts! For instance, let $p = 7$, $q = 11$, $y = 23$. Then a bit of algebra shows that
\[
    10^2, 67^2, 32^2, 45^2 = 23 \mod 77.
\]

\subsection{Exercises}

\begin{exercise} \label{exercise:rsa}
Alice and Bob decide to communicate using RSA. Bob picks primes $p = 5$ and $q = 13$, and public exponent $e = 19$.
\begin{enumerate}
    \item What information does Bob publish as his public key? Give the speciﬁc values from this example.

    \item What information comprises Bob’s private key? Compute it in this example.

    \item What is the process for Alice to send a message to Bob? Alice first coverts each character into a number  (as given in the character - number conversion table attached). She then encrypts each number using Bob's public information, and converts it back to a character to send to Bob. Illustrate how she would encrypt the message "Hell".

    \item Explain the process for Bob to decrypt messages he receives and decrypt the message ‘:Uo’.

    \item What wou


\section{RSA I: The system}
\label{sec:14}


\subsection{Public-key cryptography}

\paragraph{One-way functions}
A function $f$ is said to be a \Define{one-way function} if:
\begin{enumerate}
    \item For all inputs $x$ it is easy to compute $f(x)$.

    \item Given some (random) $y$, it is hard to find an $x$ such that $y = f(x)$.
\end{enumerate}

By easy, we usually mean that it may be computed in time polynomial in $\log x$. Hardness is not so easy to define. At first, you may want inverting $f$ to be an NP-hard problem. But note that NP-hardness is a worst case criterion; random cases, or the majority of cases may still be easy (Knapsack encryption suffers from that drawback). Also, hardness is time dependent: what was in feasible in 1977 is relatively easy today. The minimum requirement for hardness is that there is no known efficient algorithm that works on random instances.




\paragraph{One-way functions}

We will look at two candidate one-way functions: multiplication and exponentiation modulo a prime. In this lecture, we focus on multiplication.

\begin{description}
\item [Multiplication] Given two integers $a$, $b$, it is easy (polynomial time) to compute the product $ab$.

\item[Inverting (factoring)] Given a large integer $c$, is it difficult to find integers $a$, $b$ such that $ab = c$?
\end{description}

Not in general. If you pick $a$, $b$ randomly then each is even with probability $1/2$. So $c = a \cdot b$ is even with probability $3/4$. You can often return $c = 2 \cdot (c/2)$.
Better to pick two $k$ bit primes, $a$, $b$. Now it is usually hard to invert.
The decision problem (Does $c$ have a factor less than $m$?) is not thought to be in P. It is in both NP and co-NP (given a prime factorisation of $c$ you can check both membership and non-membership). The best known algorithm (general number field sieve), for an $n$ bit number runs in time
\[
    \exp \{ (1.92 + o(1)) (\ln n)^{1/3} (\ln \ln n)^{2/3} \}.
\]





\paragraph{Public-key cryptography}
A \Define{trapdoor function} is a one-way function that can be easily inverted with a secret key. A public-key cryptosystem uses a trapdoor function as follows:
\begin{enumerate}
    \item Alice encrypts her message $M$, using a one-way function $f$: $C = f(M)$.

    \item Bob has a secret method of opening the trapdoor and inverting $f$.

    \item Even if Eve intercepts $f(M)$, she cannot invert without knowing Bob’s secret trapdoor.
\end{enumerate}
Trapdoor functions were first proposed by James Ellis at GCHQ in 1969. The first public key cryptosystem was developed by Clifford Cocks, also at GCHQ, in 1973. Both developments remained secret until 1997.



\paragraph{RSA cryptosystem}
Rivest, Shamir and Adleman rediscovered, and published, Cocks’ scheme in 1977.
It relies on the difficulty of factoring for security.

\begin{description}
\item[Key generation]~
\begin{enumerate}
    \item Bob chooses two large primes $p$ and $q$.

    \item Bob computes the public modulus $n = pq$, and the public exponent $e$ co-prime to $\phi(n) = (p-1)(q-1)$. ($e< \phi(n)$.)

    \item Bob publishes his public key $(n,e)$.

    \item His private key is the unique integer $d$ such that $ed = 1 \mod \phi(n)$.
\end{enumerate}

\item [Encryption]~
\begin{enumerate}
    \item Alice breaks her message into blocks so that each block $M$ satisfies $M<n$.

    \item Alice encrypts $M$ as $C = M^e \mod n$.
\end{enumerate}

\item[Decryption]~
\begin{enumerate}
    \item Bob can decrypt using $C^d = M \mod n$.
\end{enumerate}
\end{description}


\paragraph{Proof that RSA works}
The correctness of decryption is nontrivial. It follows from this simple number theoretic result, which can be viewed as a simple case of Lagrange's theorem on the order of subgroups of a finite group.

\begin{theorem}
For any integer $n$ and any $a \in \Z_n^*$,
\[
    a^{\phi(n)} = 1 \mod n.
\]
\end{theorem}
Therefore, the decryption is correct:
\[
    C^d = M^{ed} = M^{ed + k \phi(n)} = M \mod n.
\]


\paragraph{Computing time of RSA}
All the operations (key generation, encryption and decryption) are done in expected polynomial time.

Primality testing is very fast using randomised algorithms, and possible using a deterministic polynomial time algorithm. If we select random odd $k$-bit numbers, we expect to test polynomially many before we find a prime.

Computing $n = pq$ and $\phi(n) = (p-1)(q-1)$ is simple multiplication.

Finding $e$ by random sampling and testing (Euclid’s Algorithm), we expect to take polynomially many tests. We can also pick $e$ to be a prime number, then we just need to check it does not divide $\phi(n)$. In practice, we often use the prime number
\[
    e = 65537 = 2^{16}+1.
\]
Finding $d$ is again \Important{Euclid’s Algorithm}. Encryption and decryption can be achieved by \Important{repeated squaring}.



\paragraph{Generating random prime numbers}
In practice, one uses the \Important{Miller-Rabin primality test}. It is a yes-biased Monte Carlo algorithm, that is it always give an answer, and never returns ``$n$ is composite'' when $n$ is actually prime. The algorithm runs very fast. The probability of getting it wrong is exponentially small.



\paragraph{TLS}
Amongst the many applications of RSA, let us mention Transport Layer Security (\Define{TLS}). This is the newer version of SSL, used as the protocol for secure websites (https). Here is a massively reduced summary:
\begin{enumerate}
	\item Client contacts server and they agree to use the highest level cryptography suite they both know.

	\item The client generates a random number and communicates it to the server using public key cryptography (RSA) and the server’s public key.

	\item The server and client both compute a secret master key from the random number.

	\item The master key is used to communicate using symmetric cryptography (AES).
\end{enumerate}
Why? Because symmetric cryptography is about 1000 times faster than RSA.




\subsection{See further}

\paragraph{The Rabin cryptosystem}
RSA is not the only publick-key cryptosystem based on the complexity of factoring. Another example is the \Define{Rabin cryptosystem}, described below.
\begin{description}
    \item[Key generation] Let $n = pq$, where $p$ and $q$ are primes and $p,q = 3 \mod 4$. Then $n$ is the public key, while $(p,q)$ is the private key.

    \item[Encryption] Let $x \in \Z_n^*$, then
    \[
        e(x) = x^2 \mod n.
    \]

    \item[Decryption] Let $y \in \Z_n^*$, then
    \[
        d(x) = \sqrt{y} \mod n.
    \]
\end{description}
(The restriction $p,q = 3 \mod n$ can actually be omitted.)

The one main drawback of the Rabin cryptosystem is that encryption is not injective. Indeed, any ciphertext $y$ has four possible square roots, and hence corresponds to four possible plaintexts! For instance, let $p = 7$, $q = 11$, $y = 23$. Then a bit of algebra shows that
\[
    10^2, 67^2, 32^2, 45^2 = 23 \mod 77.
\]

\subsection{Exercises}

\begin{exercise} \label{exercise:rsa}
Alice and Bob decide to communicate using RSA. Bob picks primes $p = 5$ and $q = 13$, and public exponent $e = 19$.
\begin{enumerate}
    \item What information does Bob publish as his public key? Give the speciﬁc values from this example.

    \item What information comprises Bob’s private key? Compute it in this example.

    \item What is the process for Alice to send a message to Bob? Alice first coverts each character into a number  (as given in the character - number conversion table attached). She then encrypts each number using Bob's public information, and converts it back to a character to send to Bob. Illustrate how she would encrypt the message "Hell".

    \item Explain the process for Bob to decrypt messages he receives and decrypt the message ‘:Uo’.

    \item What would the encryption of the numbers 0, 1 and 64 be? Why aren't they used in the character conversion table?
\end{enumerate}


\begin{tabular}{cccccccccccccccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19\\
- & - & \textvisiblespace & \return & A & B & C & D & E & F & G & H & I & J & K & L & M & N & O & P\\
20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32 & 33 & 34 & 35 & 36 & 37 & 38 & 39\\
Q & R & S & T & U & V & W & X & Y & Z & a & b & c & d & e & f & g & h & i & j \\
40 & 41 & 42 & 43 & 44 & 45 & 46 & 47 & 48 & 49 & 50 & 51 & 52 & 53 & 54 & 55 & 56 & 57 & 58 & 59\\
k & l & m & n & o & p & q & r & s & t & u & v & w & x & y & z & . & , & ? & !\\
60 & 61 & 62 & 63 & 64\\
: & ; & ' & \tab & -
\end{tabular}
\end{exercise}
ld the encryption of the numbers 0, 1 and 64 be? Why aren't they used in the character conversion table?
\end{enumerate}


\begin{tabular}{cccccccccccccccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19\\
- & - & \textvisiblespace & \return & A & B & C & D & E & F & G & H & I & J & K & L & M & N & O & P\\
20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32 & 33 & 34 & 35 & 36 & 37 & 38 & 39\\
Q & R & S & T & U & V & W & X & Y & Z & a & b & c & d & e & f & g & h & i & j \\
40 & 41 & 42 & 43 & 44 & 45 & 46 & 47 & 48 & 49 & 50 & 51 & 52 & 53 & 54 & 55 & 56 & 57 & 58 & 59\\
k & l & m & n & o & p & q & r & s & t & u & v & w & x & y & z & . & , & ? & !\\
60 & 61 & 62 & 63 & 64\\
: & ; & ' & \tab & -
\end{tabular}
\end{exercise}









\section{RSA I: The system}
\label{sec:14}


\subsection{Public-key cryptography}

\paragraph{One-way functions}
A function $f$ is said to be a \Define{one-way function} if:
\begin{enumerate}
    \item For all inputs $x$ it is easy to compute $f(x)$.

    \item Given some (random) $y$, it is hard to find an $x$ such that $y = f(x)$.
\end{enumerate}

By easy, we usually mean that it may be computed in time polynomial in $\log x$. Hardness is not so easy to define. At first, you may want inverting $f$ to be an NP-hard problem. But note that NP-hardness is a worst case criterion; random cases, or the majority of cases may still be easy (Knapsack encryption suffers from that drawback). Also, hardness is time dependent: what was in feasible in 1977 is relatively easy today. The minimum requirement for hardness is that there is no known efficient algorithm that works on random instances.




\paragraph{One-way functions}

We will look at two candidate one-way functions: multiplication and exponentiation modulo a prime. In this lecture, we focus on multiplication.

\begin{description}
\item [Multiplication] Given two integers $a$, $b$, it is easy (polynomial time) to compute the product $ab$.

\item[Inverting (factoring)] Given a large integer $c$, is it difficult to find integers $a$, $b$ such that $ab = c$?
\end{description}

Not in general. If you pick $a$, $b$ randomly then each is even with probability $1/2$. So $c = a \cdot b$ is even with probability $3/4$. You can often return $c = 2 \cdot (c/2)$.
Better to pick two $k$ bit primes, $a$, $b$. Now it is usually hard to invert.
The decision problem (Does $c$ have a factor less than $m$?) is not thought to be in P. It is in both NP and co-NP (given a prime factorisation of $c$ you can check both membership and non-membership). The best known algorithm (general number field sieve), for an $n$ bit number runs in time
\[
    \exp \{ (1.92 + o(1)) (\ln n)^{1/3} (\ln \ln n)^{2/3} \}.
\]





\paragraph{Public-key cryptography}
A \Define{trapdoor function} is a one-way function that can be easily inverted with a secret key. A public-key cryptosystem uses a trapdoor function as follows:
\begin{enumerate}
    \item Alice encrypts her message $M$, using a one-way function $f$: $C = f(M)$.

    \item Bob has a secret method of opening the trapdoor and inverting $f$.

    \item Even if Eve intercepts $f(M)$, she cannot invert without knowing Bob’s secret trapdoor.
\end{enumerate}
Trapdoor functions were first proposed by James Ellis at GCHQ in 1969. The first public key cryptosystem was developed by Clifford Cocks, also at GCHQ, in 1973. Both developments remained secret until 1997.



\paragraph{RSA cryptosystem}
Rivest, Shamir and Adleman rediscovered, and published, Cocks’ scheme in 1977.
It relies on the difficulty of factoring for security.

\begin{description}
\item[Key generation]~
\begin{enumerate}
    \item Bob chooses two large primes $p$ and $q$.

    \item Bob computes the public modulus $n = pq$, and the public exponent $e$ co-prime to $\phi(n) = (p-1)(q-1)$. ($e< \phi(n)$.)

    \item Bob publishes his public key $(n,e)$.

    \item His private key is the unique integer $d$ such that $ed = 1 \mod \phi(n)$.
\end{enumerate}

\item [Encryption]~
\begin{enumerate}
    \item Alice breaks her message into blocks so that each block $M$ satisfies $M<n$.

    \item Alice encrypts $M$ as $C = M^e \mod n$.
\end{enumerate}

\item[Decryption]~
\begin{enumerate}
    \item Bob can decrypt using $C^d = M \mod n$.
\end{enumerate}
\end{description}


\paragraph{Proof that RSA works}
The correctness of decryption is nontrivial. It follows from this simple number theoretic result, which can be viewed as a simple case of Lagrange's theorem on the order of subgroups of a finite group.

\begin{theorem}
For any integer $n$ and any $a \in \Z_n^*$,
\[
    a^{\phi(n)} = 1 \mod n.
\]
\end{theorem}
Therefore, the decryption is correct:
\[
    C^d = M^{ed} = M^{ed + k \phi(n)} = M \mod n.
\]


\paragraph{Computing time of RSA}
All the operations (key generation, encryption and decryption) are done in expected polynomial time.

Primality testing is very fast using randomised algorithms, and possible using a deterministic polynomial time algorithm. If we select random odd $k$-bit numbers, we expect to test polynomially many before we find a prime.

Computing $n = pq$ and $\phi(n) = (p-1)(q-1)$ is simple multiplication.

Finding $e$ by random sampling and testing (Euclid’s Algorithm), we expect to take polynomially many tests. We can also pick $e$ to be a prime number, then we just need to check it does not divide $\phi(n)$. In practice, we often use the prime number
\[
    e = 65537 = 2^{16}+1.
\]
Finding $d$ is again \Important{Euclid’s Algorithm}. Encryption and decryption can be achieved by \Important{repeated squaring}.



\paragraph{Generating random prime numbers}
In practice, one uses the \Important{Miller-Rabin primality test}. It is a yes-biased Monte Carlo algorithm, that is it always give an answer, and never returns ``$n$ is composite'' when $n$ is actually prime. The algorithm runs very fast. The probability of getting it wrong is exponentially small.



\paragraph{TLS}
Amongst the many applications of RSA, let us mention Transport Layer Security (\Define{TLS}). This is the newer version of SSL, used as the protocol for secure websites (https). Here is a massively reduced summary:
\begin{enumerate}
	\item Client contacts server and they agree to use the highest level cryptography suite they both know.

	\item The client generates a random number and communicates it to the server using public key cryptography (RSA) and the server’s public key.

	\item The server and client both compute a secret master key from the random number.

	\item The master key is used to communicate using symmetric cryptography (AES).
\end{enumerate}
Why? Because symmetric cryptography is about 1000 times faster than RSA.




\subsection{See further}

\paragraph{The Rabin cryptosystem}
RSA is not the only publick-key cryptosystem based on the complexity of factoring. Another example is the \Define{Rabin cryptosystem}, described below.
\begin{description}
    \item[Key generation] Let $n = pq$, where $p$ and $q$ are primes and $p,q = 3 \mod 4$. Then $n$ is the public key, while $(p,q)$ is the private key.

    \item[Encryption] Let $x \in \Z_n^*$, then
    \[
        e(x) = x^2 \mod n.
    \]

    \item[Decryption] Let $y \in \Z_n^*$, then
    \[
        d(x) = \sqrt{y} \mod n.
    \]
\end{description}
(The restriction $p,q = 3 \mod n$ can actually be omitted.)

The one main drawback of the Rabin cryptosystem is that encryption is not injective. Indeed, any ciphertext $y$ has four possible square roots, and hence corresponds to four possible plaintexts! For instance, let $p = 7$, $q = 11$, $y = 23$. Then a bit of algebra shows that
\[
    10^2, 67^2, 32^2, 45^2 = 23 \mod 77.
\]

\subsection{Exercises}

\begin{exercise} \label{exercise:rsa}
Alice and Bob decide to communicate using RSA. Bob picks primes $p = 5$ and $q = 13$, and public exponent $e = 19$.
\begin{enumerate}
    \item What information does Bob publish as his public key? Give the speciﬁc values from this example.

    \item What information comprises Bob’s private key? Compute it in this example.

    \item What is the process for Alice to send a message to Bob? Alice first coverts each character into a number  (as given in the character - number conversion table attached). She then encrypts each number using Bob's public information, and converts it back to a character to send to Bob. Illustrate how she would encrypt the message "Hell".

    \item Explain the process for Bob to decrypt messages he receives and decrypt the message ‘:Uo’.

    \item What would the encryption of the numbers 0, 1 and 64 be? Why aren't they used in the character conversion table?
\end{enumerate}


\begin{tabular}{cccccccccccccccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 & 18 & 19\\
- & - & \textvisiblespace & \return & A & B & C & D & E & F & G & H & I & J & K & L & M & N & O & P\\
20 & 21 & 22 & 23 & 24 & 25 & 26 & 27 & 28 & 29 & 30 & 31 & 32 & 33 & 34 & 35 & 36 & 37 & 38 & 39\\
Q & R & S & T & U & V & W & X & Y & Z & a & b & c & d & e & f & g & h & i & j \\
40 & 41 & 42 & 43 & 44 & 45 & 46 & 47 & 48 & 49 & 50 & 51 & 52 & 53 & 54 & 55 & 56 & 57 & 58 & 59\\
k & l & m & n & o & p & q & r & s & t & u & v & w & x & y & z & . & , & ? & !\\
60 & 61 & 62 & 63 & 64\\
: & ; & ' & \tab & -
\end{tabular}
\end{exercise}





\section{Symmetric cryptography II: Cryptanalysis}
\label{sec:13}


\subsection{Weaknesses of DES}


\paragraph{Attacking DES}
Even in 1977 the key size (56 bits) was considered ‘low’.
The NSA ‘encouraged’ the dropping of key size from 128 to 56 bits at the time - could they mount a brute force attack even then?
Proposed machines to break DES by brute force are:
\begin{itemize}
    \item 1977. Diffie and Hellman outline a \$20million machine to crack DES in 1 day.

    \item 1993. Wiener proposed a \$1million machine to crack DES in 7 hours.

    \item 1997. RSA corp. sponsored a competition to crack DES. It was won by a program using downtime of home PCs connected to the internet. 1 PC would have taken 2285 years to search the key space. They had 78,000 PCs signed up by the time they found the key, after searching 1/4 of the key space.

    \item 1998. The EFF built a \$250,000 machine that cracks DES in 2 days.

    \item 2006. A \$10,000 machine (COPACOBANA) is built from off-the-shelf kit that cracks DES in less than a week.
\end{itemize}



\paragraph{Key size}
If our key size is 56 bits, there are $2^{56}$ possible keys and testing randomly we would expect to test about $2^{55}$ before we cracked it. Suppose we can test a billion ($2^{30}$) possible keys per second. There are $2^{25}$ seconds per year, so we test about $2^{55}$ keys per year.

The amount of time required to break a 128-bit key is very much greater. About
\[
    2^{127} = 170,141,183,460,469,231,731,687,303,715,884,105,728
\]
possibilities must be checked. A device that checks a billion billion keys ($2^{60}$) per second would require about $10^{13}$ years to exhaust the key space. This is a thousand times longer than the age of the universe.

AES permits the use of $256$-bit keys. Breaking a symmetric $256$-bit key by brute force requires $2^{128}$ times more computational power than a $128$-bit key. A device that could check a billion billion ($10^{18}$) AES keys per second would require about $10^{51}$ years to exhaust the 256-bit key space.





\paragraph{TripleDES and DES-X}
Two main variants of DES were introduced to increase the key space/size.

\Define{TripleDES}: Take three DES keys $K_1$, $K_2$, $K_3$. Encrypt a block $M$ as
\[
    C = e( d( e(M, K_1), K_2), K_3),
\]
where $e$ is the standard DES encryption, and $d$ is the standard DES decryption. The key size is now $3 \times 56 = 168$ bits, but effective security is only ca. 112 bits due to meet-in-the-middle attacks. TripleDES is backwards compatible taking $K_1 = K_2 = K_3$.

\Define{DES-X}: Take a DES key $K$, and two supplementary 64 bit keys $K_1$, $K_2$. Encrypt a block $M$ as
\[
    C = K_2 + e( M + K_1, K ).
\]
The  Key size is now 184 bits, but effectively ca. 119 bits. Since only one DES encryption is performed, this is much faster than TripleDES.



\subsection{Cryptanalysis of block ciphers}

\paragraph{Meet-in-the-middle}
Suppose we are using a $k$-bit cryptosystem with encryption function $e$ and decryption function $d$. We decide to increase security by encrypting twice as follows.: choose two keys $K_1$ and $K_2$ each of $k$ bits, and then do
\[
    C = e( e(M,K_1) , K_2).
\]

The effective security is not $2k$ bits. Suppose a plaintext/ciphertext pair $M$, $C$ is publicly known. The attacker could compute a table of $e(M,K)$ for all $2^k$ possible keys $K$. Then she can compute $d(C,K')$ for all possible keys $K'$, checking for a match:
\[
    e(M,K) = d(C,K').
\]
This gives a pair of keys such that
\[
    e( e(M,K) , K') = C.
\]
The total encryptions/decryptions performed is $2^{k+1}$. The space required is only enough to store $2^k$ keys and text snippets.


\paragraph{Linear cryptanalysis}
Linear cryptanalysis is quite involved, so we only give the basic ideas.

Suppose that it is possible to find a probabilistic linear relationship between a subset of plaintext bits and a subset of state bits immediately after the substitutions performed in the last round, In other words, there exists a subset of bits whose exclusive-or behaves in a \Important{non-random fashion} (it takes the value $0$ with probability bounded away from $1/2$).

Now assume that an attacker has a large number of plaintext-ciphertext pairs for the same unknown key $K$ (i.e. this is a known-plaintext attack). For each of the plaintext-ciphertext pairs, we begin to decrypt the ciphertext, using all possible candidate keys for the last round of the cipher. We keep track of when the linear relationship does hold for each key. At the end of this process, we hope that the candidate key with the most biased count (furthest away from $1/2$ of the number of pairs) contains the correct value for these bits.

Performing a linear attack involves computing a linear approximation of the S-boxes. Therefore, a secure S-box must have a high \Important{nonlinearity}, defined as the distance to any linear or affine function.

\Define{Linear cryptanalysis} was introduced by Matsui in 1994. He actually performed a linear attack on DES the same year. The linear cryptanalysis of DES is a known-plaintext attack using $2^{43}$ plaintext-ciphertext pairs, all of which are encrypted using the same unknown key. This actual attack did not have any impact on the security of DES due to the large number of pairs required. Nonetheless, AES was designed to resit linear cryptanalysis.


\paragraph{Differential cryptanalysis}
Differential cryptanalysis is similar to linear cryptanalysis in many respects. The main difference is that it involves comparing the \Important{xor of two inputs} to the xor of the corresponding two outputs.

Differential cryptanalysis is a chosen-plaintext attack. The attacker has a large number of tuples $(x, x^*, y, y^*)$ (input, input, output, output), where the value $x' = x \oplus x^*$ is fixed and $x$ and $x^*$ are encrypted using the same key.

\Define{Differential cryptanalysis} was discovered in the late 1980s by Biham and Shamir. They noted that DES was resistant to differential cryptanalysis. In fact, IBM and the NSA knew about differential cryptanalysis since the mid-70s and hence designed DES to be robust against this sort of attack.



\subsection{See further}

\paragraph{Design of S-boxes}
Mathematically, an S-box is a function $\{0,1\}^n \to \{0,1\}^m$ for some $m$ and $n$. It can then be viewed as a tuple of $m$ Boolean functions $(s_1, \dots, s_m)$, where $s_i : \{0,1\}^n \to \{0,1\}$ is a Boolean function.

The security of the block cipher depends on the S-box, and hence on its component Boolean functions. As such, different parameters of Boolean functions have been introduced: nonlinearity, balancedness, resiliency, correlation immunity, algebraic degree, etc.





\subsection{Exercises}

\begin{exercise}
Perform a meet-in-the-middle attack on the simple Feistel cipher of Exercise \ref{exercise:feistel}. (You can choose the solution of that exercise as your publicly known plaintext-ciphertext pair.)
\end{exercise}


\section{Symmetric cryptography I: Block ciphers}
\label{sec:12}


An $n$-bit \Define{block cipher} is a function $e : \{0,1\}^n \times \mathcal{K} \to \{0,1\}^n$, such that for each
key $K \in \alphabet{K}$, $e(M,K)$ is an invertible permutation of $\{0,1\}^n$ (the encryption function for $K$).



\subsection{Modes of operations}

A block cipher is not a cryptosystem per se. It is only used to encrypt a particular block of data. It forms a cryptosystem when the mode of operation is specified.

\paragraph{Electronic Codebook}
The simplest of the encryption modes is the electronic codebook (\Define{ECB}) mode (named after conventional physical codebooks). The message is divided into blocks, and each block is encrypted separately.

\begin{center}
    \includegraphics[width=15cm]{Cryptography/ECB_encryption.png}
    \includegraphics[width=15cm]{Cryptography/ECB_decryption.png}
\end{center}


The disadvantage of this method is a lack of diffusion. Because ECB encrypts identical plaintext blocks into identical ciphertext blocks, it does not hide data patterns well Therefore, global patterns across the plaintext are still kept in the ciphertext. As such, ECB is not recommended for use in cryptographic protocols.


\begin{center}
    \includegraphics[width=5cm]{Cryptography/Tux.jpg} \hspace{2cm} \includegraphics[width=5cm]{Cryptography/Tux_ecb.jpg}
\end{center}

\paragraph{Cipher Block Chaining}
In the cipher block chaining (\Define{CBC}) mode of operation, each block of plaintext is XORed with the previous ciphertext block before being encrypted. This way, each ciphertext block depends on all plaintext blocks processed up to that point. To make each message unique, an initialization vector (\Important{IV}) must be used in the first block.


\begin{center}
    \includegraphics[width=15cm]{Cryptography/CBC_encryption.png}
    \includegraphics[width=15cm]{Cryptography/CBC_decryption.png}
\end{center}


\paragraph{Cipher Feedback}
The cipher feedback (\Define{CFB}) mode, in its simplest variation is using the entire output of the block cipher. In this variation, it is very similar to CBC, makes a block cipher into a self-synchronizing stream cipher.



\begin{center}
    \includegraphics[width=15cm]{Cryptography/CFB_encryption.png}
    \includegraphics[width=15cm]{Cryptography/CFB_decryption.png}
\end{center}












\subsection{Data Encryption Standard}

\paragraph{Feistel cipher}
A \Define{Feistel cipher} is an iterated cipher mapping a $2t$-bit plaintext $(L_0,R_0)$, for
$t$-bit blocks $L_0$ and $R_0$, to a ciphertext $(R_r, L_r)$, through an $r$-round process where $r \ge 1$.

For $1 \le i \le r$, round $i$ maps $(L_{i-1}, R_{i-1}), K_i \to (L_i,R_i)$ as follows:
\begin{align*}
    L_i &= R_{i-1},\\
    R_i &= L_{i-1} \oplus f(R_{i-1}, K_i),
\end{align*}
where each subkey $K_i$ is derived from the cipher key $K$.


Typically in a Feistel cipher, $r \ge 3$ and often is even. The Feistel structure specifically
orders the ciphertext output as $(R_r, L_r)$ rather than $(L_r,R_r)$; the blocks are exchanged
from their usual order after the last round. Decryption is thereby achieved using the same
$r$-round process but with subkeys used in reverse order, $K_r$ through $K_1$.

The $f$ function of the Feistel cipher need not be invertible to allow inversion of
the Feistel cipher. We illustrate that successive rounds of a Feistel cipher operate on alternating halves of the ciphertext, while the other remains constant.

\begin{center}
    \includegraphics[width=10cm]{Cryptography/feistel.png}
\end{center}

The Data Encryption Standard (\Important{DES}) is a Feistel cipher which processes plaintext blocks of $n = 64$ bits, producing $64$-bit ciphertext blocks.

\paragraph{DES key schedule}
The effective size of the secret key $K$ is $k = 56$ bits. More precisely, the input key $K$ is specified as a $64$-bit key, $8$ bits of which (bits $8, 16, \dots, 64$) are used as odd-parity bits:
\[
    k_8 = k_1 \oplus k_2 \oplus k_3 \oplus k_4 \oplus k_5 \oplus k_6 \oplus k_7 \oplus 1
\]
(and so on for $k_{16}, k_{24}, \dots, k_{64}$). The $2^{56}$ keys implement (at most) $2^{56}$ of the $2^{64}!$ possible bijections on $64$-bit blocks. A widely held belief is that the parity bits were introduced to reduce the effective key size from $64$ to $56$ bits, to intentionally reduce the cost of exhaustive key search by a factor of $256$.\\
~\\
Algorithm DES key schedule\\
INPUT: 64-bit key $K = k_1 \dots k_{64}$.\\
OUTPUT: sixteen $48$-bit keys $K_i$, $1 \le i \le 16$.\\
1. Define $v_i$, $1 \le i \le 16$ as follows: $v_i = 1$ for $i \in \{1, 2, 9, 16\}$; $v_i = 2$ otherwise.\\
2. $(C_0,D_0) \gets PC1(K)$. \\
3. For $i$ from $1$ to $16$, let $X^{\gets v}$ denote $X$ shifted cyclically to the left $v$ times, then do
\begin{align*}
    C_i &\gets C_{i-1}^{\gets v_i},\\
    D_i &\gets D_{i-1}^{\gets v_i},\\
    K_i &\gets PC2(C_i,D_i).
\end{align*}


\begin{tabular}{ccccccc}
\multicolumn{7}{c}{$PC1$}\\
57 & 49 & 41 & 33 & 25 & 17 & 9\\
1 & 58 & 50 & 42 & 34 & 26 & 18\\
10 & 2 & 59 & 51 & 43 & 35 & 27\\
19 & 11 & 3 & 60 & 52 & 44 & 36\\
\multicolumn{7}{c}{above for $C_i$; below for $D_i$}\\
63 & 55 & 47 & 39 & 31 & 23 & 15\\
7 & 62 & 54 & 46 & 38 & 30 & 22\\
14 & 6 & 61 & 53 & 45 & 37 & 29\\
21 & 13 & 5 & 28 & 20 & 12 & 4
\end{tabular}
\hfill
\begin{tabular}{cccccc}
\multicolumn{6}{c}{$PC2$}\\
14 & 17 & 11 & 24 & 1 & 5\\
3 & 28 & 15 & 6 & 21 & 10\\
23 & 19 & 12 & 4 & 26 & 8\\
16 & 7 & 27 & 20 & 13 & 2\\
41 & 52 & 31 & 37 & 47 & 55\\
30 & 40 & 51 & 45 & 33 & 48\\
44 & 49 & 39 & 56 & 34 & 53\\
46 & 42 & 50 & 36 & 29 & 32
\end{tabular}



\paragraph{Feistel function for DES}

Encryption proceeds in $16$ stages or rounds. From the input key $K$, sixteen $48$-bit
subkeys $K_i$ are generated, one for each round. Within each round, $8$ fixed, $6$-to-$4$ bit substitution mappings (S-boxes) $S_i$, collectively denoted $S$, are used. The $64$-bit
plaintext is divided into $32$-bit halves $L_0$ and $R_0$. Each round is functionally equivalent, taking $32$-bit inputs $L_{i-1}$ and $R_{i-1}$ from the previous round and producing $32$-bit outputs
$L_i$ and $R_i$ for $1 \le i \le 16$, as follows:
\begin{align*}
    L_i &= R_{i-1},\\
    R_i &= L_{i-1} \oplus f(R_{i-1}, K_i),
\end{align*}
where
\[
    f(R_{i-1}, K_i) = P( S( E( R_{i-1} ) \oplus K_i ) ).
\]
Here $E$ is a fixed expansion permutation mapping $R_{i-1}$ from $32$ to $48$ bits. $P$ is another fixed permutation on $32$ bits. The tables below give $E$ and $P$, that is
\begin{align*}
    E(r_1, r_2, r_3, \dots, r_{31}, r_{32}) &= (r_{32}, r_1, r_2, \dots, r_{32}, r_1)\\
    P(t_1, t_2, t_3, \dots, t_{31}, t_{32}) &= (t_{16}, t_7, t_{20}, \dots, t_4, t_{25}).
\end{align*}

\begin{tabular}{cccccc}
\multicolumn{6}{c}{$E$}\\
32 & 1 & 2 & 3 & 4 & 5\\
4 & 5 & 6 & 7 & 8 & 9\\
8 & 9 & 10 & 11 & 12 & 13\\
12 & 13 & 14 & 15 & 16 & 17\\
16 & 17 & 18 & 19 & 20 & 21\\
20 & 21 & 22 & 23 & 24 & 25\\
24 & 25 & 26 & 27 & 28 & 29\\
28 & 29 & 30 & 31 & 32 & 1
\end{tabular}
\hfill
\begin{tabular}{cccc}
\multicolumn{4}{c}{$P$}\\
16 & 7 & 20 & 21\\
29 & 12 & 28 & 17\\
1 & 15 & 23 & 26\\
5 & 18 & 31 & 10\\
2 & 8 & 24 & 14\\
32 & 27 & 3 & 9\\
19 & 13 & 30 & 6\\
22 & 11 & 4 & 25
\end{tabular}

The $S_i$ S-boxes are given in the table below. The \Important{S-boxes} are the only \Important{nonlinear} components of the encryption, and as such are the crucial component of its security. In DES, the S-boxes were introduced as lookup tables, with no indication of why they were chosen like that in the first place.

They are applied as follows. First, let $B_i = (b_1, b_2, b_3, b_4, b_5, b_6)$, then $\text{row} = 2b_1 + b_6$, $\text{column} = 8b_2 + 4b_3 + 2b_4 + b_5$. Then look-up the table and write the entry in binary (on four bits). Four instance, here is how to compute $S_1(011011)$:
\begin{align*}
    B &= 011011\\
    \text{row} &= 1\\
    \text{column} &= 13\\
    S_1[1,13] &= 5 = 0101.
\end{align*}

\begin{center}
    \includegraphics[width=15cm]{Cryptography/dess.png}
\end{center}



Here is how to compute the Feistel function $f$:
\begin{enumerate}
    \item Expand $R_{i-1} = r_1 r_2 \dots r_{32}$ from $32$ to $48$ bits using $E$:
$T \gets E(R_{i-1})$. (Thus $T = r_{32} r_1 r_2 \dots r_{32} r_1$.)

    \item $T \gets T \oplus K_i$. Represent $T$ as eight $6$-bit character strings: $(B_1, \dots ,B_8) = T$.

    \item $T \gets (S_1(B_1), S_2(B_2), \dots, S_8(B_8))$.

    \item $T \gets P(T)$.
\end{enumerate}

\begin{center}
    \includegraphics[width=10cm]{Cryptography/desf.png}
\end{center}





\paragraph{The whole DES algorithm}
An initial bit permutation ($IP$) precedes the first round; following the last round, the left and right halves are exchanged and, finally, the resulting string is bit-permuted by the inverse of $IP$, sometimes referred to as final bit permutation $FP = IP^{-1}$. Similarly to the tables above, we have
\begin{align*}
    IP(m_1, m_2, \dots, m_{64}) &= (m_{58}, m_{50}, \dots, m_7)\\
    FP(b_1, b_2, \dots, b_{64}) &= (b_{40}, b_8, \dots, b_{25}).
\end{align*}

\begin{tabular}{cccccccc}
\multicolumn{8}{c}{$IP$}\\
58 & 50 & 42 & 34 & 26 & 18 & 10 & 2\\
60 & 52 & 44 & 36 & 28 & 20 & 12 & 4\\
62 & 54 & 46 & 38 & 30 & 22 & 14 & 6\\
64 & 56 & 48 & 40 & 32 & 24 & 16 & 8\\
57 & 49 & 41 & 33 & 25 & 17 & 9 & 1\\
59 & 51 & 43 & 35 & 27 & 19 & 11 & 3\\
61 & 53 & 45 & 37 & 29 & 21 & 13 & 5\\
63 & 55 & 47 & 39 & 31 & 23 & 15 & 7
\end{tabular}
\hfill
\begin{tabular}{cccccccc}
\multicolumn{8}{c}{$FP$}\\
40 & 8 & 48 & 16 & 56 & 24 & 64 & 32\\
39 & 7 & 47 & 15 & 55 & 23 & 63 & 31\\
38 & 6 & 46 & 14 & 54 & 22 & 62 & 30\\
37 & 5 & 45 & 13 & 53 & 21 & 61 & 29\\
36 & 4 & 44 & 12 & 52 & 20 & 60 & 28\\
35 & 3 & 43 & 11 & 51 & 19 & 59 & 27\\
34 & 2 & 42 & 10 & 50 & 18 & 58 & 26\\
33 & 1 & 41 & 9 & 49 & 17 & 57 & 25
\end{tabular}
~\\
~\\
Algorithm Data Encryption Standard (DES)\\
INPUT: plaintext $M = (m_1, \dots, m_{64})$; $64$-bit key $K = (k_1, \dots, k_{64})$.\\
OUTPUT: $64$-bit ciphertext block $C = (c_1, \dots, c_{64})$. \\
1. Compute sixteen $48$-bit round keys $K_i$ from $K$.\\
2. $(L_0, R_0) \gets IP(M)$. \\
3. for $i$ from $1$ to $16$, compute $L_i$ and $R_i$ as above.\\
4. $B = (b_1, b_2, \dots, b_{64}) \gets (R_{16}, L_{16})$. \\
5. $C \gets FP(B)$.


Decryption involves the same key and algorithm, but with subkeys applied to the internal rounds in the reverse order.



\begin{center}
    \includegraphics[width=5cm]{Cryptography/des.png}
\end{center}





\subsection{See further}

\paragraph{Other modes of operation}
A few other modes of operation have been proposed. We can mention Output feedback (\Define{OFB}), similar to CFB, but where the output of the encryption block function (rather than the ciphertext) serves as the feedback. Another related mode of operation is Counter (\Define{CTR}), which encrypts successive values of a counter.


\paragraph{AES}
The Advanced Encryption Standard (\Define{AES}) was introduced in 2002 in order to replace DES as the new standard for block ciphers. Unlike DES, where some details were kept secret, AES was selected using a competition where all the algorithms had to explain all the details and design choices.

Overview of AES: 128 bit message block represented as a 4 by 4 array of bytes. It applies 10 rounds of:
\begin{itemize}
    \item S-box transformation
    \item ShiftRows
    \item MixColumns
    \item Add sub-key.
\end{itemize}
The first three operations are all fixed. The fourth one is the only use of the key. Again, the S-box substitutions are the key non-linear element. The rest helps diffuse uncertainty.



\subsection{Exercises}


\begin{exercise} \label{exercise:feistel}
By hand, encrypt the following plaintext $M = 01110001$ ($8$ bits) using a Feistel cipher with 4 rounds and function
\[
    f(x_1, x_2, x_3, x_4) = x_1x_2 + x_2x_3 + x_3x_4 + x_4x_1 \mod 2.
\]
For simplicity, we apply $f$ to $R_{i-1} \oplus K_i$, so that the actual Feistel function is
\[
f(r_1, r_2, r_3, r_4, k_1, k_2, k_3, k_4) = (r_1 + k_1)(r_2 + k_2) + (r_2 + k_2)(r_3 + k_3) + (r_3 + k_3)(r_4 + k_4)  + (r_4 + k_4) (r_1 + k_1) \mod 2.
\]
You can choose anything you want for the four $4$-bit subkeys $K_1, K_2, K_3, K_4$.

Decrypt the ciphertext and verify that you get the right plaintext.
\end{exercise}


\begin{exercise} \label{exercise:weak_keys}
A DES weak key is a key $K$ such that $E_K(E_K(x)) = x$ for all $x$, i.e., defining
an involution.
Verify that the four keys in the table below are indeed weak keys. I give $K$ in hexadecimal and the corresponding $C_0$ and $D_0$; note that the subkeys are easy to determine in those cases!\\
~\\
\begin{tabular}{lll}
weak key (hexadecimal) & $C_0$ & $D_0$\\
0101 0101 0101 0101 & $0\dots0$ & $0\dots0$\\
FEFE FEFE FEFE FEFE & $1\dots1$ & $1\dots1$\\
1F1F 1F1F 0E0E 0E0E & $0\dots0$ & $1\dots1$\\
E0E0 E0E0 F1F1 F1F1 & $1\dots1$ & $0\dots0$
\end{tabular}

Those actually are the only weak keys of DES.
\end{exercise}


\section{Introduction to cryptography}
\label{sec:11}


\subsection{Fundamentals of cryptography}

\paragraph{Basic set-up}

All messages are sent through an external channel:

\begin{center}
\begin{tikzpicture}
	%\draw (0,0) rectangle (4,4);

	\node                       (A)     at (-1,1)   {\Structure{Alice}};
	\node                       (B)     at (13,1)   {\Structure{Bob}};
	\node                       (E)     at (10,0)   {\Important{Eve}};

	\node 						(M)		at (0,1)	{$M$};
	\node[draw, left] 			(Enc)	at (3,1) 	{Encoder $e$};
	\node[draw] 				(K) 	at (5,2) 	{$K$};
	\node[draw, right] 			(Dec)	at (9,1) 	{Decoder $d$};
	\node 						(HM)	at (12,1) 	{$M$};
	\node[draw, right] 			(Eve)	at (7,0) 	{Eavesdropper};

	\draw[-latex] (M) 	-- (Enc);
	\draw[-latex] (Enc) -- (Dec) node[pos=0.5, above]{$C$};
	\draw[-latex] (Dec)	-- (HM);

	\draw[-latex] (K) -| (Enc);
	\draw[-latex] (K) -| (Dec);
	\draw[-latex] (6,1) |- (Eve);
\end{tikzpicture}
\end{center}

Now we are interested in protecting the contents of the message from eavesdroppers, known as Eve. The message sender, and encoder, is traditionally called Alice.
The receiver, and decoder, is traditionally called Bob.





\paragraph{Notation}

The message $M \in \alphabet{M}$ is known as the \Define{plaintext}. Alice and Bob have some secret information $K \in \alphabet{K}$, known as the \Define{key}.
Alice encrypts using an encryption function $e : \alphabet{M} \times \alphabet{K} \to \alphabet{C}$.
The transmitted sequence
\[
    C = e(M,K) \in \alphabet{C}
\]
is called the \Define{ciphertext}. Bob receives $C$, and decrypts using the decryption function $d : \alphabet{C} \times \alphabet{K} \to \alphabet{M}$. We require that
\[
    d( e(M,K), K ) = M
\]
for all plaintexts $M$. This implies that for a given key $K$, the encryption function is injective:
\[
    e(M_1, K) = e(M_2, K) \implies M_1 = M_2.
\]




\paragraph{Kerchoff's principles}

Clearly, if Eve knows both the decoding algorithm $d$ and the key $K$, then she can decipher the ciphertext. So at least one of the two should be kept secret. Auguste Kerchoffs argued that only the key should be kept secret:
\begin{quote}
    The cipher method must not be required to be secret, and it must be able to fall into the hands of the enemy without inconvenience.
\end{quote}
This is known as \Define{Kerchoff's principle}.

There are three main arguments supporting Kerchoff's principle:
\begin{enumerate}
    \item It is much easier to maintain secrecy of a short key than to keep secret the (more complex) algorithm they are using. (Imagine using encryption to secure the communication between all pairs of employees in some organisation.)

    \item If the shared, secret information is ever exposed, then it will be much easier to change a key than to replace an encryption scheme.

    \item For large-scale deployment, it is much easier for users to all rely on the same encryption algorithm or software (with different keys) than for everyone to use their own custom algorithm. In fact, it is desirable for encryption schemes to be standardise so that compatibility is ensured by default and users will use a scheme that has undergone public scrutiny without weaknesses being found.
\end{enumerate}

Kerchoff's principle leads us to use cryptosystems whose designs are completely public, instead of trying to rely on ``security by obscurity.''


\paragraph{Breaking a cryptosystem}
The enemy (Eve) is assumed to know the encryption function $e$, the decryption function $d$, and also to have various additional information, such as language statistics etc.
She will certainly have some ciphertext $C$.
All she lacks to decipher to message is the key $K$.

There are three standard forms of attack:
\begin{enumerate}
    \item Ciphertext-only attack.

    \item Known-plaintext attack.
Eve is assumed to also have a sample of plaintext and its encryption.

    \item Chosen-plaintext attack.
Eve is able to acquire chosen samples of plaintext/ciphertext pairs.
\end{enumerate}
Current cryptosystems are required to be secure against chosen-plaintext attacks.




\subsection{Examples of cryptosystems}

\paragraph{Early history}
Aeneas Tacticus has a chapter ``On secret messages'' in his classic military manual on the art of war (c. 360BC).
His most secret method was to thread a string through holes in a piece of bone in an order corresponding to the letters of the message.


In the \Important{Caesar cipher}, allegedly used by Julius Caesar, involves shifting all the letters of the alphabet a fixed number of letters later in the alphabet (wrapping from z round to a). For instance, shifting by four characters, ``\texttt{pinkfloyd}'' becomes ``\texttt{tmrojpsch}''.


\paragraph{Substitution cipher}

In the \Define{substitution cipher}, the key is a permutation of the letters of the alphabet:
\begin{align*}
          &\texttt{ABCDEFGHIJKLMNOPQRSTUVWXYZ}\\
       K = \ &\texttt{QWERTYUIOPASDFGHJKLZXCVBNM}
\end{align*}
Encryption is achieved by applying the permutation letter-wise. For instance, using the key above,
\begin{align*}
    M &= \texttt{SENDMORETROOPS}\\
    C &= \texttt{LTFRDGKTZKGGHL}
\end{align*}

This scheme is highly vulnerable to \Important{frequency analysis} for large texts. As we saw earlier, the letter frequencies are heavily biased in text. Since the substitution cipher preserves the character frequencies, the attacker can guess the key by analysing the frequencies in the text. For instance, the most frequent letter in the ciphertext is probably the image of \texttt{E} under the permutation, and so on.




\paragraph{Vigen\`ere cipher}

The \Define{Vigen\`ere cipher} is a polyalphabetic substitution cipher, an extension of the Caesar and (simple) substitution ciphers. The key is a sequence of letters, e.g. $K = \texttt{BOTTLEOFRUM}$
The key is repeated until it is as long as the message, and corresponding message and key characters are added (mod 26).
\begin{align*}
    M &= \texttt{THETREASUREISUNDERTHEROCKBYTHETHIRDPALMTREE}\\
    K &= \texttt{BOTTLEOFRUMBOTTLEOFRUMBOTTLEOFRUMBOTTLEOFRU}\\
    C &= \texttt{VWYNDJPYMMRKHOHPJGZZZEQREVKYWKLCVTSJUXRIXWZ}
\end{align*}
If the length of the key is guess right, then the attacker can still use frequency analysis.



\paragraph{Autokey Vigenère cipher}
The basic idea is to break the frequency analysis by having a non-repeating key. Again, the key is a sequence of letters e.g. $K = \texttt{BOTTLEOFRUM}$. The key is repeated only once, instead it is augmented to $K^*$ with the start of the message.
\begin{align*}
    M &= \texttt{THETREASUREISUNDERTHEROCKBYTHETHIRDPALMTREE}\\
    K &= \texttt{BOTTLEOFRUM}\\
    K^* &= \texttt{BOTTLEOFRUMTHETREASUREISUNDERTHEROCKBYTHETH}\\
    C &= \texttt{VWYNDJPYMMRCAZHVJSMCWWXVFPCYZYBMAGGACKGBWYM}
\end{align*}

This system was considered unbreakable for nearly 300 years. Then in 1863, a Prussian major discovered that the the length of the keyword can be accurately determined by looking at the separation of repeated digrams. Once the key length is known, frequency analysis can be used to determine the Caesar shift for that set of character positions.

For instance, let us guess a key length of $11$.
Frequency test each letter of alphabet as candidates for first letter of $K$. When we hit \texttt{B} we decode \texttt{T}, use \texttt{T} to decode \texttt{I}, \texttt{I} to get \texttt{O}, \texttt{O} to get \texttt{R}, etc.









\subsection{Perfect cryptosystems}

\paragraph{Perfect cryptosystems}
A cryptosystem is \Define{perfect} if knowledge of the ciphertext $C$ gives absolutely no information about the plaintext $M$.

\begin{theorem}
In a perfect cryptosystem there must be at least as many possible keys as possible plaintexts.
\end{theorem}

\begin{proof}
Suppose $|\alphabet{K}| < |\alphabet{M}|$ and let $C$ be a ciphertext. Let $d(C)$ denote the set of plaintexts that be decrypted from $C$:
\[
    d(C) = \{M \in \alphabet{M} : \exists K \in \alphabet{K} e(M,K) = C\}.
\]
We then have $d(C) \subseteq \alphabet{M}$. Also, since for every key $K$ encryption is injective, we have $|d(C)| \le |\alphabet{K}| < |\alphabet{M}|$. Thus, $d(C) \subset \alphabet{M}$ and there exists $M^* \in \alphabet{M} \setminus d(C)$: the ciphertext $C$ gives away the information that the plaintext is not $M^*$.
\end{proof}


\paragraph{One-time pad}
In the \Define{one-time pad}, the key length of the Vigen\`ere cipher is extended to be as long as the plaintext. The key characters are generated uniformly at random.
\begin{align*}
    M &= \texttt{THETREASUREISUNDERTHEROCKBYTHETHIRDPALMTREE}\\
    K &= \texttt{BHMNHRPBHQOILQWDBWPQCJNMYPKIPBZPWFZMDFNRBUC}\\
    C &= \texttt{VPRHZWQUCITRELKHGOJYHBCPJRJCXGTXFXDCERALTZH}
\end{align*}

Each character of $C$ is now a uniformly random letter, independently of all others. Thus each ciphertext $C$ is equally likely, no matter what the plaintext $M$ is. The One-time pad is a perfect cryptosystem.

There are two main problems with the one-time pad:
\begin{enumerate}
    \item Key generation. In theory, one would need to generate a very large amount of random characters. Alternatively, one could use a cryptographically secure pseudo-random number generator.

    \item Key distribution. The keys are usually not shared amongst only two people, but a whole book of keys is deployed to many agents in the same organisation. This deployment of a large amount of data to many people is not secure in itself.
\end{enumerate}




\subsection{See further}

\paragraph{Enigma}
Mechanical cipher machines came into wide use by World War II. The most famous is the German Enigma machine.

Enigma was a polyalphabetic substitution cipher based on electromechanical rotors that changed the substitution cipher after each letter. The substitution cipher would not repeat for 16900 characters. Probably would have been unbreakable if better procedures had been followed during use.

Work on breaking the Enigma cipher led to major advances in computing, in large part by Alan Turing and Gordon Welchman.




\subsection{Exercises}

\begin{exercise} \label{exercise:substitution1}
I have used a simple substitution cipher to encode some text; the ciphertext is the file \texttt{ciphertext1.txt}. Compute the letter frequencies in the paragraph. Look up the letter frequencies for common English on the web. Use this knowledge to decipher the paragraph. If it is too much work to compute letter frequencies for the whole paragraph, you could use a short part of the paragraph. Think about how this would affect the results.
\end{exercise}


\begin{exercise} \label{exercise:substitution2}
I did the same with another text to get \texttt{ciphertext2.txt}. This one may be a bit more difficult - the letter frequencies are a bit odd.
\end{exercise}








\section{Video compression}
\label{sec:10}


\subsection{The main tenets of video compression}


Video compression is based on two principles:
\begin{enumerate}
    \item \Important{spatial redundancy:} in each frame because of pixel correlation

    \item \Important{temporal redundancy:} a video frame is very similar to its immediate neighbours (predecessor and successor).
\end{enumerate}

A typical technique for video compression starts by encoding the first frame using a still image compression method. It should then encode several successive frames by identifying the differences between a frame and
its predecessor, and encoding these differences. Those are referred to as an inter frame (or non intra frame) If a frame is very different from its predecessor, it should be coded independently of any other frame. Such a frame is called an intra frame.

Encoding a frame $F_i$ in terms of its predecessor
$F_{i-1}$ introduces some distortions. Using a long sequence of inter frames may lead to accumulated errors. This
is why intra frames should be used from time to time inside a sequence, not just at its
beginning. Therefore, a inter frame could be encoded based on both past and future frames: those are called bidirectional.

In short, we have three kinds of frames:
\begin{itemize}
    \item \Important{Intra frames}, usually labelled $I$. An $I$ frame is decoded independently of all the other frames.

    \item \Important{Predictive frames}, labelled $P$. A $P$ frame is decoded using the preceding $P$ frame.

    \item \Important{Bidirectional frames}, labelled $B$. A $B$ frame is decoded using the preceding and following $I$ or $P$ frames.
\end{itemize}

The encoding and decoding is then done in a different order to the display order.

\subsection{Motion compensation}

\paragraph{Motion compensation}
The difference between consecutive frames is usually small because it is the result of moving the scene, the camera, or both between frames. This feature can therefore be exploited to achieve better compression. If the encoder discovers that a part $P$ of the preceding frame has been rigidly moved to a different location in the current frame, then $P$ can be compressed
by writing the following three items on the compressed stream: its previous location,
its current location, and information identifying the boundaries of $P$.

Motion compensation is effective if objects are just translated, not scaled or rotated.
Drastic changes in illumination from frame to frame also reduce the effectiveness of this
method.

In principle, such a part can have any shape. In practice, we are limited to equal size square
blocks. The encoder scans the current frame block by block. For each block $B$ it searches the preceding frame for a similar block $C$. Finding such a block, the encoder writes the difference between its past and present locations on the output. This difference is of the form
\[
    (C_x - B_x, C_y - By) = ( \Delta_x, \Delta_y),
\]
so it is called a \Define{motion vector}.

\paragraph{Block Search}
If $B$ is the current block in the current frame, then the previous frame has to be
searched for a block similar to $B$. The search is normally restricted to
a small area (called the \Important{search area}) around $B$, defined by the maximum displacement parameters $dx$ and $dy$. These parameters specify the maximum horizontal and vertical distances, in pixels, between B and any matching block in the previous frame. If $B$ is a square with side $b$, the search area will contain $(b + 2dx)(b + 2dy)$ pixels and will consist of $(2dx+1)(2dy +1)$ distinct, overlapping $b \times b$ squares. The number of
candidate blocks in this area is therefore proportional to $dxdy$.

\paragraph{Distortion measure}
This is the most sensitive part of the encoder. The distortion
measure selects the best match for block B. It has to be simple and fast, but also
reliable. Here are a few choices.

The \Define{mean absolute error} (or mean absolute difference) calculates the average of the
absolute differences between a pixel $B_{i,j}$ in $B$ and its counterpart $C_{i,j}$ in a candidate block $C$:
\[
    \frac{1}{b^2} \sum_{i=1}^b \sum_{j=1}^b \left| B_{i,j} - C_{i,j}  \right|.
\]
This measure is calculated for each of the $(2dx+1)(2dy +1)$ distinct, overlapping
$b \times b$ candidate blocks, and the smallest distortion (say, for block $\hat{C}$) is examined. If it is smaller than the search threshold, then $\hat{C}$ is selected as the match for $B$. Otherwise, there is no match for $B$, and $B$ has to be encoded without motion compensation. The \Define{mean square difference} is similar:
\[
    \frac{1}{b^2} \sum_{i=1}^b \sum_{j=1}^b ( B_{i,j} - C_{i,j}  )^2.
\]
The \Define{pel difference classification} (PDC) counts how many differences $|B_{i,j} - C_{i,j}|$ are below a given PDC parameter $p$.

\paragraph{Motion Vector Correction}
Once a block $C$ has been selected as the best match for $B$, a \Important{motion vector} is computed as the difference between the upper-left corner of $C$ and the upper-left corner of $B$.

Regardless of how the matching was determined, the motion vector may be wrong because of noise, local minima in the frame, or because the matching algorithm is not perfect. It is possible to apply smoothing techniques to the motion vectors after they have been calculated, in an attempt to improve the matching. Spatial correlations in the image suggest that the motion vectors should also be correlated. If certain vectors are found to violate this, they can be \Important{corrected}.

\paragraph{Coding motion vectors}
A large part of the current frame (perhaps close to
half of it) may be converted to motion vectors, which is why the way these vectors are
encoded is crucial; it must also be lossless. Two properties of motion vectors help in
encoding them:
\begin{enumerate}
    \item they are correlated;
    \item their distribution is nonuniform.
\end{enumerate}
Therefore they can be efficiently coded using e.g. Huffman or arithmetic coding.


\begin{center}
    \includegraphics[width=10cm]{DataCompression/motion_compensation.png}
\end{center}



\paragraph{Coding  the prediction error}
Motion compensation is lossy, since a block $B$ is normally matched to a slightly different block $C$. Compression can be improved by coding the difference between the current uncompressed and compressed frames on a block by block basis and only for blocks that differ much. This is usually done by \Important{transform coding}. The difference is then written on the output, following each frame.




\subsection{MPEG-1}

This section gives a brief overview of the MPEG-1 standard. We focus on the video compression part; MPEG also compresses audio. MPEG uses $I$, $P$, and $B$ pictures (frames). The pictures are arranged in a certain order, called the coding order, but (after being decoded) they are output and displayed in a different order, called the display order.

\paragraph{Macroblocks and slices}
The basic building block of an MPEG picture is the \Important{macroblock}. It
consists of a $16 \times 16$ block of luminance (grayscale) samples and two $8 \times 8$ blocks of the matching chrominance samples. The MPEG compression of a macroblock consists mainly in passing each of the six blocks through a DCT, then quantizing and encoding the results. It is very similar to JPEG compression (but with different quantization and code tables).

A picture in MPEG is organized in \Important{slices}, where each slice is a contiguous set of
macroblocks (in raster order) that have the same luminance component. The concept of slices makes sense because a picture may often contain large uniform areas, causing many contiguous macroblocks to have the same luminance. Each square in the picture below is a macroblock; those are ordered in slices. Notice that a slice can continue from scan line to scan line.

\begin{center}
    \includegraphics[width=6cm]{DataCompression/macroblocks.png}
\end{center}

\paragraph{Transform coding}
When a picture is encoded in nonintra mode (i.e., it is encoded by means of another
picture, normally its predecessor), the MPEG encoder generates the differences between the pictures, then applies the DCT to the differences. In such a case, the DCT does not
contribute much to the compression, because the differences are already decorrelated.
Nevertheless, the DCT is useful even in this case, since it is followed by quantization,
and the quantization in nonintra coding can be quite deep. The precision of the numbers processed by the DCT in MPEG also depends on whether intra or nonintra coding is used.

The actual quantization step is more sophisticated than in JPEG, so we will not go through it here. The quantized numbers QDCT are Huffman coded, using the nonadaptive Huffman method and Huffman code tables that were computed by gathering statistics from many training image sequences. The particular code table being used depends on the type of picture being encoded. The encoding part is similar to that of JPEG, using a zigzag scan, the EOB symbol, etc.


\paragraph{Layer structure}
The output of the encoder is arranged into layers: sequence, group of pictures (GOP), picture, slice, macroblock, and block. (Note that, on top of $I$, $P$ and $B$ pictures, MPEG also allows for $D$ pictures, where only the DC information is kept; those are very rare.)

\begin{center}
    \includegraphics[width=12cm]{DataCompression/mpeg_layers.png}
\end{center}


\subsection{See further}

\paragraph{Suboptimal search methods}
In motion compensation, instead of searching all the candidate blocks in the search area, some algorithms only search some of them. These suboptimal search methods are heuristics that trade compression efficiency for speed.

\paragraph{MP3} MPEG-1 had to include a way of compressing audio. The MPEG-1 Audio Layer III (\Important{MP3}) audio compression format then became massively popular, and has outlived the video part of MPEG-1!

\paragraph{MPEG-4 and H.264} The MPEG standard has evolved over time. After MPEG-1 came MPEG-2, used in DVDs. A leap forward was made in \Important{MPEG-4}, with the ability to define objects and to code on the object level. This was further improved in \Important{H.264}, also referred to as Advanced Video Coding (AVC), or MPEG-4 Part 10, or Advanced Video Coding (MPEG-4 AVC). H.264 is the standard now, used in e.g. BluRay, and is still under maintenance and improvement.



\subsection{Exercises}
\begin{exercise}
We could generalise the mean absolute and squared error as follows. Let $x = (x_0, \dots, x_{N-1})$ be a vector, and let $p \ge 1$, then its \Define{$p$-norm} is defined as
\[
    \| x \|_p := \left( \sum_{i=0}^{N-1} |x_i|^p \right)^{\frac{1}{p}}.
\]
(Forgetting the scaling factor $1/b^2$, the absolute error is given by $\| B - C \|_1$.)
\begin{enumerate}
    \item Prove that the $p$-norm is indeed a norm, i.e. for all vectors $x$ and $y$ and any scalar $a$,
    \begin{align*}
        \| x \|_p &\ge 0\\
        \| x \|_p &=0 \implies x = 0\\
        \|x + y \|_p &\le \| x \|_p + \| y \|_p\\
        \| ax \|_p &= |a| \| x \|_p.
    \end{align*}

    \item Comment on the ``fairness'' of the $p$-norm, as $p$ varies from $1$ towards infinity.

    \item In particular, what is $\lim_{p \to \infty} \| x\|_p$?

    \item What happens when $0 < p < 1$?

    \item What do you get for $p = 0$?
\end{enumerate}
\end{exercise}


\begin{exercise}
We mentioned that motion vectors were correlated and with a nonuniform distribution. How could one take advantage of these two properties when encoding? (You needn't come up with the solution that's actually used in MPEG, just think of a few different approaches and try to assess their benefits/drawbacks.)
\end{exercise}




\section{Video compression}
\label{sec:10}


\subsection{The main tenets of video compression}


Video compression is based on two principles:
\begin{enumerate}
    \item \Important{spatial redundancy:} in each frame because of pixel correlation

    \item \Important{temporal redundancy:} a video frame is very similar to its immediate neighbours (predecessor and successor).
\end{enumerate}

A typical technique for video compression starts by encoding the first frame using a still image compression method. It should then encode several successive frames by identifying the differences between a frame and
its predecessor, and encoding these differences. Those are referred to as an inter frame (or non intra frame) If a frame is very different from its predecessor, it should be coded independently of any other frame. Such a frame is called an intra frame.

Encoding a frame $F_i$ in terms of its predecessor
$F_{i-1}$ introduces some distortions. Using a long sequence of inter frames may lead to accumulated errors. This
is why intra frames should be used from time to time inside a sequence, not just at its
beginning. Therefore, a inter frame could be encoded based on both past and future frames: those are called bidirectional.

In short, we have three kinds of frames:
\begin{itemize}
    \item \Important{Intra frames}, usually labelled $I$. An $I$ frame is decoded independently of all the other frames.

    \item \Important{Predictive frames}, labelled $P$. A $P$ frame is decoded using the preceding $P$ frame.

    \item \Important{Bidirectional frames}, labelled $B$. A $B$ frame is decoded using the preceding and following $I$ or $P$ frames.
\end{itemize}

The encoding and decoding is then done in a different order to the display order.

\subsection{Motion compensation}

\paragraph{Motion compensation}
The difference between consecutive frames is usually small because it is the result of moving the scene, the camera, or both between frames. This feature can therefore be exploited to achieve better compression. If the encoder discovers that a part $P$ of the preceding frame has been rigidly moved to a different location in the current frame, then $P$ can be compressed
by writing the following three items on the compressed stream: its previous location,
its current location, and information identifying the boundaries of $P$.

Motion compensation is effective if objects are just translated, not scaled or rotated.
Drastic changes in illumination from frame to frame also reduce the effectiveness of this
method.

In principle, such a part can have any shape. In practice, we are limited to equal size square
blocks. The encoder scans the current frame block by block. For each block $B$ it searches the preceding frame for a similar block $C$. Finding such a block, the encoder writes the difference between its past and present locations on the output. This difference is of the form
\[
    (C_x - B_x, C_y - By) = ( \Delta_x, \Delta_y),
\]
so it is called a \Define{motion vector}.

\paragraph{Block Search}
If $B$ is the current block in the current frame, then the previous frame has to be
searched for a block similar to $B$. The search is normally restricted to
a small area (called the \Important{search area}) around $B$, defined by the maximum displacement parameters $dx$ and $dy$. These parameters specify the maximum horizontal and vertical distances, in pixels, between B and any matching block in the previous frame. If $B$ is a square with side $b$, the search area will contain $(b + 2dx)(b + 2dy)$ pixels and will consist of $(2dx+1)(2dy +1)$ distinct, overlapping $b \times b$ squares. The number of
candidate blocks in this area is therefore proportional to $dxdy$.

\paragraph{Distortion measure}
This is the most sensitive part of the encoder. The distortion
measure selects the best match for block B. It has to be simple and fast, but also
reliable. Here are a few choices.

The \Define{mean absolute error} (or mean absolute difference) calculates the average of the
absolute differences between a pixel $B_{i,j}$ in $B$ and its counterpart $C_{i,j}$ in a candidate block $C$:
\[
    \frac{1}{b^2} \sum_{i=1}^b \sum_{j=1}^b \left| B_{i,j} - C_{i,j}  \right|.
\]
This measure is calculated for each of the $(2dx+1)(2dy +1)$ distinct, overlapping
$b \times b$ candidate blocks, and the smallest distortion (say, for block $\hat{C}$) is examined. If it is smaller than the search threshold, then $\hat{C}$ is selected as the match for $B$. Otherwise, there is no match for $B$, and $B$ has to be encoded without motion compensation. The \Define{mean square difference} is similar:
\[
    \frac{1}{b^2} \sum_{i=1}^b \sum_{j=1}^b ( B_{i,j} - C_{i,j}  )^2.
\]
The \Define{pel difference classification} (PDC) counts how many differences $|B_{i,j} - C_{i,j}|$ are below a given PDC parameter $p$.

\paragraph{Motion Vector Correction}
Once a block $C$ has been selected as the best match for $B$, a \Important{motion vector} is computed as the difference between the upper-left corner of $C$ and the upper-left corner of $B$.

Regardless of how the matching was determined, the motion vector may be wrong because of noise, local minima in the frame, or because the matching algorithm is not perfect. It is possible to apply smoothing techniques to the motion vectors after they have been calculated, in an attempt to improve the matching. Spatial correlations in the image suggest that the motion vectors should also be correlated. If certain vectors are found to violate this, they can be \Important{corrected}.

\paragraph{Coding motion vectors}
A large part of the current frame (perhaps close to
half of it) may be converted to motion vectors, which is why the way these vectors are
encoded is crucial; it must also be lossless. Two properties of motion vectors help in
encoding them:
\begin{enumerate}
    \item they are correlated;
    \item their distribution is nonuniform.
\end{enumerate}
Therefore they can be efficiently coded using e.g. Huffman or arithmetic coding.


\begin{center}
    \includegraphics[width=10cm]{DataCompression/motion_compensation.png}
\end{center}



\paragraph{Coding  the prediction error}
Motion compensation is lossy, since a block $B$ is normally matched to a slightly different block $C$. Compression can be improved by coding the difference between the current uncompressed and compressed frames on a block by block basis and only for blocks that differ much. This is usually done by \Important{transform coding}. The difference is then written on the output, following each frame.




\subsection{MPEG-1}

This section gives a brief overview of the MPEG-1 standard. We focus on the video compression part; MPEG also compresses audio. MPEG uses $I$, $P$, and $B$ pictures (frames). The pictures are arranged in a certain order, called the coding order, but (after being decoded) they are output and displayed in a different order, called the display order.

\paragraph{Macroblocks and slices}
The basic building block of an MPEG picture is the \Important{macroblock}. It
consists of a $16 \times 16$ block of luminance (grayscale) samples and two $8 \times 8$ blocks of the matching chrominance samples. The MPEG compression of a macroblock consists mainly in passing each of the six blocks through a DCT, then quantizing and encoding the results. It is very similar to JPEG compression (but with different quantization and code tables).

A picture in MPEG is organized in \Important{slices}, where each slice is a contiguous set of
macroblocks (in raster order) that have the same luminance component. The concept of slices makes sense because a picture may often contain large uniform areas, causing many contiguous macroblocks to have the same luminance. Each square in the picture below is a macroblock; those are ordered in slices. Notice that a slice can continue from scan line to scan line.

\begin{center}
    \includegraphics[width=6cm]{DataCompression/macroblocks.png}
\end{center}

\paragraph{Transform coding}
When a picture is encoded in nonintra mode (i.e., it is encoded by means of another
picture, normally its predecessor), the MPEG encoder generates the differences between the pictures, then applies the DCT to the differences. In such a case, the DCT does not
contribute much to the compression, because the differences are already decorrelated.
Nevertheless, the DCT is useful even in this case, since it is followed by quantization,
and the quantization in nonintra coding can be quite deep. The precision of the numbers processed by the DCT in MPEG also depends on whether intra or nonintra coding is used.

The actual quantization step is more sophisticated than in JPEG, so we will not go through it here. The quantized numbers QDCT are Huffman coded, using the nonadaptive Huffman method and Huffman code tables that were computed by gathering statistics from many training image sequences. The particular code table being used depends on the type of picture being encoded. The encoding part is similar to that of JPEG, using a zigzag scan, the EOB symbol, etc.


\paragraph{Layer structure}
The output of the encoder is arranged into layers: sequence, group of pictures (GOP), picture, slice, macroblock, and block. (Note that, on top of $I$, $P$ and $B$ pictures, MPEG also allows for $D$ pictures, where only the DC information is kept; those are very rare.)

\begin{center}
    \includegraphics[width=12cm]{DataCompression/mpeg_layers.png}
\end{center}


\subsection{See further}

\paragraph{Suboptimal search methods}
In motion compensation, instead of searching all the candidate blocks in the search area, some algorithms only search some of them. These suboptimal search methods are heuristics that trade compression efficiency for speed.

\paragraph{MP3} MPEG-1 had to include a way of compressing audio. The MPEG-1 Audio Layer III (\Important{MP3}) audio compression format then became massively popular, and has outlived the video part of MPEG-1!

\paragraph{MPEG-4 and H.264} The MPEG standard has evolved over time. After MPEG-1 came MPEG-2, used in DVDs. A leap forward was made in \Important{MPEG-4}, with the ability to define objects and to code on the object level. This was further improved in \Important{H.264}, also referred to as Advanced Video Coding (AVC), or MPEG-4 Part 10, or Advanced Video Coding (MPEG-4 AVC). H.264 is the standard now, used in e.g. BluRay, and is still under maintenance and improvement.



\subsection{Exercises}
\begin{exercise}
We could generalise the mean absolute and squared error as follows. Let $x = (x_0, \dots, x_{N-1})$ be a vector, and let $p \ge 1$, then its \Define{$p$-norm} is defined as
\[
    \| x \|_p := \left( \sum_{i=0}^{N-1} |x_i|^p \right)^{\frac{1}{p}}.
\]
(Forgetting the scaling factor $1/b^2$, the absolute error is given by $\| B - C \|_1$.)
\begin{enumerate}
    \item Prove that the $p$-norm is indeed a norm, i.e. for all vectors $x$ and $y$ and any scalar $a$,
    \begin{align*}
        \| x \|_p &\ge 0\\
        \| x \|_p &=0 \implies x = 0\\
        \|x + y \|_p &\le \| x \|_p + \| y \|_p\\
        \| ax \|_p &= |a| \| x \|_p.
    \end{align*}

    \item Comment on the ``fairness'' of the $p$-norm, as $p$ varies from $1$ towards infinity.

    \item In particular, what is $\lim_{p \to \infty} \| x\|_p$?

    \item What happens when $0 < p < 1$?

    \item What do you get for $p = 0$?
\end{enumerate}
\end{exercise}


\begin{exercise}
We mentioned that motion vectors were correlated and with a nonuniform distribution. How could one take advantage of these two properties when encoding? (You needn't come up with the solution that's actually used in MPEG, just think of a few different approaches and try to assess their benefits/drawbacks.)
\end{exercise}




\section{Statistical compression I: Huffman coding}
\label{sec:01}

\subsection{Prefix codes}

\paragraph{Memoryless sources}

We have some data that we wish to encode. It could be anything: Spoken English, Data from a digital camera sensor, DNA string, etc.

We model our data as coming from a memoryless source $X$. We imagine that symbols are emitted at random according to the probability distribution of $X$. In other words, we view our data as a random string $X_1, X_2, \dots$ over some alphabet $\mathcal{X}$. Our memoryless assumption is that those form a sequence of independent identically distributed (i.i.d.) random variables: $X_i \sim X$ for all $i$.

More concretely, for any $x \in \mathcal{X}$ and any $i$, the probability
\[
    \probability(X_i = x)
\]
is independent of $i$, and of all previous or future emitted symbols.

Note that this is not always a valid assumption. We will look into source modelling into more detail in the next lectures.



\paragraph{The coding problem}


We have a source emitting symbols in $\mathcal{X} = \{x_1, \dots, x_n\}$ with respective probabilities  $\{p_1, \dots, p_n\}$.


Question: If  $\mathcal{D}$ is an alphabet of  $D$ code symbols, how can we encode the source symbols using code words (finite strings of code symbols) as economically as possible?

Formally: a \Define{source code} is a map $C : \mathcal{X} \to \mathcal{D}^*$
where  $\mathcal{D}^*$     is the set of all finite strings of symbols in  $\mathcal{D}$.

The words $C(x)$ are called the \Define{codewords}, and the integers $|C(x)|$ (the length of $C(x)$) are the \Define{word lengths}.


We can extend the code to messages as follows. A \Define{message} is any finite string of source symbols $m = m_1 \dots m_k \in \mathcal{X}^*$ and its encoding is the obvious concatenation
\[
    C(m) = C(m_1) C(m_2) \dots C(m_k).
\]



\paragraph{Prefix codes}


A code $C$ is \Define{uniquely decodable} (a.k.a. uniquely decipherable) if every finite string in $\mathcal{D}^*$ is the image of at most one message.

A prefix of a word $w = w_1 \dots w_k \in \mathcal{D}^*$ is any word of the form $w_1 \dots w_l$ for some $0 \le l \le k$ (for $l=0$, we obtain the empty word). A code is \Define{prefix} (a.k.a. instantaneous or prefix-free) if there are no two distinct source symbols $x, y \in \mathcal{X}$ such that $C(x)$ is a prefix of  $C(y)$         .

\begin{theorem}
A prefix code is uniquely decodable.
\end{theorem}

\begin{proof}
Let $C$ be a prefix code, and let $w = C(m)$ for some message $m = m_1 \dots m_k \in \mathcal{X}^*$. We give a decoding algorithm which, given $w$, determines $m$. Let $w = w_1 \dots w_l$.

Let $i$ be the smallest integer such that $w_1 \dots w_i$ is a codeword, say $w_1 \dots w_i = C(x)$. Then the $m_1 = x$. Indeed, if $m_1 = y \ne x$, then $C(x)$ is a prefix of $C(y)$, which is a contradiction. Then repeat this step, beginning with $w_{i+1}$ and hence determining $m_2$, and so on until $w$ is empty.
\end{proof}

\begin{example}
Let $\mathcal{X} = \{a, b, c, d, e\}$, $\mathcal{D} = \{0,1\}$ and
\begin{align*}
    C(a) &= 01\\
    C(b) &= 100\\
    C(c) &= 101\\
    C(d) &= 1101\\
    C(e) &= 1111.
\end{align*}

Suppose we need to decode the word $C(m) = w = 10010111011111100101$. We proceed as follows. We read the word until we reach a codeword:
\begin{align*}
    w_1 &= 1\\
    w_1 w_2 &= 10\\
    w_1 w_2 w_3 &= 100 = C(b).
\end{align*}
Therefore $m_1 = b$. We continue until we reach a codeword:
\begin{align*}
    w_4 &= 1\\
    w_4 w_5 &= 10\\
    w_4 w_5 w_6 &= 101 = C(c).
\end{align*}
Therefore $m_2 = c$. And so on... Exercise \ref{exercise:prefix} asks you to finish this simple example.
\end{example}




\subsection{Huffman codes}


\paragraph{Compact codes}

Our main aim is to design codes where the typical length of messages is reduced dramatically. The basic idea is to assign short codewords to more frequent symbols and longer codewords to less frequent ones.


More formally, the \Define{average length} (a.k.a. expected length) of the code is
\[
    L(C) = \expectation( |C(X)| ) = \sum_{x \in \mathcal{X}} |C(x)| \probability(X = x).
\]

A code is \Define{compact} (for a given source $X$) if it is uniquely decodable and it minimises the average length of codewords over all uniquely decodable codes.


\begin{theorem}
A uniquely decodable code with prescribed word lengths exists if and only if a prefix code with the same word lengths exists.
\end{theorem}

We shall prove this result in Lecture \ref{sec:35}.

\begin{corollary}
For any source $X$, there is a compact prefix code for $X$.
\end{corollary}


\paragraph{Binary Huffman code}


The key is to construct a tree where the leaves correspond to the symbols in  $\mathcal{X}$ and the paths from the root to the leaves give the codewords.

The tree is constructed iteratively. Suppose $\mathcal{X} = \{x_1, \dots, x_n\}$ with $p_1 \ge p_2 \ge  \dots \ge p_{n-1} \ge p_n$. Then merge $x_{n-1}$ and $x_n$ into a new symbol, say $x_{n-1, n}$ with probability $p_{n-1} + p_n$, and let $x_{n-1}$ and $x_n$ be the children of $x_{n-1, n}$ on the tree. Label the edges from $x_{n-1, n}$ to its children as $0$ and $1$, respectively. Repeat for the new source $X^{(1)} = \{x_1, \dots, x_{n-2}, x_{n-1, n} \}$ (making sure to order the symbols in non-decreasing probability). Repeat until the final source $X^{(n-1)}$ only has one symbol left with probability $1$; that symbol is the root of the tree.

Once the tree is built, read off the labels on the path from the root to a leaf to get the corresponding codeword.

\begin{example} \label{example:huffman}
Let $X$ with respective probabilities $a: 0.4, b: 0.2, c: 0.15, d:0.15, e:0.1$.

\begin{tikzpicture}[xscale=2]
    %\node (a0) at (0,4) {$a$}; %0.4
    %\node (b0) at (0,3) {$b$}; %0.2
    %\node (c0) at (0,2) {$c$}; %0.15
    \node (d0) at (1,2) {$d$}; %0.15
    \node (e0) at (1,1) {$e$}; %0.1

    %\node (a1) at (1,4) {$a$}; %0.4
    %\node (de1) at (1,3) {$de$}; %0.25
    \node (b1) at (1,4) {$b$}; %0.2
    \node (c1) at (1,3) {$c$}; %0.15

    %\node (a2) at (2,4) {$a$}; %0.4
    \node (bc2) at (2,4) {$bc$}; %0.35
    \node (de2) at (2,2) {$de$}; %0.25

    \node (bcde3) at (3,4) {$bcde$}; %0.6
    \node (a3) at (3,3) {$a$}; %0.4

    \node (abcde4) at (4,4) {$abcde$}; %1

\draw (de2) -- node[above] {$0$} ++(d0);
\draw (de2) -- node[below] {$1$} ++(e0);

\draw (bc2) -- node[above] {$0$} ++(b1);
\draw (bc2) -- node[below] {$1$} ++(c1);

\draw (bcde3) -- node[above] {$0$} ++(bc2);
\draw (bcde3) -- node[below] {$1$} ++(de2);

\draw (abcde4) -- node[above] {$0$} ++(bcde3);
\draw (abcde4) -- node[below] {$1$} ++(a3);


\end{tikzpicture}

The code is then
\begin{align*}
    C(a) &= 1\\
    C(b) &= 000\\
    C(c) &= 001\\
    C(d) &= 010\\
    C(e) &= 011
\end{align*}
The average length is then $2.2$ bits per symbol.
\end{example}


Note that there is no need for general tie-breaking rules. Indeed, different merges may yield different codes, and maybe even different code lengths, but always the same expected length. Similarly, the assignment of $0$ or $1$ does not change the code lengths.



\paragraph{Huffman codes are compact} Clearly, Huffman codes are prefix. The proof that they are compact is by induction on the number of symbols and omitted. It can be found in \cite[Section 5.8]{CT06}.


\paragraph{Non-binary Huffman codes}

Huffman codes can be extended to non-binary alphabets: If we have an alphabet of $D$ characters, we group the $D$ least likely symbols at each stage of reducing the source. When expanding the code we append each of the $D$ characters to one of the least likely symbols’ codewords.

We must end up with exactly $D$ symbols in the final source, so we may need to pad the original source up to $D+k(D-1)$ by adding symbols of probability $0$.

\subsection{See further}

\paragraph{Codes and Automata} The mathematical theory of uniquely decodable codes is reviewed in \cite{BPR10}, where they are simply referred to as codes. The language generated by a prefix code can be recognised by very a simple deterministic finite automaton; in fact, the relation between codes and automata is very deep and explored throughout the book. Note that this book hardly talks about data compression!

\paragraph{Canonical Huffman codes} As we shall see in Exercise \ref{exercise:huffman_unequal_lengths}, there can be several different Huffman trees for the same source. However, there is always a so-called canonical Huffman tree (and hence code) with a special shape that can be easily computed; see \cite[3.2.2]{Say12}. We shall encounter a similar idea in Lecture \ref{sec:35}.

\paragraph{Adaptive Huffman coding} Huffman coding is based on a source $X$ with given probabilities. In general, the probability of an element is computed by its relative frequency in the message; for instance, if the message has 100 characters, 34 of them are ``e'', then the probability of ``e'' is 34\%. Computing those probabilities then requires scanning the whole document before building the tree. Adaptive Huffman coding, on the other hand, builds the Huffman tree as the document is scanned, making small updates (if any) each time a new character is scanned.

\subsection{Exercises}

\begin{exercise} \label{exercise:not_prefix}
Let $\mathcal{X} = \{x_1, \dots, x_q\}$ for $q \ge 2$. Give a binary code $C : \mathcal{X} \to \{0,1\}^*$ that is uniquely decodable but neither prefix nor suffix.
\end{exercise}

\begin{exercise} \label{exercise:prefix}
Finish the example of decoding a prefix code.
\end{exercise}

\begin{exercise} \label{exercise:decoding_prefix}
How could you make the decoding algorithm of prefix codes more efficient? Would you use that modification for decoding Huffman codes? How would you include the decision problem: given $w \in \mathcal{D}^*$, determine whether $w$ is a codeword.
\end{exercise}

\begin{exercise} \label{exercise:huffman}
Construct a binary Huffman code for $X$ with probabilities $0.5, 0.2, 0.15, 0.1, 0.05$. What is the average length, and how does it compare with the one in Example \ref{example:huffman}?
\end{exercise}

\begin{exercise} \label{exercise:huffman_unequal_lengths}
Let $X$ have probabilities $(1/3, 1/3, 1/4, 1/{12})$. Show that, depending on how you merge, the binary Huffman coding procedure may lead to different code lengths, namely $(2,2,2,2)$ or $(1,2,3,3)$. Verify that the average length remains the same, though.
\end{exercise}



\section{Statistical compression II: Arithmetic coding}
\label{sec:02}

%\subsection{Adaptive Huffman coding}


\subsection{Arithmetic coding}


\paragraph{Limitation of Huffman coding} Consider a source with a heavily imbalanced distribution: say $a :0.99$ and $b: 0.01$. Suppose we want to encode the sequence
\[
    m = aaaaaaaaaa
\]
(of length $10$) using Huffman coding. Then we would require $10$ bits (the length of the message).

However, if you compute the probability of that particular $10$-character sequence, we get
\[
    p(m) = p(a)^{10} \approx 0.904.
\]
So if we were to compute the Huffman code based on all $2^{10}$ possible sequences, $m$ would be encoded as only one bit!

The main limitation of Huffman is then apparent: the codewords are only defined for symbols, not messages. Arithmetic coding offers a way of working at the \Important{sequence level}, thereby assigning a particular tag to any sequence, without working out all the tags for all sequences of the same length.

Suppose $X = \{a_1, \dots, a_n\}$ with respective probabilities $p_1, \dots, p_n$. We want to encode the message $m = a_{i_1} \dots a_{i_k}$. The output of the arithmetic encoder will be a \Important{number} in the range $[0,1)$ that uniquely describes $m$.

\begin{example} \label{example:arithmetic}
Let $X = \{a_1, a_2, a_3\}$ with probabilities $p_1 = 0.4, p_2 = 0.5, p_3 = 0.1$. The interval $[0,1)$ is subdivided among the three symbols as
\[
    a_1 : [0, 0.4), \quad a_2 : [0.4, 0.9), \quad a_3 : [0.9, 1).
\]
The interval is then recursively subdivided in the same fashion, e.g. $a_1 : [0, 0.4)$ is subdivided into
\[
    a_1a_1 : [0, 0.16), \quad a_1a_2 : [0.16, 0.36), \quad a_1a_3 : [0.36, 0.4).
\]
The final code for $a_1a_3$ could be any number in the range $[0.36, 0.4)$. The decoding is performed by iteratively performing the splits and choosing the interval where the code belongs. For instance, say we send $c = 0.36$ (obviously, we only send $36$), then the decoder first finds out that $c \in [0, 0.4)$ hence $m_1 = a_1$; then $c \in [0.36, 0.4)$ hence $m_2 = a_2$.
\end{example}




For each symbol processed, the current interval gets smaller and requires more bits to express it, but the final output is a single number for the whole sequence, which is not simply the concatenation of the codewords for its symbols. We illustrate the encoding and decoding processes in more detail by using a slightly more complex example.



We show the compression steps for the string ``SWISS\textvisiblespace MISS''. This time, the probabilities directly arise from the character frequencies, which are computed as a preliminary step to the encoding process.\\
~\\

\begin{tabular}{l|l|l|l}
    Character $x$    & Frequency  & Probability          & Range  $[L(x), H(x) )$     \\
    \hline
    S       & 5     & 0.5    & [0.5, 1.0)         \\
    W       & 1     & 0.1    & [0.4, 0.5)         \\
    I       & 2     & 0.2    & [0.2, 0.4)         \\
    M       & 1     & 0.1    & [0.1, 0.2)         \\
    \textvisiblespace       & 1     & 0.1     & [0.0, 0.1)
\end{tabular}
~\\
~\\

The encoding process begins by defining two variables $Low$ and $High$ and setting them to $0$ and $1$, respectively.They define an interval $[Low, High)$. As symbols are being input and processed, the values of $High$ and $Low$ are moved closer together. As the symbol $x$ is being input and processed, $Low$ and $High$ are updated according to
\begin{align*}
    High    &\gets Low + (High - Low) H(x),\\
    Low     &\gets Low + (High - Low) L(x).
\end{align*}
~\\

\begin{tabular}{l|l|l|l|l}
    $x$   & $L(x)$  & $H(x)$  & $Low$     & $High$     \\
    \hline
        &       &       & 0     & 1     \\
    S   & 0.5   & 1.0     & 0.5   & 1.0     \\
    W   & 0.4   & 0.5   & 0.70   & 0.75     \\
    I   & 0.2   & 0.4   & 0.71   & 0.72     \\
    S   & 0.5   & 1.0     & 0.715   & 0.72     \\
    S   & 0.5   & 1.0     & 0.7175   & 0.72     \\
    \textvisiblespace   & 0.0   & 0.1     & 0.7175   & 0.71775     \\
    M   & 0.1   & 0.2   & 0.717525   & 0.717550     \\
    I   & 0.2   & 0.4   & 0.717530   & 0.717535     \\
    S   & 0.5   & 1.0     & 0.7175325   & 0.717535     \\
    S   & 0.5   & 1.0     & 0.71753375   & 0.717535
\end{tabular}
~\\

The final code is the final value of $Low$, $0.71753375$ of which only the eight digits $71753375$ need to be written.


The decoder first inputs the symbols and their range, and reconstructs the table of frequencies and probabilities. It then inputs the rest of the code. The first digit is $7$, so the number is $0.7\ldots \in [0.5, 1)$: the first symbol is then S. It carries on, updating the code number to remove the effect of the character it just input. More explicitly, after the character $x$, it performs the update
\[
    C \gets \frac{C - L(x)}{H(x) - L(x)}.
\]
The decoder carries on until $C = 0$, in which case there should be a way to make it stop (either an end-of-file symbol is part of the input, or the length of the input was given in the header of the code).\\
~\\

\begin{tabular}{l|l|l|l}
    $x$   & $L(x)$      & $H(x)$      & $C$     \\
    \hline
        &           &           & 0.71753375     \\
    S   & 0.5       & 1.0       & 0.4350675     \\
    W   & 0.4       & 0.5       & 0.350675     \\
    I   & 0.2       & 0.4      & 0.753375     \\
    S   & 0.5      & 1.0      & 0.50675     \\
    S   & 0.5      & 1.0      & 0.0135     \\
    \textvisiblespace   & 0.0      & 0.1      & 0.135     \\
    M   & 0.1      & 0.2      & 0.35     \\
    I   & 0.2      & 0.4      & 0.75     \\
    S   & 0.5      & 1.0      & 0.5     \\
    S   & 0.5      & 1.0      & 0
\end{tabular}


\subsection{Implementation details}

\paragraph{Using integers} The encoding as described before is not practical, since it uses numbers of unlimited precision for $Low$ and $High$. The decoder process is also impractical: the number $C$ can be a very long integer.


Any practical implementation of arithmetic coding should only use integers and should not be very long. Here is an implementation that uses integers with only four digits (We only give the encoder, but the decoder can be worked out as ``doing the same in reverse,'' as we are getting used to seeing.)

The main idea is that once the leftmost digits of $Low$ and $High$ are equal, then they remain equal henceforth. So we should ``forget about'' the leftmost digit once the encoder has output it. This is done by shifting the digits. Using four digits, we first initialise $L^* = 0000$ (corresponding to $Low = 0.0000\dots = 0$) and $H^* = 9999$ (corresponding to $High = 0.9999\dots = 1$), and we proceed as follows.\\
~\\

\begin{tabular}{l|l|l|l|l|l}
    $x$   & $Low$     & $High$     & Digit & $L^*$ & $H^*$ \\
    \hline
        & 0     & 1     &       & 0000  & 9999 \\
    S   & 0.5   & 1     &       & 5000  & 9999 \\
    W   & 0.7   & 0.75  & 7     & 0000  & 4999 \\
    I    & 0.1 & 0.2 &  1     & 0000  & 9999 \\
    S    & 0.5 & 1.0 &       & 5000  & 9999 \\
    S    & 0.75 & 1.0 &       & 7500  & 9999 \\
    \textvisiblespace    & 0.75 & 0.775 &  7    & 5000  & 7499 \\
    M    & 0.525 & 0.55 &  5     & 2500  & 4999 \\
    I    & 0.3 & 0.35 & 3      & 0000  & 4999 \\
    S    & 0.25 & 0.5 &       & 2500  & 4999 \\
    S    & 0.375 & 0.5 & 3750   &   & 4999
\end{tabular}
~\\

In this toy example, we used four digits, but in practice we should be using enough to make sure that enough information is conveyed by $H^*$ and $L^*$ at all times. Another potential issue is that of underflow, when for instance $High$ decreases too fast and loses its significant digits. Scaling is then performed to avoid this situation.

\paragraph{Using binary strings} Firstly, note that we can choose to output any number in the range $[Low, High)$, ans not necessarily $Low$ per se. A certain choice of value may have fewer digits in its binary expansion, and hence require less space than $Low$. Moreover, obviously operations should be carried out in binary instead of decimal.

It can be shown that, if one uses the number $(Low + High)/2$, then one only needs to transmit the first
\[
    l = \left\lceil \log \frac{1}{p(m)} \right\rceil + 1
\]
bits of that number, where $p(m)$ is the probability of the input sequence $m$. As we shall see in Lecture \ref{sec:35}, this is very close to optimality indeed.

































\section{Lempel-Ziv I: LZ77}
\label{sec:03}


\subsection{Limitations of statistical compression}

In Lectures \ref{sec:01} and \ref{sec:02} we looked at compact codes for data being emitted by a memoryless source - a random process.

This week we look at encoding a fixed file of data efficiently. Rather than having estimates for the probabilities of each symbol, we can look at the whole message and determine the frequency of each symbol. Compact codes for memoryless sources are guaranteed to be optimal on average, but we may not have an average message. If the encoding is not determined in advance (as can be done for known sources), but is message dependent, then we must transmit the code as well as the encoded message.

Recall, memoryless sources emit each symbol independently of any previous symbols. There is no ‘pattern’ to the data beyond the frequency of each symbol. For instance, consider the message
\[
    abbaeadcaadccbaabaaa
\]
(20 characters). The letter frequencies are a:10/20, b:4/20, c:3/20, d:2/20, e:1/20. These agree exactly with the probabilities in Exercise \ref{exercise:huffman}. Using the Huffman code $abbaeadcaadccbaabaaa$ becomes
\[
    100001011110110010110110010010001100111
\]
(39 characters, as expected - 1.95 bits on average).

It is not obvious that we can do better here, and in general for randomly chosen messages with these frequencies we simply can't! But what about:
\[
    aaaaaaaaaabbbbcccdde
\]
or:
\[
    ababababacacacadadae ?
\]
Again, Huffman coding would yield 39 characters. Clearly, those messages have more than just statistical redundancy; they also have a form of structural redundancy to which statistical methods such as Huffman coding are oblivious.





\paragraph{Source modelling}

A lot of work was done in the early days of text compression to model natural languages and to understand their redundancy. The first work is Shannon's statistical analysis of English text \cite{Sha51}, and it has been significantly refined over the years (see Cover and King \cite{CK78} for a survey of techniques).

Using a completely different approach, Zipf \cite{Zip49} exhibited a remarkable variety of hyperbolic laws in social sciences; in particular the distribution of words in a natural language approximately satisfies the beautiful law described below. Suppose a natural language has $N$ words, sorted in non-increasing frequency ($p(1) \ge p(2) \ge \dots p(r) \dots \ge p(N)$). Then the probability of the word at the $r$-th rank is
\[
    p(r) = \frac{ \mu }{ r },
\]
with
\[
    \mu \approx \frac{ 1 }{ \log_e N + \gamma },
\]
where $\gamma = 0.577\dots$ is the Euler-Mascheroni constant. Finer models have been proposed, e.g. by Mandelbrot \cite{Man52}.

We have already looked at modelling English as a sequence of random letters with frequencies. This is called the \Define{first-order model} of English. Random text from this model (plus space) would look like:
\begin{quote}
    \texttt{ocroh hli rgwr nmielwis eu ll nbnesebya th eei alhenhttpa oobttva nah brl}
\end{quote}



We could do better by regarding English not as 26 letters, but as $26^2$ pairs of letters (\Define{digrams}), E.g. AB QU ZA QZ. If we analyse the frequencies of digrams, we can choose the next letter based upon the previous letter and the digram frequencies. E.g. Q will almost certainly be followed by U, T is most likely to be followed by H. This is called the \Define{second-order model} of English.  Random text from this model (plus space) would look like:
\begin{quote}
    \texttt{on ie antsoutinys are t inctore st be s deamy achin d ilonasive tucoowe at teasonare fuso tizin andy tobe seace ctisbe}
\end{quote}

Random text from the third-order model of English would look like:
\begin{quote}
\texttt{in no ist lat whey cratict froure birs grocid pondenome of demonstures of the reptagin is regoactiona of cre}
\end{quote}
There are finer and finer models of the English language, some based on $n$-gram frequencies, other (more accurate), based on frequencies of sequences of words. Examples of text generated from $12$-gram model (for letters) and $6$-gram model (for text) can be found in \cite[Chapter 4]{BCW90}.

One could then consider using Huffman coding (or any other statistical technique) with finer and finer models. There are two major issues with this approach.
\begin{enumerate}
    \item The alphabet of the source $X$ explodes! If we consider just the $4$-gram model (for letters), then the alphabet is of size $26^4 = 456,976$. In general, the alphabet size grows exponentially with $n$ for $n$-grams.

    \item The model is only appropriate for a particular sort of text. The model for English is inappropriate for German or French, let alone Greek, Russian or Chinese. So that strategy is not easily portable.
\end{enumerate}



\subsection{Lempel-Ziv}

The main idea of dictionary based compression is to construct a table (dictionary) of commonly used subsequences and refer to this to build the coded message. The main idea behind Lempel-Ziv (LZ77) is to use the message itself as a dictionary.

The LZ77 encoding algorithm works as follows. The encoding scans the message from first to last character. For implementation purposes, it uses a sliding window, of size $W$ and a look-ahead buffer of size $L$. Consider the message $m_1 \dots m_n$. When encoding at character $i$, look for the largest $l$ such that the first $l$ characters of the look-ahead buffer match $l$ consecutive characters in the sliding window, i.e.
\[
    m_i \dots m_{i+l-1} = m_{i-d} \dots m_{i-d+l-1}
\]
where   $d \le W$ and $l \le L$. Append the coded message with $(d,l,m_{i+l})$. Resume encoding at character $i+l+1$.

The L77 decoding algorithm reads a list of triplets $(d,l,m_{i+l})$, which it interprets as the instruction:
\begin{quote}
Print out $m_{i-d} \dots m_{i-d+l-1} m_{i+l}$ (the $l$ successive characters of $m$ starting from $d$ positions ago, and then $m_{i+l}$).
\end{quote}


\begin{example}
Encoding the sequence
\[
    ABRACADABRA
\]
using LZ77 (with say infinite $W$ and $L$) yields
\[
(0,0,A) \quad
(0,0,B) \quad
(0,0,R) \quad
(3,1,C) \quad
(2,1,D) \quad
(7,4,-)
\]
\end{example}

\begin{example}
A longer example now:
\begin{quote}
    Peter Piper picked a peck of pickled peppers;\\
    A peck of pickled peppers Peter Piper picked;\\
    If Peter Piper picked a peck of pickled peppers,\\
    Where's the peck of pickled peppers Peter Piper picked?
\end{quote}

We obtain:
\begin{align*}
&(0,0,P) (0,0,e)(0,0,t)(2,1,r)(0,0, )(6,1,i)(0,0,p)(6,3,p)(6,1,c)(0,0,k)(7,1,d)\\
&(7,1,a) (9,2,e)(9,2, )(0,0,o)(0,0,f)(17,5,l)(18,3,p)(4,1,p)(32,3,s)(0,0,;)\\
&(0,0,A)(26,24, )(71,18,;)(0,0,I)(38,2,P)(93,43,,)\\
&(0,0,W)(0,0,h)(6,2,e)(0,0,')(75,2,t)(8,2, ) (103,42,?)
\end{align*}


193 characters encoded as 34 triples.
If each triple is 3 bytes - that is 193 bytes reduced to 102 bytes.

\end{example}




\paragraph{How much space do we need?}

Each triple in the encoding includes $d \le W$, $l \le L$ and a character. For ASCII, the character takes $8$ bits. In total, we need
\[
    \log_2 ( W + 1 ) + \log_2 (L + 1) + 8
\]
bits to encode a triple.

Typical values are $W = 2^{16} - 1 = 65535$, and $L = 2^8 - 1 = 255$, so we need $16+8+8$ bits per triple, i.e. $4$ bytes. This much can be wasteful, especially if the value of $l$ is very low ($l=0$ means that this is a new character for instance).


\subsection{Applications of LZ77}

\paragraph{LZSS} Lempel-Ziv-Storer-Szymanski (\Define{LZSS}) is a popular variant of LZ77 introduced in 1982. The main improvement is that it includes a flag to distinguish between new characters and tokens. That way, a new character does not need to be encoded as a full token, and tokens only have two fields instead of three.

\paragraph{DEFLATE} \Define{Deflate} is a lossless compression technique that combines LZSS and Huffman coding. The key idea is that Lempel-Ziv removes structural redundancy from the data, but its output still has some statistical redundancy; the latter is then removed by Huffman coding.

Deflate is everywhere: in \texttt{gzip}, in the \Important{ZIP} file format, in \Important{PNG}, etc. (Technically, ZIP allows for many different compression techniques, but Deflate is the one that's used most of the time.)

\paragraph{LZMA} The Lempel–Ziv–Markov chain algorithm (\Define{LZMA}) was developed for 7z. Its description is out of the scope of these lectures.

\subsection{See further}

\paragraph{Variants} There are many variants of LZ77, e.g. LZX, LZRW1, LZRW4. Have a look!

\paragraph{VCDIFF} \Define{File differencing} refers to any method that compresses the differences between two files (say the source and the target files). The term \Important{delta compression} is also used. VCDIFF is a method for file differencing based on LZ77. The basic idea is very simple:
\begin{enumerate}
    \item append the target file to the source file to make one massive file

    \item use LZ77 to compress that massive file

    \item only save the part relating to the target file of the output of LZ77.
\end{enumerate}
The implementation is more involved; see \cite{KMMV02}. In general, delta compression is an important problem that is still the subject of ongoing research.


\subsection{Exercises}

\begin{exercise}
Decode the following string encoded with LZ77.
\begin{align*}
&(0,0,r)(0,0,i)(0,0,n)(0,0,g)(0,0, ) \\
& (0,0,a)(2,1,r)(7,4,o)(7,2,o)(0,0,s)(9,1,e)(3,1, )\\
& (16,2,p)(9,1,c)(0,0,k)(9,1,t)(7,1,f)(0,0,u)(0,0,l)(1,1, )\\
& (11,1,f)(15,3,s)(24,5,t)(6,1,s)(0,0,h)(11,1,o)(8,9,w)(20,1, )\\
& (11,1,l)(33,2,f)(5,4,d)(15,1,w)(0,0,n)
\end{align*}
\end{exercise}


\begin{exercise}
Watch this youtube video on the repetitiveness of pop music: \href{https://www.youtube.com/watch?v=_tjFwcmHy5M}{\textcolor{blue}{Pop Music is Stuck on Repeat}}. Can you guess which is the most repetitive song in the history of the Billboard Hot 100?
\end{exercise}

\begin{exercise}
Write your own LZ77 encoder (in Python, Java, or any other language). Can you find famous pieces of fiction that compresses massively, or hardly at all?
\end{exercise}








\section{Lempel-Ziv II: LZ78}
\label{sec:04}


\subsection{LZ78}

\paragraph{Basic idea} The \Define{LZ78} method does not use any search buffer, look-ahead buffer, or sliding window. Instead, it simply keeps a dictionary of previously encountered strings. The dictionary starts with the empty string at position zero and its size is only limited by the memory size.

The encoder outputs \Important{two-field tokens} (instead of three-field tokens in LZ77). Each token simply corresponds to a new string in the dictionary: it is of the form
\[
    (i, x),
\]
where $i$ is the position of the longest match in the dictionary and $x$ is the final character of the string.

Nothing is ever deleted from the dictionary:
\begin{itemize}
    \item Advantage over LZ77: future strings will be compressed even if they only match strings in the distant past;

    \item Drawback: the dictionary can become very large!
\end{itemize}

\paragraph{Example} Once again, it is best explained via a simple example. Say we want to compress
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily%\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}

The tokens are then (in order!):\\
~\\
\begin{tabular}{l|l|l}
     Dictionary position    & String    & Token  \\
     \hline
     0                      &  $\epsilon$         &        \\
     1                        & s           &  (0, s)      \\
     2                   & i          & (0,i)       \\
     3                   &  r         & (0,r)       \\
     4                   &  \textvisiblespace         & (0,\textvisiblespace)       \\
     5                   &  si         &    (1,i)    \\
     6                   &  d         & (0,d)       \\
     7                   &  \textvisiblespace e         &  (4,e)      \\
     8                   &  a         & (0,a)       \\
     9                   &  st         &    (1,t)    \\
     10                   & m          &    (0,m)    \\
     11                   & an          &   (8,n)     \\
     12                   & \textvisiblespace ea          & (7,a)       \\
     13                   & sil          &  (5,l)      \\
     14                   & y          &    (0,y)
\end{tabular}
~\\
And the compressed output is the list of tokens
\begin{quote}
    (0,s) (0,i) (0,r) (0,\textvisiblespace) (1,i) (0,d) (4,e) (0,a) (1,t) (0,m) (8,n) (7,a) (5,l) (0,y)
\end{quote}

Once again, the decoder sees these tokens as ``instructions.'' But following these instructions means searching in the dictionary. A useful data structure for the dictionary is a \Important{tree}, where the root is the empty string and a new string is added to the tree as a child of the string it refers to on its token. Such a tree is called a \Define{trie}.

\begin{tikzpicture}
    \node (0) at (3,0) {$\epsilon$};

    \node (1) at (0,-1) {s};
    \node (2) at (1,-1) {i};
    \node (3) at (2,-1) {r};
    \node (4) at (3,-1) {\textvisiblespace};
    \node (5) at (0,-2) {si};
    \node (6) at (4,-1) {d};
    \node (7) at (3,-2) {\textvisiblespace e};
    \node (8) at (5,-1) {a};
    \node (9) at (1,-2) {st};
    \node (10) at (6,-1) {m};
    \node (11) at (5,-2) {an};
    \node (12) at (3,-3) {\textvisiblespace ea};
    \node (13) at (0,-3) {sil};
    \node (14) at (7,-1) {y};

    \draw (0) -- (1);
    \draw (0) -- (2);
    \draw (0) -- (3);
    \draw (0) -- (4);
    \draw (1) -- (5);
    \draw (0) -- (6);
    \draw (4) -- (7);
    \draw (0) -- (8);
    \draw (1) -- (9);
    \draw (0) -- (10);
    \draw (8) -- (11);
    \draw (7) -- (12);
    \draw (5) -- (13);
    \draw (0) -- (14);


\end{tikzpicture}



\subsection{LZW}


\paragraph{Basic idea} Lempel-Ziv-Welch (\Define{LZW}) is a variant of LZ78, with two main differences.
\begin{enumerate}
    \item The dictionary is \Important{initialised with all possible characters}. If we are compressing an ASCII file, then positions 0 to 255 are filled at initialisation.

    \item The tokens only have \Important{one field}! Since we always work with at least one character (that can always be found in the dictionary), there is no need to output the next character.
\end{enumerate}

Let us go back to our example:
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily%\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}

The dictionary is initialised with all 256 ASCII characters in positions 0 to 255, e.g. a is in position 97, b in 98, s in 115, z in 122. The first character in the string is s (in the dictionary at position 115). Since si does not appear in the dictionary, we add si to the dictionary at 256, and we continue with the character i. Again, since ir is not in the dictionary, we add ir at 257 and continue with the character r.

The dictionary (omitting positions 0 to 255) and the tokens look like this:\\
~\\
\begin{tabular}{l|l|l|l}
     Position & String & Token & What the token encodes \\
     \hline
     256 & si & 115 & s\\
     257 & ir & 105 & i\\
     258 & r\textvisiblespace & 114 & r\\
     259 & \textvisiblespace a & 32 & \textvisiblespace\\
     260 & sid & 256 & si\\
     261 & d\textvisiblespace & 100 & d\\
     262 & \textvisiblespace e & 32 & \textvisiblespace\\
     263 & ea & 101 & e\\
     264 & as & 97 & a\\
     265 & st & 115 & s\\
     266 & tm & 116 & t\\
     267 & ma & 109 & m\\
     268 & an & 97 & a\\
     269 & n\textvisiblespace & 110 & n\\
     270 & \textvisiblespace ea & 262 & \textvisiblespace e\\
     271 & asi & 264 & as\\
     272 & il & 105 & i\\
     273 & ly & 108 & l\\
        &   & 121 & y
\end{tabular}
~\\~\\
The output is then
\[
    115, 105, 114, 32, 256, 100, 32, 101, 97, 115, 116, 109, 97, 110, 262, 264, 105, 108, 121
\]

The dictionary can once again be stored as a tree, but the implementation is more complex than for LZ78. A thorough description is given in \cite[3.13.2]{Sal04}.


\subsection{Applications of LZW}

\paragraph{GIF} The ubiquitous Graphics Interchange Format (\Define{GIF}) uses a variation of LZW. It uses a dynamic, growing dictionary. It starts with the number of bits per pixel $b$: $b=2$ for monochromatic images, $b=8$ for an image with $256$ colours of shades of grey. The dictionary starts with $2^{b+1}$ entries and is doubled in size every time it fills up until it reaches $2^{12} = 4,096$ entries. At that point, the encoder may want to start a new dictionary!

GIF is not actually that good at image compression because it is unidimensional. It scans the image row after row, so it can detect similarities within a row but has trouble dealing with similarities across rows instead.

\paragraph{Limitations} One major issue of using LZW (e.g. for GIF), is that LZW is \Important{patented}. In response to that, the Portable Network Graphics format was created in the mid-90s (finalised in 96). It is based on DEFLATE (and hence LZSS) instead.

Another application of LZW was the Unix shell compression utility \texttt{compress}, that was used in the 80s. However, it was superseded by \texttt{gzip}, which typically outperforms it in terms of compression ratio.

\subsection{See further}

\paragraph{Variants} LZ78 and LZW also have a few variants, notably LZMW, LZAP and LZY. Have another look!

\paragraph{Kolmogorov complexity} The principle of Lempel-Ziv encoding is to construct a list of instructions to the decoder of the form ``Copy that string (and add that character).'' But what if we allowed any sort of instructions?

The \Define{Kolmogorov complexity} is a concept that predates Lempel-Ziv. It aims at evaluating the ``intrinsic'' complexity of a binary string. Simply put, the Kolmogorov complexity of a string $x$ w.r.t. a Turing machine $U$, denoted $K_U(x)$, is the shortest length of a program for $U$ that prints out $x$ and halts. Obviously, $K_U(x)$ is not computable. But still, we can say a lot about the Kolmogorov complexity of a random string: it's about the length of the string. Therefore, almost any string is incompressible! The study of Kolmogorov complexity and associated concepts (e.g. Solomonoff's universal probability or Chaitin's Omega number) is very intriguing but outside the scope of this course.



 \subsection{Exercises}

\begin{exercise}
Encode the string
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}
with LZ78: give the dictionary table, the trie, and the output.

Encode the same string with LZW.
\end{exercise}


\begin{exercise}
Select a few (small) images, and compare the file sizes for those when saved as .bmp, .gif and .png.
\end{exercise}










\section{Context-based compression}
\label{sec:05}



\subsection{Context}

\paragraph{Context-based compression} Statistical compression (mainly for text, but not only) can be based on two properties. The first property is the frequency of symbols: the model assigns probabilities to the symbol according to their frequency in the document.

The second one is the \Important{context}. In practice, the context a symbol consists of the $N$ symbols preceding it (note that we cannot use symbols succeeding it, as the decoder typically does not know them yet!). Context-based compression then uses the context of a symbol to \Important{predict} it (i.e. to assign it a probability).

For instance, let's use a context of only one character. The letter h occurs in typical English text only about 5\% of the time. However, if the current symbol is t, then there is a much higher probability (around 30\%) that the next symbol will be h, since the digram th is very common in English. Note that the prediction is about assigning probabilities, not trying to figure out the next symbol exactly (which is impossible).

\paragraph{Static v Adaptive contexts} A static context-based modeler always uses the same probabilities, which are stored in some large table. Those probabilities are usually obtained by crawling through many documents (say typical English texts). There are issues with that approach, notably the fact that this might assign zero probabilities to some strings.

An adaptive context-based modeler also maintains tables of probabilities of all the possible digrams (or trigrams, or even longer $n$-grams). But this time the tables are updated all the time as more data are input, which adapts the probabilities to the particular data being compressed. \Important{Adaptive context-based compression} might be slower, but typically results in better compression.



\paragraph{Context length} One may think at first that the larger the number $N$ of symbols in the context, the better the compression. However, this might not be the case:
\begin{enumerate}
    \item A large $N$ requires to write the first $N$ symbols in plain text, which might hurt the overall compression.

    \item If $N$ is too large, then there are simply too many contexts, which makes storing, reading off, and writing on the table of probabilities infeasible.

    \item A very long context contains information about the nature of old data. It is not uncommon to have files where different parts have different symbol distributions.
\end{enumerate}
Therefore, in practice relatively small contexts are used in practice (for text compression, traditional methods use no more than 10 characters).


\subsection{PPM}

\paragraph{Basic idea} Prediction by Partial Matching (\Define{PPM}) is based on an encoder that keeps a statistical model of the text. It starts with an order-$N$ context. It searches its data structure for an occurrence of the current context $C$ followed by the next symbol $S$. If it finds no such occurrence, if decreases the order of the context to $N-1$ and tries again (the new context $C'$ is the final $N-1$ characters of $C$). It keeps \Important{shortening the context} until it is successful.

The encoder reads the next symbol $S$ from the input stream, looks at the current order-$N$ context $C$ (the last $N$ symbols read), and based on the previous input data, computes the probability $P$ that $S$ will appear following $C$. The encoder then calls an adaptive arithmetic encoder to encode $S$ with probability $P$. If the probability $P$ is zero, the PPM encoder tries with a smaller context; it reduces the context until $P \ne 0$. What if the symbol $S$ has not been seen yet (and hence, even with order-$0$ context, we still have $P = 0$)? Then the PPM encoder enters \Define{order-$(-1)$ context}, where the probability of $S$ is simply $1/$(size of alphabet).

\begin{example} \label{example:ppm_contexts}
Let us look at the contexts and frequency counts for the following string with $11$ symbols:
\begin{quote}
    xyzzxyxyzzx
\end{quote}

\begin{tabular}{r l | r l | r l | r l | r l}
    Order 4 & & Order 3 & & Order 2 & & Order 1 & & Order 0 &   \\
    \hline
    xyzz $\to$ x & 2 & xyz $\to$ z & 2 & xy $\to$ z & 2 & x $\to$ y & 3 & x & 4\\
    yzzx $\to$ y & 1 & yzz $\to$ x & 2 & xy $\to$ x & 1 & y $\to$ z & 2 & y & 3\\
    zzxy $\to$ x & 1 & zzx $\to$ y & 1 & yz $\to$ z & 2 & y $\to$ x & 1 & z & 4\\
    zxyx $\to$ y & 1 & zxy $\to$ x & 1 & zz $\to$ x & 2 & z $\to$ z & 2 &  & \\
    xyxy $\to$ z & 1 & xyx $\to$ y & 1 & zx $\to$ y & 1 & z $\to$ x & 2 &  & \\
    yxyz $\to$ z & 1 & yxy $\to$ z & 1 & yx $\to$ y & 1 & & &  &
\end{tabular}
\end{example}


Now, how does the encoder tell the decoder which order context it is currently using (and hence what the decoder should be using too)? The answer is to have a dedicated \Important{escape symbol}, which we'll denote esc, which should be output whenever the context size is decreased. Since this is a new character, we should also assign a probability for the escape symbol for every encountered context. There are various ways (heuristics) of assigning such probabilities. Here, we will use the so-called Method A, where the escape symbol is assigned a frequency of 1.


We are now in position to give a more explicit example. Encoding a full sequence is actually quite tedious to explain so we'll only encode a few characters. We use contexts of order at most 2. Let us consider
\begin{quote}
    this\textvisiblespace is\textvisiblespace the\textvisiblespace tithe
\end{quote}
The first few symbols are not very interesting, so let us skip forward. Let's assume we have already encoded ``this\textvisiblespace is'' and we wish to encode the next character \textvisiblespace.

We assume the word length for arithmetic coding is six bits (we used four decimal digits in our example in Lecture \ref{sec:02}). For the sake of simplicity, we have $Low = 0$ and hence $L^* = 000000$ and $High = 1$ hence $H^* = 111111$. (As we shall see, the low and high values may vary over time.)

Here is what the table of contexts looks like\\
~\\
\begin{tabular}{lr | lr | lr}
    Order 2 &           & Order 1 & & Order 0   \\
    \hline
    th $\to$ i      & 1 & t $\to$ h & 1 & t & 1\\
    th $\to$ esc    & 1 & t $\to$ esc & 1 & h & 1 \\
    hi $\to$ s      & 1 & h $\to$ i & 1 & i & 2\\
    hi $\to$ esc    & 1 & h $\to$ esc & 1 & s & 2 \\
    \Important{is $\to$ \textvisiblespace}      & \Important{1} & i $\to$ s & 2 & \textvisiblespace & 1\\
    \Important{is $\to$ esc}    & \Important{1} & i $\to$ esc & 1 & esc & 1 \\
    s\textvisiblespace $\to$ i      & 1 & \textvisiblespace $\to$ i & 1 \\
    s\textvisiblespace $\to$ esc    & 1 & \textvisiblespace $\to$ esc & 1 \\
    \textvisiblespace i $\to$ s      & 1 & s $\to$ \textvisiblespace & 1 \\
    \textvisiblespace i $\to$ esc    & 1 & s $\to$ esc & 1
\end{tabular}
~\\
~\\
The  second-order context is ``\Important{is}''. We use characters in the order of the table: the first row gives the first interval and so on. In this context, the probability of the space sign ``\textvisiblespace'' and the probability of the escape symbol esc are both equal to 1/2, and
\[
    L(\text{\textvisiblespace}) = 0, H(\text{\textvisiblespace}) = 1/2 = L(esc), H(esc) = 1.
\]
    The update equations for the new $Low$ and $High$ are
\begin{align*}
    Low &\gets Low + (High - Low) L(x) = 0,\\
    L^* &\gets 000000,\\
    High & \gets Low + (High - Low) H(x) = 1/2,\\
    H^* &\gets 011111.
\end{align*}
Since the first (most significant) bit of $L^*$ and $H^*$ coincide, we shift that bit out and shift $0$ into $L^*$ and shift $1$ into $H^*$. So we obtain:
\begin{enumerate}
    \item Encoded sequence for ``\textvisiblespace'': $0$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}

The table of contexts now becomes:\\
~\\
\begin{tabular}{lr | lr | lr}
    Order 2 &           & Order 1 & & Order 0   \\
    \hline
    th $\to$ i      & 1 & t $\to$ h & 1 & \Structure{t} & \Structure{1}\\
    th $\to$ esc    & 1 & t $\to$ esc & 1 & h & 1 \\
    hi $\to$ s      & 1 & h $\to$ i & 1 & i & 2\\
    hi $\to$ esc    & 1 & h $\to$ esc & 1 & s & 2 \\
    is $\to$ \textvisiblespace      & 2 & i $\to$ s & 2 & \textvisiblespace & 2\\
    is $\to$ esc    & 1 & i $\to$ esc & 1 & esc & 1 \\
    \Structure{s\textvisiblespace $\to$ i}      & \Structure{1} & \Structure{\textvisiblespace $\to$ i} & \Structure{1} \\
    \Structure{s\textvisiblespace $\to$ esc}    & \Structure{1} & \Structure{\textvisiblespace $\to$ esc} & \Structure{1} \\
    \textvisiblespace i $\to$ s      & 1 & s $\to$ \textvisiblespace & 2 \\
    \textvisiblespace i $\to$ esc    & 1 & s $\to$ esc & 1
\end{tabular}
~\\
~\\
The next symbol is ``t''. The second-order context is ``\Structure{s\textvisiblespace }''. Since ``t'' has zero frequency in this context, we need to encode the escape symbol. By a similar argument as above, we obtain
\begin{enumerate}
    \item Encoded escape symbol sequence: $1$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
We need to look at the first-order context, which is ``\Structure{\textvisiblespace}''. Again, ``t'' does not appear with this context, so we encode another escape symbol. We obtain
\begin{enumerate}
    \item Encoded escape symbol sequence: $1$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
We need to look at the zero-th order context. This time, ``t'' has already appeared, and is assigned the interval $[0, 1/9 )$. We then have
\begin{align*}
    Low &\gets Low + (High - Low) L(x) = 0,\\
    L^* &\gets 000000,\\
    High & \gets Low + (High - Low) H(x) = 1/9,\\
    H^* &\gets 000111.
\end{align*}
Since the three leftmost bits are equal, we shift them out. We finally obtain
\begin{enumerate}
    \item Encoded sequence: for ``t'': $000$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
So, to encode ``\textvisiblespace t'', we have transmitted $011000$.

Note that there would be a slight difference in practice: to keep everything integral, we would use $High = 63$, $Low = 0$ and perform an update of the form
\begin{align*}
    Low &\gets Low + \left\lfloor (High - Low + 1) \frac{ 1 }{ 9 }  \right\rfloor  = 0 = 000000\\
    High & \gets High + \left\lfloor (High - Low + 1) \frac{ 1 }{ 9 }  \right\rfloor - 1  = 6 = 000110
\end{align*}
Then we would have: Higher bound $H^* = 110111$.



\paragraph{Methods B and C} Two other main ways of assigning frequencies to the escape symbols aim to make the escape symbol more probable, which typically reduces the size of the resulting sequence for that symbol. The main idea is that if a context is followed by many different characters, then you are likely to encounter yet another character following that same context. For instance, think of the context ``s'' in English, which can be followed by virtually any other letter. \Important{Methods B and C} give the escape symbol a count equal to the number of symbols following the context; Method B then subtracts the count of every other symbol by one, while Method C does not amend those.


\subsection{See further}

\paragraph{RAR} The main application of PPM is in the Roshal Archive (\Define{RAR}) file format.


\paragraph{Context mixing} In \Define{context mixing}, the next-symbol predictions of two or more statistical models are combined to yield a prediction that is often more accurate than any of the individual predictions. The \Important{PAQ} series of data compression programs use context mixing; they are the cutting edge in lossless compression in terms of compression ratio (at the expense of speed and memory usage) \cite{KF11}.

Note that the problem of mixing different contexts is a very challenging issue in machine learning; that could be a very interesting topic for a project...


\paragraph{BWT} The Burrows-Wheeler transform (\Define{BWT}) is a very clever way of converting a list of symbols into one that is much more structured. You only need a little more information to make sure that the transform does not lose any information. By structured, we mean that it is ``almost sorted.'' After the transform, one can use very simple techniques to efficiently encode the structured list. Unfortunately, BWT-based compression requires to scan and to manipulate the whole message, which is an important drawback compared to the adaptive PPM.


\subsection{Exercises}


\begin{exercise} \label{exercise:ppm_contexts}
Update the table in Example \ref{example:ppm_contexts} if the following character is x.
\end{exercise}

\begin{exercise}
Finish the encoding of the sequence ``this\textvisiblespace is\textvisiblespace the\textvisiblespace tithe''. To update $High$ and $Low$, you may use our simple technique based on rational numbers, or use the version with integers instead.
\end{exercise}




\section{Transform coding I: Mathematical background}
\label{sec:06}



\subsection{Fourier series and Fourier transform}

\paragraph{Fourier series}
The representation of periodic functions in terms of a series of sines and cosines was first used by Fourier in 1812. This idea has spread like wildfire across mathematics and its applications and is used in a ridiculous range of areas under different guises.

In this section, we want to give an introduction to Fourier series. We shall be rather informal: we will not prove many of the claimed results, and we will not worry about the precise assumptions we make.

Let $f$ be a periodic function of period $T$, i.e.
\[
    f(t) = f(t + nT) \quad \forall t \in \R, n \in \Z.
\]
Then we can write $f(t)$ as
\[
    f(t) = \sum_{k=-\infty}^\infty c_k e^{\i k \omega t},
\]
where $\i = \sqrt{-1}$ and
\[
    \omega = \frac{2 \pi}{ T }.
\]
Equivalently, we have
\[
    f(t) = a_0 + \sum_{k=1}^\infty a_k \cos ( k \omega t ) + \sum_{k=1}^\infty b_k \sin ( k \omega t ).
\]
In other words, the periodic functions of period $T$ form a vector space, and $\{ e^{\i n \omega t} \}$ forms a basis. In fact, this basis is orthonormal w.r.t. the inner product
\[
    \langle f(t), g(t) \rangle = \frac{1}{T} \int_0^T f(t) g^*(t) \ dt,
\]
where $g^*(t)$ denotes the complex conjugate of $g$. The coefficients can then be obtained as follows:
\begin{equation} \label{eq:fourier}
    c_k = \langle f(t), e^{\i k \omega t} \rangle = \frac{1}{T} \int_0^T f(t) e^{-\i k \omega t} dt.
\end{equation}

The intuition behind this representation is as follows. Suppose we have a periodic signal $f$. Then its $c_k$ Fourier coefficients decompose the signal into basic fluctuating signals; each $e^{\i k \omega t}$ fluctuates at a frequency of $k \omega/(2 \pi)$. As such, these coefficients give us a measure of the different amounts of fluctuation in the signal.

\paragraph{Discrete Fourier transform}
We want to handle signals that are not periodic, but instead limited in time. Suppose we have a signal $f(t)$ that is limited in time (say from $t=0$ to $t = t_1$). Then we can extend it to a periodic signal by doing
\[
    f_P(t) = \sum_{n \in \Z} f(t - nT),
\]
where $T > t_1$. This is the so-called periodic extension of $f$.

Moreover, we are dealing with discrete signals: instead of $f(t)$, we are considering $\{f_0, f_1, \dots, f_{N-1} \}$. We can discretise Equation \eqref{eq:fourier} as follows:
\[
    F_k = \frac{1}{\sqrt{N}} \sum_{n=0}^{N-1}  f_n e^{-\i \frac{2 \pi k n }{N}} \quad k = 0, 1, \dots, N-1.
\]
The coefficients $F_k$ are called the discrete Fourier transform (\Define{DFT}) of $f$. We can recover the signal from its DFT by
\[
    f_n = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} F_k e^{\i \frac{2 \pi k n }{N}} \quad n = 0, 1, \dots, N-1.
\]

\paragraph{Matrix representation of DFT}
Let $f = (f_0, f_1, \dots, f_{N-1})$ and let $F = (F_0, \dots, F_{n-1})$ be its DFT. Then we have
\[
    F = {\bf A} f,
\]
where ${\bf A}$ is an $N \times N$ matrix with coefficients
\[
    a_{i,j} = \frac{1}{\sqrt{N}} e^{- \i \frac{2\pi}{N} ij}.
\]
For example, with $N = 4$ we obtain
\[
    {\bf A} = \frac{1}{\sqrt{4}} \begin{pmatrix}
    1 & 1 & 1 & 1\\
    1 & -\i & -1 & \i\\
    1 & -1 & 1 & -1\\
    1 & \i & -1 & -\i
    \end{pmatrix}
\]

\paragraph{Unitary and orthogonal transforms}
The matrix ${\bf A}$ from the DFT is \Define{unitary}: it satisfies
\[
    {\bf A}^\dagger = {\bf A}^{-1},
\]
where ${\bf A}^\dagger$ is its conjugate transpose: $a^\dagger_{i,j} = a_{j,i}^*$. It is easily shown that the following are equivalent for a matrix ${\bf U}$:
\begin{enumerate}
    \item ${\bf U}$ is unitary;

    \item ${\bf U}$ preserves the inner product, i.e.
    \[
        \langle x,y \rangle = \langle {\bf U} x, {\bf U}y \rangle;
    \]

    \item ${\bf U}$ preserves the norm, i.e.
    \[
         \| x \|^2 = \sum_{i=0}^{N-1} |x_i|^2 = \langle x,x \rangle = \langle {\bf U} x, {\bf U}x \rangle = \sum_{i=0}^{N-1} |({\bf U}x)_i|^2 = \| {\bf U} x \|^2.
    \]
\end{enumerate}

We will not actually use the DFT and instead we will restrict ourselves to real matrices. A real unitary matrix is called \Define{orthogonal}. So an orthogonal matrix is a real matrix ${\bf A}$ such that
\[
    {\bf A}^{-1} = {\bf A}^\top.
\]



\subsection{Two-dimensional transforms}

\paragraph{Two-dimensional data}
In general, in one dimension, we could apply any orthogonal transform as such:
\[
    \theta = {\bf A}x, \quad x = {\bf A}^\top \theta.
\]
We will use transforms for two-dimensional data (small blocks of pixels). How are we going to apply a one-dimensional transform to two-dimensional data? The answer is actually easy: we apply the transform column-wise and row-wise.

\paragraph{Matrix form} More succinctly, let ${\bf X}$ be an $N \times N$ matrix with entries $\{x_{i,j} : i,j = 0, \dots, N-1\}$. We then perform
\[
    \Theta = {\bf A} {\bf X} {\bf A}^\top.
\]
($\Theta$ is another $N \times N$ matrix.) Multiplying on the left by ${\bf A}$ performs the transform column-wise, while multiplying on the right by ${\bf A}^\top$ performs the transform row-wise. By associativity,
\[
    \Theta = ({\bf A} {\bf X}) {\bf A}^\top = {\bf A} ( {\bf X} {\bf A}^\top ),
\]
and hence the order does not matter!

Note that the inverse is straightforward:
\[
    {\bf X} = {\bf A}^\top \Theta {\bf A}.
\]

\paragraph{Basis matrices}
Let ${\bf X}$ be an $N \times N$ matrix. For all $i,j \in \{0, \dots, N-1\}$, let ${\bf E}_{i,j}$ be the matrix with a single $1$ in position $(i,j)$ and $0$ everywhere else. Then those matrices form a basis for the vector space of all $N \times N$ matrices, and we have the decomposition
\[
    {\bf X} = \sum_{i=0}^{N-1} \sum_{j=0}^{N-1} x_{i,j} {\bf E}_{i,j}.
\]
(Trivial, isn't it?)

Any two-dimensional orthogonal transform also yields a similar decomposition. We still denote $\Theta = {\bf A} {\bf X} {\bf A}^\top$, and we denote the entries of $\Theta$ as $\theta_{i,j}$. We then have
\begin{align*}
    {\bf X} &= {\bf A}^\top \Theta {\bf A}\\
            &= {\bf A}^\top \left( \sum_{i,j} \theta_{i,j} {\bf E}_{i,j} \right) {\bf A}\\
            &= \sum_{i,j} \theta_{i,j} {\bf A}_{i,j},
\end{align*}
where the matrices ${\bf A}_{i,j} := {\bf A}^\top {\bf E}_{i,j} {\bf A}$ are the \Define{basis matrices} of the transform. More concretely, denote the $k$-th row of ${\bf A}$ as $a_k$, then
\[
    {\bf A}_{i,j} = a_i^\top a_j.
\]

For instance, let
\[
    {\bf A} =  \frac{1}{ \sqrt{2} }
    \begin{pmatrix}
    1 & 1\\
    1 & -1
    \end{pmatrix}.
\]
Then the four basis matrices are given by
\begin{alignat*}{3}
    {\bf A}_{0,0} &= \frac{1}{2}
    \begin{pmatrix}
    1 & 1\\
    1 & 1
    \end{pmatrix} &
    \qquad & &
    {\bf A}_{0,1} &= \frac{1}{2}
    \begin{pmatrix}
    1 & -1\\
    1 & -1
    \end{pmatrix} \\
    {\bf A}_{1,0} &= \frac{1}{2}
    \begin{pmatrix}
    1 & 1\\
    -1 & -1
    \end{pmatrix} &
    \qquad & &
    {\bf A}_{1,1} &= \frac{1}{2}
    \begin{pmatrix}
    1 & -1\\
    -1 & 1
    \end{pmatrix}.
\end{alignat*}




\subsection{Modus operandi of transform coding}


\paragraph{Norm and energy}
The square of the norm of a vector can be viewed as its \Important{energy}. Orthogonal transforms preserve the energy (as they are unitary matrices). The efficacy of a transform depends on how much energy compaction is provided by the transform. Intuitively, the transform helps to accumulate the energy on a few symbols; those symbols should be kept, while the other ones can be discarded at little loss.

One measure of the energy compaction offered by a transform is the \Define{transform coding gain}, defined as follows (for simplicity, we define it for one-dimensional data). Let $\sigma_i^2$ be the variance of the transformed coefficient $\theta_i$ for $i=0, \dots, N-1$, then the transform coding gain is the ration of the arithmetic mean of variances over their geometric mean:
\[
    G_{\mathrm{TC}} := \frac{ \frac{1}{N} \sum_{i=0}^{N-1} \sigma_i^2 }{ \left(\sum_{i=0}^{N-1} \sigma_i^2 \right)^{\frac{1}{N}} }.
\]
(The derivation of this gain is out of the scope of this course.)


\paragraph{Three steps of transform coding}
Transform coding consists of three steps.
\begin{description}
\item[Step 1: Transform] First, the data is split into blocks of size $N$. Each block is mapped into a transform sequence using a reversible transform (usually orthogonal).

\item[Step 2: Quantization] Secondly, the transformed sequence is quantized. The quantization strategy depends on three main factors:
\begin{enumerate}
    \item the desired average bit rate

    \item the statistics of the various elements of the transformed sequence

    \item the effect of distortion in the transformed coefficients on the reconstructed sequence.
\end{enumerate}
Quantization is an important problem with some nice maths behind it, but we will skip the details here.

\item[Step 3: Encoding] Thirdly, the quantized value is encoded using some binary encoding technique, e.g. Huffman or arithmetic coding.
\end{description}









\subsection{See further}

\paragraph{FFT} The Fast Fourier Transform (\Define{FFT}) is, as its name suggests, a very efficient way of computing the DFT via divide and conquer. It was due to Cooley and Tuckey in 1965\dots even though Gauss had already discovered it in 1805!


\paragraph{Karhunen-Lo\`eve transform}
The \Define{KL} transform is the one that maximises the transform coding gain. However, it is based on the actual data and is impractical for our purposes.













\subsection{Exercises}

\begin{exercise}
Walsh-Hadamard transform. For all $N$ a power of $2$, the discrete Walsh-Hadamard transform (\Define{DWHT}) is defined as follows. For $N = 1$, let ${\bf H}_1 = (1)$. For $N \ge 1$, let
\[
    {\bf H}_{2N} = C \begin{pmatrix}
    {\bf H}_N & {\bf H}_N\\
    {\bf H}_N & - {\bf H}_N
    \end{pmatrix}.
\]
(This transform comes under different names; to make matters worse, those matrices are sometimes called Sylvester matrices.)
\begin{enumerate}
    \item Verify that the Discrete Walsh-Hadamard Transform is indeed orthogonal for the right choice of constant $C$.

    \item Give ${\bf H}_2$, ${\bf H}_4$ and ${\bf H}_8$.

    \item Give all sixteen basis matrices for $N = 4$.

    \item Find a closed form formula for the $(i,j)$ entry of ${\bf H}_N$.
\end{enumerate}
\end{exercise}


\begin{exercise}
Give the sixteen basis matrices for the DFT with $N = 4$.
\end{exercise}

\begin{exercise}
Consider the data
\[
    {\bf X} = \begin{pmatrix}
    4 & 3 & 2 & 1\\
    3 & 2 & 1 & 1\\
    2 & 1 & 1 & 0\\
    1 & 1 & 0 & 1
    \end{pmatrix}.
\]
\begin{enumerate}
    \item Compute its two-dimensional DFT.

    \item Compute its two-dimensional DWHT.
\end{enumerate}
\end{exercise}







\section{Video compression}
\label{sec:10}


\subsection{The main tenets of video compression}


Video compression is based on two principles:
\begin{enumerate}
    \item \Important{spatial redundancy:} in each frame because of pixel correlation

    \item \Important{temporal redundancy:} a video frame is very similar to its immediate neighbours (predecessor and successor).
\end{enumerate}

A typical technique for video compression starts by encoding the first frame using a still image compression method. It should then encode several successive frames by identifying the differences between a frame and
its predecessor, and encoding these differences. Those are referred to as an inter frame (or non intra frame) If a frame is very different from its predecessor, it should be coded independently of any other frame. Such a frame is called an intra frame.

Encoding a frame $F_i$ in terms of its predecessor
$F_{i-1}$ introduces some distortions. Using a long sequence of inter frames may lead to accumulated errors. This
is why intra frames should be used from time to time inside a sequence, not just at its
beginning. Therefore, a inter frame could be encoded based on both past and future frames: those are called bidirectional.

In short, we have three kinds of frames:
\begin{itemize}
    \item \Important{Intra frames}, usually labelled $I$. An $I$ frame is decoded independently of all the other frames.

    \item \Important{Predictive frames}, labelled $P$. A $P$ frame is decoded using the preceding $P$ frame.

    \item \Important{Bidirectional frames}, labelled $B$. A $B$ frame is decoded using the preceding and following $I$ or $P$ frames.
\end{itemize}

The encoding and decoding is then done in a different order to the display order.

\subsection{Motion compensation}

\paragraph{Motion compensation}
The difference between consecutive frames is usually small because it is the result of moving the scene, the camera, or both between frames. This feature can therefore be exploited to achieve better compression. If the encoder discovers that a part $P$ of the preceding frame has been rigidly moved to a different location in the current frame, then $P$ can be compressed
by writing the following three items on the compressed stream: its previous location,
its current location, and information identifying the boundaries of $P$.

Motion compensation is effective if objects are just translated, not scaled or rotated.
Drastic changes in illumination from frame to frame also reduce the effectiveness of this
method.

In principle, such a part can have any shape. In practice, we are limited to equal size square
blocks. The encoder scans the current frame block by block. For each block $B$ it searches the preceding frame for a similar block $C$. Finding such a block, the encoder writes the difference between its past and present locations on the output. This difference is of the form
\[
    (C_x - B_x, C_y - By) = ( \Delta_x, \Delta_y),
\]
so it is called a \Define{motion vector}.

\paragraph{Block Search}
If $B$ is the current block in the current frame, then the previous frame has to be
searched for a block similar to $B$. The search is normally restricted to
a small area (called the \Important{search area}) around $B$, defined by the maximum displacement parameters $dx$ and $dy$. These parameters specify the maximum horizontal and vertical distances, in pixels, between B and any matching block in the previous frame. If $B$ is a square with side $b$, the search area will contain $(b + 2dx)(b + 2dy)$ pixels and will consist of $(2dx+1)(2dy +1)$ distinct, overlapping $b \times b$ squares. The number of
candidate blocks in this area is therefore proportional to $dxdy$.

\paragraph{Distortion measure}
This is the most sensitive part of the encoder. The distortion
measure selects the best match for block B. It has to be simple and fast, but also
reliable. Here are a few choices.

The \Define{mean absolute error} (or mean absolute difference) calculates the average of the
absolute differences between a pixel $B_{i,j}$ in $B$ and its counterpart $C_{i,j}$ in a candidate block $C$:
\[
    \frac{1}{b^2} \sum_{i=1}^b \sum_{j=1}^b \left| B_{i,j} - C_{i,j}  \right|.
\]
This measure is calculated for each of the $(2dx+1)(2dy +1)$ distinct, overlapping
$b \times b$ candidate blocks, and the smallest distortion (say, for block $\hat{C}$) is examined. If it is smaller than the search threshold, then $\hat{C}$ is selected as the match for $B$. Otherwise, there is no match for $B$, and $B$ has to be encoded without motion compensation. The \Define{mean square difference} is similar:
\[
    \frac{1}{b^2} \sum_{i=1}^b \sum_{j=1}^b ( B_{i,j} - C_{i,j}  )^2.
\]
The \Define{pel difference classification} (PDC) counts how many differences $|B_{i,j} - C_{i,j}|$ are below a given PDC parameter $p$.

\paragraph{Motion Vector Correction}
Once a block $C$ has been selected as the best match for $B$, a \Important{motion vector} is computed as the difference between the upper-left corner of $C$ and the upper-left corner of $B$.

Regardless of how the matching was determined, the motion vector may be wrong because of noise, local minima in the frame, or because the matching algorithm is not perfect. It is possible to apply smoothing techniques to the motion vectors after they have been calculated, in an attempt to improve the matching. Spatial correlations in the image suggest that the motion vectors should also be correlated. If certain vectors are found to violate this, they can be \Important{corrected}.

\paragraph{Coding motion vectors}
A large part of the current frame (perhaps close to
half of it) may be converted to motion vectors, which is why the way these vectors are
encoded is crucial; it must also be lossless. Two properties of motion vectors help in
encoding them:
\begin{enumerate}
    \item they are correlated;
    \item their distribution is nonuniform.
\end{enumerate}
Therefore they can be efficiently coded using e.g. Huffman or arithmetic coding.


\begin{center}
    \includegraphics[width=10cm]{DataCompression/motion_compensation.png}
\end{center}



\paragraph{Coding  the prediction error}
Motion compensation is lossy, since a block $B$ is normally matched to a slightly different block $C$. Compression can be improved by coding the difference between the current uncompressed and compressed frames on a block by block basis and only for blocks that differ much. This is usually done by \Important{transform coding}. The difference is then written on the output, following each frame.




\subsection{MPEG-1}

This section gives a brief overview of the MPEG-1 standard. We focus on the video compression part; MPEG also compresses audio. MPEG uses $I$, $P$, and $B$ pictures (frames). The pictures are arranged in a certain order, called the coding order, but (after being decoded) they are output and displayed in a different order, called the display order.

\paragraph{Macroblocks and slices}
The basic building block of an MPEG picture is the \Important{macroblock}. It
consists of a $16 \times 16$ block of luminance (grayscale) samples and two $8 \times 8$ blocks of the matching chrominance samples. The MPEG compression of a macroblock consists mainly in passing each of the six blocks through a DCT, then quantizing and encoding the results. It is very similar to JPEG compression (but with different quantization and code tables).

A picture in MPEG is organized in \Important{slices}, where each slice is a contiguous set of
macroblocks (in raster order) that have the same luminance component. The concept of slices makes sense because a picture may often contain large uniform areas, causing many contiguous macroblocks to have the same luminance. Each square in the picture below is a macroblock; those are ordered in slices. Notice that a slice can continue from scan line to scan line.

\begin{center}
    \includegraphics[width=6cm]{DataCompression/macroblocks.png}
\end{center}

\paragraph{Transform coding}
When a picture is encoded in nonintra mode (i.e., it is encoded by means of another
picture, normally its predecessor), the MPEG encoder generates the differences between the pictures, then applies the DCT to the differences. In such a case, the DCT does not
contribute much to the compression, because the differences are already decorrelated.
Nevertheless, the DCT is useful even in this case, since it is followed by quantization,
and the quantization in nonintra coding can be quite deep. The precision of the numbers processed by the DCT in MPEG also depends on whether intra or nonintra coding is used.

The actual quantization step is more sophisticated than in JPEG, so we will not go through it here. The quantized numbers QDCT are Huffman coded, using the nonadaptive Huffman method and Huffman code tables that were computed by gathering statistics from many training image sequences. The particular code table being used depends on the type of picture being encoded. The encoding part is similar to that of JPEG, using a zigzag scan, the EOB symbol, etc.


\paragraph{Layer structure}
The output of the encoder is arranged into layers: sequence, group of pictures (GOP), picture, slice, macroblock, and block. (Note that, on top of $I$, $P$ and $B$ pictures, MPEG also allows for $D$ pictures, where only the DC information is kept; those are very rare.)

\begin{center}
    \includegraphics[width=12cm]{DataCompression/mpeg_layers.png}
\end{center}


\subsection{See further}

\paragraph{Suboptimal search methods}
In motion compensation, instead of searching all the candidate blocks in the search area, some algorithms only search some of them. These suboptimal search methods are heuristics that trade compression efficiency for speed.

\paragraph{MP3} MPEG-1 had to include a way of compressing audio. The MPEG-1 Audio Layer III (\Important{MP3}) audio compression format then became massively popular, and has outlived the video part of MPEG-1!

\paragraph{MPEG-4 and H.264} The MPEG standard has evolved over time. After MPEG-1 came MPEG-2, used in DVDs. A leap forward was made in \Important{MPEG-4}, with the ability to define objects and to code on the object level. This was further improved in \Important{H.264}, also referred to as Advanced Video Coding (AVC), or MPEG-4 Part 10, or Advanced Video Coding (MPEG-4 AVC). H.264 is the standard now, used in e.g. BluRay, and is still under maintenance and improvement.



\subsection{Exercises}
\begin{exercise}
We could generalise the mean absolute and squared error as follows. Let $x = (x_0, \dots, x_{N-1})$ be a vector, and let $p \ge 1$, then its \Define{$p$-norm} is defined as
\[
    \| x \|_p := \left( \sum_{i=0}^{N-1} |x_i|^p \right)^{\frac{1}{p}}.
\]
(Forgetting the scaling factor $1/b^2$, the absolute error is given by $\| B - C \|_1$.)
\begin{enumerate}
    \item Prove that the $p$-norm is indeed a norm, i.e. for all vectors $x$ and $y$ and any scalar $a$,
    \begin{align*}
        \| x \|_p &\ge 0\\
        \| x \|_p &=0 \implies x = 0\\
        \|x + y \|_p &\le \| x \|_p + \| y \|_p\\
        \| ax \|_p &= |a| \| x \|_p.
    \end{align*}

    \item Comment on the ``fairness'' of the $p$-norm, as $p$ varies from $1$ towards infinity.

    \item In particular, what is $\lim_{p \to \infty} \| x\|_p$?

    \item What happens when $0 < p < 1$?

    \item What do you get for $p = 0$?
\end{enumerate}
\end{exercise}


\begin{exercise}
We mentioned that motion vectors were correlated and with a nonuniform distribution. How could one take advantage of these two properties when encoding? (You needn't come up with the solution that's actually used in MPEG, just think of a few different approaches and try to assess their benefits/drawbacks.)
\end{exercise}




\section{Statistical compression I: Huffman coding}
\label{sec:01}

\subsection{Prefix codes}

\paragraph{Memoryless sources}

We have some data that we wish to encode. It could be anything: Spoken English, Data from a digital camera sensor, DNA string, etc.

We model our data as coming from a memoryless source $X$. We imagine that symbols are emitted at random according to the probability distribution of $X$. In other words, we view our data as a random string $X_1, X_2, \dots$ over some alphabet $\mathcal{X}$. Our memoryless assumption is that those form a sequence of independent identically distributed (i.i.d.) random variables: $X_i \sim X$ for all $i$.

More concretely, for any $x \in \mathcal{X}$ and any $i$, the probability
\[
    \probability(X_i = x)
\]
is independent of $i$, and of all previous or future emitted symbols.

Note that this is not always a valid assumption. We will look into source modelling into more detail in the next lectures.



\paragraph{The coding problem}


We have a source emitting symbols in $\mathcal{X} = \{x_1, \dots, x_n\}$ with respective probabilities  $\{p_1, \dots, p_n\}$.


Question: If  $\mathcal{D}$ is an alphabet of  $D$ code symbols, how can we encode the source symbols using code words (finite strings of code symbols) as economically as possible?

Formally: a \Define{source code} is a map $C : \mathcal{X} \to \mathcal{D}^*$
where  $\mathcal{D}^*$     is the set of all finite strings of symbols in  $\mathcal{D}$.

The words $C(x)$ are called the \Define{codewords}, and the integers $|C(x)|$ (the length of $C(x)$) are the \Define{word lengths}.


We can extend the code to messages as follows. A \Define{message} is any finite string of source symbols $m = m_1 \dots m_k \in \mathcal{X}^*$ and its encoding is the obvious concatenation
\[
    C(m) = C(m_1) C(m_2) \dots C(m_k).
\]



\paragraph{Prefix codes}


A code $C$ is \Define{uniquely decodable} (a.k.a. uniquely decipherable) if every finite string in $\mathcal{D}^*$ is the image of at most one message.

A prefix of a word $w = w_1 \dots w_k \in \mathcal{D}^*$ is any word of the form $w_1 \dots w_l$ for some $0 \le l \le k$ (for $l=0$, we obtain the empty word). A code is \Define{prefix} (a.k.a. instantaneous or prefix-free) if there are no two distinct source symbols $x, y \in \mathcal{X}$ such that $C(x)$ is a prefix of  $C(y)$         .

\begin{theorem}
A prefix code is uniquely decodable.
\end{theorem}

\begin{proof}
Let $C$ be a prefix code, and let $w = C(m)$ for some message $m = m_1 \dots m_k \in \mathcal{X}^*$. We give a decoding algorithm which, given $w$, determines $m$. Let $w = w_1 \dots w_l$.

Let $i$ be the smallest integer such that $w_1 \dots w_i$ is a codeword, say $w_1 \dots w_i = C(x)$. Then the $m_1 = x$. Indeed, if $m_1 = y \ne x$, then $C(x)$ is a prefix of $C(y)$, which is a contradiction. Then repeat this step, beginning with $w_{i+1}$ and hence determining $m_2$, and so on until $w$ is empty.
\end{proof}

\begin{example}
Let $\mathcal{X} = \{a, b, c, d, e\}$, $\mathcal{D} = \{0,1\}$ and
\begin{align*}
    C(a) &= 01\\
    C(b) &= 100\\
    C(c) &= 101\\
    C(d) &= 1101\\
    C(e) &= 1111.
\end{align*}

Suppose we need to decode the word $C(m) = w = 10010111011111100101$. We proceed as follows. We read the word until we reach a codeword:
\begin{align*}
    w_1 &= 1\\
    w_1 w_2 &= 10\\
    w_1 w_2 w_3 &= 100 = C(b).
\end{align*}
Therefore $m_1 = b$. We continue until we reach a codeword:
\begin{align*}
    w_4 &= 1\\
    w_4 w_5 &= 10\\
    w_4 w_5 w_6 &= 101 = C(c).
\end{align*}
Therefore $m_2 = c$. And so on... Exercise \ref{exercise:prefix} asks you to finish this simple example.
\end{example}




\subsection{Huffman codes}


\paragraph{Compact codes}

Our main aim is to design codes where the typical length of messages is reduced dramatically. The basic idea is to assign short codewords to more frequent symbols and longer codewords to less frequent ones.


More formally, the \Define{average length} (a.k.a. expected length) of the code is
\[
    L(C) = \expectation( |C(X)| ) = \sum_{x \in \mathcal{X}} |C(x)| \probability(X = x).
\]

A code is \Define{compact} (for a given source $X$) if it is uniquely decodable and it minimises the average length of codewords over all uniquely decodable codes.


\begin{theorem}
A uniquely decodable code with prescribed word lengths exists if and only if a prefix code with the same word lengths exists.
\end{theorem}

We shall prove this result in Lecture \ref{sec:35}.

\begin{corollary}
For any source $X$, there is a compact prefix code for $X$.
\end{corollary}


\paragraph{Binary Huffman code}


The key is to construct a tree where the leaves correspond to the symbols in  $\mathcal{X}$ and the paths from the root to the leaves give the codewords.

The tree is constructed iteratively. Suppose $\mathcal{X} = \{x_1, \dots, x_n\}$ with $p_1 \ge p_2 \ge  \dots \ge p_{n-1} \ge p_n$. Then merge $x_{n-1}$ and $x_n$ into a new symbol, say $x_{n-1, n}$ with probability $p_{n-1} + p_n$, and let $x_{n-1}$ and $x_n$ be the children of $x_{n-1, n}$ on the tree. Label the edges from $x_{n-1, n}$ to its children as $0$ and $1$, respectively. Repeat for the new source $X^{(1)} = \{x_1, \dots, x_{n-2}, x_{n-1, n} \}$ (making sure to order the symbols in non-decreasing probability). Repeat until the final source $X^{(n-1)}$ only has one symbol left with probability $1$; that symbol is the root of the tree.

Once the tree is built, read off the labels on the path from the root to a leaf to get the corresponding codeword.

\begin{example} \label{example:huffman}
Let $X$ with respective probabilities $a: 0.4, b: 0.2, c: 0.15, d:0.15, e:0.1$.

\begin{tikzpicture}[xscale=2]
    %\node (a0) at (0,4) {$a$}; %0.4
    %\node (b0) at (0,3) {$b$}; %0.2
    %\node (c0) at (0,2) {$c$}; %0.15
    \node (d0) at (1,2) {$d$}; %0.15
    \node (e0) at (1,1) {$e$}; %0.1

    %\node (a1) at (1,4) {$a$}; %0.4
    %\node (de1) at (1,3) {$de$}; %0.25
    \node (b1) at (1,4) {$b$}; %0.2
    \node (c1) at (1,3) {$c$}; %0.15

    %\node (a2) at (2,4) {$a$}; %0.4
    \node (bc2) at (2,4) {$bc$}; %0.35
    \node (de2) at (2,2) {$de$}; %0.25

    \node (bcde3) at (3,4) {$bcde$}; %0.6
    \node (a3) at (3,3) {$a$}; %0.4

    \node (abcde4) at (4,4) {$abcde$}; %1

\draw (de2) -- node[above] {$0$} ++(d0);
\draw (de2) -- node[below] {$1$} ++(e0);

\draw (bc2) -- node[above] {$0$} ++(b1);
\draw (bc2) -- node[below] {$1$} ++(c1);

\draw (bcde3) -- node[above] {$0$} ++(bc2);
\draw (bcde3) -- node[below] {$1$} ++(de2);

\draw (abcde4) -- node[above] {$0$} ++(bcde3);
\draw (abcde4) -- node[below] {$1$} ++(a3);


\end{tikzpicture}

The code is then
\begin{align*}
    C(a) &= 1\\
    C(b) &= 000\\
    C(c) &= 001\\
    C(d) &= 010\\
    C(e) &= 011
\end{align*}
The average length is then $2.2$ bits per symbol.
\end{example}


Note that there is no need for general tie-breaking rules. Indeed, different merges may yield different codes, and maybe even different code lengths, but always the same expected length. Similarly, the assignment of $0$ or $1$ does not change the code lengths.



\paragraph{Huffman codes are compact} Clearly, Huffman codes are prefix. The proof that they are compact is by induction on the number of symbols and omitted. It can be found in \cite[Section 5.8]{CT06}.


\paragraph{Non-binary Huffman codes}

Huffman codes can be extended to non-binary alphabets: If we have an alphabet of $D$ characters, we group the $D$ least likely symbols at each stage of reducing the source. When expanding the code we append each of the $D$ characters to one of the least likely symbols’ codewords.

We must end up with exactly $D$ symbols in the final source, so we may need to pad the original source up to $D+k(D-1)$ by adding symbols of probability $0$.

\subsection{See further}

\paragraph{Codes and Automata} The mathematical theory of uniquely decodable codes is reviewed in \cite{BPR10}, where they are simply referred to as codes. The language generated by a prefix code can be recognised by very a simple deterministic finite automaton; in fact, the relation between codes and automata is very deep and explored throughout the book. Note that this book hardly talks about data compression!

\paragraph{Canonical Huffman codes} As we shall see in Exercise \ref{exercise:huffman_unequal_lengths}, there can be several different Huffman trees for the same source. However, there is always a so-called canonical Huffman tree (and hence code) with a special shape that can be easily computed; see \cite[3.2.2]{Say12}. We shall encounter a similar idea in Lecture \ref{sec:35}.

\paragraph{Adaptive Huffman coding} Huffman coding is based on a source $X$ with given probabilities. In general, the probability of an element is computed by its relative frequency in the message; for instance, if the message has 100 characters, 34 of them are ``e'', then the probability of ``e'' is 34\%. Computing those probabilities then requires scanning the whole document before building the tree. Adaptive Huffman coding, on the other hand, builds the Huffman tree as the document is scanned, making small updates (if any) each time a new character is scanned.

\subsection{Exercises}

\begin{exercise} \label{exercise:not_prefix}
Let $\mathcal{X} = \{x_1, \dots, x_q\}$ for $q \ge 2$. Give a binary code $C : \mathcal{X} \to \{0,1\}^*$ that is uniquely decodable but neither prefix nor suffix.
\end{exercise}

\begin{exercise} \label{exercise:prefix}
Finish the example of decoding a prefix code.
\end{exercise}

\begin{exercise} \label{exercise:decoding_prefix}
How could you make the decoding algorithm of prefix codes more efficient? Would you use that modification for decoding Huffman codes? How would you include the decision problem: given $w \in \mathcal{D}^*$, determine whether $w$ is a codeword.
\end{exercise}

\begin{exercise} \label{exercise:huffman}
Construct a binary Huffman code for $X$ with probabilities $0.5, 0.2, 0.15, 0.1, 0.05$. What is the average length, and how does it compare with the one in Example \ref{example:huffman}?
\end{exercise}

\begin{exercise} \label{exercise:huffman_unequal_lengths}
Let $X$ have probabilities $(1/3, 1/3, 1/4, 1/{12})$. Show that, depending on how you merge, the binary Huffman coding procedure may lead to different code lengths, namely $(2,2,2,2)$ or $(1,2,3,3)$. Verify that the average length remains the same, though.
\end{exercise}



\section{Statistical compression II: Arithmetic coding}
\label{sec:02}

%\subsection{Adaptive Huffman coding}


\subsection{Arithmetic coding}


\paragraph{Limitation of Huffman coding} Consider a source with a heavily imbalanced distribution: say $a :0.99$ and $b: 0.01$. Suppose we want to encode the sequence
\[
    m = aaaaaaaaaa
\]
(of length $10$) using Huffman coding. Then we would require $10$ bits (the length of the message).

However, if you compute the probability of that particular $10$-character sequence, we get
\[
    p(m) = p(a)^{10} \approx 0.904.
\]
So if we were to compute the Huffman code based on all $2^{10}$ possible sequences, $m$ would be encoded as only one bit!

The main limitation of Huffman is then apparent: the codewords are only defined for symbols, not messages. Arithmetic coding offers a way of working at the \Important{sequence level}, thereby assigning a particular tag to any sequence, without working out all the tags for all sequences of the same length.

Suppose $X = \{a_1, \dots, a_n\}$ with respective probabilities $p_1, \dots, p_n$. We want to encode the message $m = a_{i_1} \dots a_{i_k}$. The output of the arithmetic encoder will be a \Important{number} in the range $[0,1)$ that uniquely describes $m$.

\begin{example} \label{example:arithmetic}
Let $X = \{a_1, a_2, a_3\}$ with probabilities $p_1 = 0.4, p_2 = 0.5, p_3 = 0.1$. The interval $[0,1)$ is subdivided among the three symbols as
\[
    a_1 : [0, 0.4), \quad a_2 : [0.4, 0.9), \quad a_3 : [0.9, 1).
\]
The interval is then recursively subdivided in the same fashion, e.g. $a_1 : [0, 0.4)$ is subdivided into
\[
    a_1a_1 : [0, 0.16), \quad a_1a_2 : [0.16, 0.36), \quad a_1a_3 : [0.36, 0.4).
\]
The final code for $a_1a_3$ could be any number in the range $[0.36, 0.4)$. The decoding is performed by iteratively performing the splits and choosing the interval where the code belongs. For instance, say we send $c = 0.36$ (obviously, we only send $36$), then the decoder first finds out that $c \in [0, 0.4)$ hence $m_1 = a_1$; then $c \in [0.36, 0.4)$ hence $m_2 = a_2$.
\end{example}




For each symbol processed, the current interval gets smaller and requires more bits to express it, but the final output is a single number for the whole sequence, which is not simply the concatenation of the codewords for its symbols. We illustrate the encoding and decoding processes in more detail by using a slightly more complex example.



We show the compression steps for the string ``SWISS\textvisiblespace MISS''. This time, the probabilities directly arise from the character frequencies, which are computed as a preliminary step to the encoding process.\\
~\\

\begin{tabular}{l|l|l|l}
    Character $x$    & Frequency  & Probability          & Range  $[L(x), H(x) )$     \\
    \hline
    S       & 5     & 0.5    & [0.5, 1.0)         \\
    W       & 1     & 0.1    & [0.4, 0.5)         \\
    I       & 2     & 0.2    & [0.2, 0.4)         \\
    M       & 1     & 0.1    & [0.1, 0.2)         \\
    \textvisiblespace       & 1     & 0.1     & [0.0, 0.1)
\end{tabular}
~\\
~\\

The encoding process begins by defining two variables $Low$ and $High$ and setting them to $0$ and $1$, respectively.They define an interval $[Low, High)$. As symbols are being input and processed, the values of $High$ and $Low$ are moved closer together. As the symbol $x$ is being input and processed, $Low$ and $High$ are updated according to
\begin{align*}
    High    &\gets Low + (High - Low) H(x),\\
    Low     &\gets Low + (High - Low) L(x).
\end{align*}
~\\

\begin{tabular}{l|l|l|l|l}
    $x$   & $L(x)$  & $H(x)$  & $Low$     & $High$     \\
    \hline
        &       &       & 0     & 1     \\
    S   & 0.5   & 1.0     & 0.5   & 1.0     \\
    W   & 0.4   & 0.5   & 0.70   & 0.75     \\
    I   & 0.2   & 0.4   & 0.71   & 0.72     \\
    S   & 0.5   & 1.0     & 0.715   & 0.72     \\
    S   & 0.5   & 1.0     & 0.7175   & 0.72     \\
    \textvisiblespace   & 0.0   & 0.1     & 0.7175   & 0.71775     \\
    M   & 0.1   & 0.2   & 0.717525   & 0.717550     \\
    I   & 0.2   & 0.4   & 0.717530   & 0.717535     \\
    S   & 0.5   & 1.0     & 0.7175325   & 0.717535     \\
    S   & 0.5   & 1.0     & 0.71753375   & 0.717535
\end{tabular}
~\\

The final code is the final value of $Low$, $0.71753375$ of which only the eight digits $71753375$ need to be written.


The decoder first inputs the symbols and their range, and reconstructs the table of frequencies and probabilities. It then inputs the rest of the code. The first digit is $7$, so the number is $0.7\ldots \in [0.5, 1)$: the first symbol is then S. It carries on, updating the code number to remove the effect of the character it just input. More explicitly, after the character $x$, it performs the update
\[
    C \gets \frac{C - L(x)}{H(x) - L(x)}.
\]
The decoder carries on until $C = 0$, in which case there should be a way to make it stop (either an end-of-file symbol is part of the input, or the length of the input was given in the header of the code).\\
~\\

\begin{tabular}{l|l|l|l}
    $x$   & $L(x)$      & $H(x)$      & $C$     \\
    \hline
        &           &           & 0.71753375     \\
    S   & 0.5       & 1.0       & 0.4350675     \\
    W   & 0.4       & 0.5       & 0.350675     \\
    I   & 0.2       & 0.4      & 0.753375     \\
    S   & 0.5      & 1.0      & 0.50675     \\
    S   & 0.5      & 1.0      & 0.0135     \\
    \textvisiblespace   & 0.0      & 0.1      & 0.135     \\
    M   & 0.1      & 0.2      & 0.35     \\
    I   & 0.2      & 0.4      & 0.75     \\
    S   & 0.5      & 1.0      & 0.5     \\
    S   & 0.5      & 1.0      & 0
\end{tabular}


\subsection{Implementation details}

\paragraph{Using integers} The encoding as described before is not practical, since it uses numbers of unlimited precision for $Low$ and $High$. The decoder process is also impractical: the number $C$ can be a very long integer.


Any practical implementation of arithmetic coding should only use integers and should not be very long. Here is an implementation that uses integers with only four digits (We only give the encoder, but the decoder can be worked out as ``doing the same in reverse,'' as we are getting used to seeing.)

The main idea is that once the leftmost digits of $Low$ and $High$ are equal, then they remain equal henceforth. So we should ``forget about'' the leftmost digit once the encoder has output it. This is done by shifting the digits. Using four digits, we first initialise $L^* = 0000$ (corresponding to $Low = 0.0000\dots = 0$) and $H^* = 9999$ (corresponding to $High = 0.9999\dots = 1$), and we proceed as follows.\\
~\\

\begin{tabular}{l|l|l|l|l|l}
    $x$   & $Low$     & $High$     & Digit & $L^*$ & $H^*$ \\
    \hline
        & 0     & 1     &       & 0000  & 9999 \\
    S   & 0.5   & 1     &       & 5000  & 9999 \\
    W   & 0.7   & 0.75  & 7     & 0000  & 4999 \\
    I    & 0.1 & 0.2 &  1     & 0000  & 9999 \\
    S    & 0.5 & 1.0 &       & 5000  & 9999 \\
    S    & 0.75 & 1.0 &       & 7500  & 9999 \\
    \textvisiblespace    & 0.75 & 0.775 &  7    & 5000  & 7499 \\
    M    & 0.525 & 0.55 &  5     & 2500  & 4999 \\
    I    & 0.3 & 0.35 & 3      & 0000  & 4999 \\
    S    & 0.25 & 0.5 &       & 2500  & 4999 \\
    S    & 0.375 & 0.5 & 3750   &   & 4999
\end{tabular}
~\\

In this toy example, we used four digits, but in practice we should be using enough to make sure that enough information is conveyed by $H^*$ and $L^*$ at all times. Another potential issue is that of underflow, when for instance $High$ decreases too fast and loses its significant digits. Scaling is then performed to avoid this situation.

\paragraph{Using binary strings} Firstly, note that we can choose to output any number in the range $[Low, High)$, ans not necessarily $Low$ per se. A certain choice of value may have fewer digits in its binary expansion, and hence require less space than $Low$. Moreover, obviously operations should be carried out in binary instead of decimal.

It can be shown that, if one uses the number $(Low + High)/2$, then one only needs to transmit the first
\[
    l = \left\lceil \log \frac{1}{p(m)} \right\rceil + 1
\]
bits of that number, where $p(m)$ is the probability of the input sequence $m$. As we shall see in Lecture \ref{sec:35}, this is very close to optimality indeed.

































\section{Lempel-Ziv I: LZ77}
\label{sec:03}


\subsection{Limitations of statistical compression}

In Lectures \ref{sec:01} and \ref{sec:02} we looked at compact codes for data being emitted by a memoryless source - a random process.

This week we look at encoding a fixed file of data efficiently. Rather than having estimates for the probabilities of each symbol, we can look at the whole message and determine the frequency of each symbol. Compact codes for memoryless sources are guaranteed to be optimal on average, but we may not have an average message. If the encoding is not determined in advance (as can be done for known sources), but is message dependent, then we must transmit the code as well as the encoded message.

Recall, memoryless sources emit each symbol independently of any previous symbols. There is no ‘pattern’ to the data beyond the frequency of each symbol. For instance, consider the message
\[
    abbaeadcaadccbaabaaa
\]
(20 characters). The letter frequencies are a:10/20, b:4/20, c:3/20, d:2/20, e:1/20. These agree exactly with the probabilities in Exercise \ref{exercise:huffman}. Using the Huffman code $abbaeadcaadccbaabaaa$ becomes
\[
    100001011110110010110110010010001100111
\]
(39 characters, as expected - 1.95 bits on average).

It is not obvious that we can do better here, and in general for randomly chosen messages with these frequencies we simply can't! But what about:
\[
    aaaaaaaaaabbbbcccdde
\]
or:
\[
    ababababacacacadadae ?
\]
Again, Huffman coding would yield 39 characters. Clearly, those messages have more than just statistical redundancy; they also have a form of structural redundancy to which statistical methods such as Huffman coding are oblivious.





\paragraph{Source modelling}

A lot of work was done in the early days of text compression to model natural languages and to understand their redundancy. The first work is Shannon's statistical analysis of English text \cite{Sha51}, and it has been significantly refined over the years (see Cover and King \cite{CK78} for a survey of techniques).

Using a completely different approach, Zipf \cite{Zip49} exhibited a remarkable variety of hyperbolic laws in social sciences; in particular the distribution of words in a natural language approximately satisfies the beautiful law described below. Suppose a natural language has $N$ words, sorted in non-increasing frequency ($p(1) \ge p(2) \ge \dots p(r) \dots \ge p(N)$). Then the probability of the word at the $r$-th rank is
\[
    p(r) = \frac{ \mu }{ r },
\]
with
\[
    \mu \approx \frac{ 1 }{ \log_e N + \gamma },
\]
where $\gamma = 0.577\dots$ is the Euler-Mascheroni constant. Finer models have been proposed, e.g. by Mandelbrot \cite{Man52}.

We have already looked at modelling English as a sequence of random letters with frequencies. This is called the \Define{first-order model} of English. Random text from this model (plus space) would look like:
\begin{quote}
    \texttt{ocroh hli rgwr nmielwis eu ll nbnesebya th eei alhenhttpa oobttva nah brl}
\end{quote}



We could do better by regarding English not as 26 letters, but as $26^2$ pairs of letters (\Define{digrams}), E.g. AB QU ZA QZ. If we analyse the frequencies of digrams, we can choose the next letter based upon the previous letter and the digram frequencies. E.g. Q will almost certainly be followed by U, T is most likely to be followed by H. This is called the \Define{second-order model} of English.  Random text from this model (plus space) would look like:
\begin{quote}
    \texttt{on ie antsoutinys are t inctore st be s deamy achin d ilonasive tucoowe at teasonare fuso tizin andy tobe seace ctisbe}
\end{quote}

Random text from the third-order model of English would look like:
\begin{quote}
\texttt{in no ist lat whey cratict froure birs grocid pondenome of demonstures of the reptagin is regoactiona of cre}
\end{quote}
There are finer and finer models of the English language, some based on $n$-gram frequencies, other (more accurate), based on frequencies of sequences of words. Examples of text generated from $12$-gram model (for letters) and $6$-gram model (for text) can be found in \cite[Chapter 4]{BCW90}.

One could then consider using Huffman coding (or any other statistical technique) with finer and finer models. There are two major issues with this approach.
\begin{enumerate}
    \item The alphabet of the source $X$ explodes! If we consider just the $4$-gram model (for letters), then the alphabet is of size $26^4 = 456,976$. In general, the alphabet size grows exponentially with $n$ for $n$-grams.

    \item The model is only appropriate for a particular sort of text. The model for English is inappropriate for German or French, let alone Greek, Russian or Chinese. So that strategy is not easily portable.
\end{enumerate}



\subsection{Lempel-Ziv}

The main idea of dictionary based compression is to construct a table (dictionary) of commonly used subsequences and refer to this to build the coded message. The main idea behind Lempel-Ziv (LZ77) is to use the message itself as a dictionary.

The LZ77 encoding algorithm works as follows. The encoding scans the message from first to last character. For implementation purposes, it uses a sliding window, of size $W$ and a look-ahead buffer of size $L$. Consider the message $m_1 \dots m_n$. When encoding at character $i$, look for the largest $l$ such that the first $l$ characters of the look-ahead buffer match $l$ consecutive characters in the sliding window, i.e.
\[
    m_i \dots m_{i+l-1} = m_{i-d} \dots m_{i-d+l-1}
\]
where   $d \le W$ and $l \le L$. Append the coded message with $(d,l,m_{i+l})$. Resume encoding at character $i+l+1$.

The L77 decoding algorithm reads a list of triplets $(d,l,m_{i+l})$, which it interprets as the instruction:
\begin{quote}
Print out $m_{i-d} \dots m_{i-d+l-1} m_{i+l}$ (the $l$ successive characters of $m$ starting from $d$ positions ago, and then $m_{i+l}$).
\end{quote}


\begin{example}
Encoding the sequence
\[
    ABRACADABRA
\]
using LZ77 (with say infinite $W$ and $L$) yields
\[
(0,0,A) \quad
(0,0,B) \quad
(0,0,R) \quad
(3,1,C) \quad
(2,1,D) \quad
(7,4,-)
\]
\end{example}

\begin{example}
A longer example now:
\begin{quote}
    Peter Piper picked a peck of pickled peppers;\\
    A peck of pickled peppers Peter Piper picked;\\
    If Peter Piper picked a peck of pickled peppers,\\
    Where's the peck of pickled peppers Peter Piper picked?
\end{quote}

We obtain:
\begin{align*}
&(0,0,P) (0,0,e)(0,0,t)(2,1,r)(0,0, )(6,1,i)(0,0,p)(6,3,p)(6,1,c)(0,0,k)(7,1,d)\\
&(7,1,a) (9,2,e)(9,2, )(0,0,o)(0,0,f)(17,5,l)(18,3,p)(4,1,p)(32,3,s)(0,0,;)\\
&(0,0,A)(26,24, )(71,18,;)(0,0,I)(38,2,P)(93,43,,)\\
&(0,0,W)(0,0,h)(6,2,e)(0,0,')(75,2,t)(8,2, ) (103,42,?)
\end{align*}


193 characters encoded as 34 triples.
If each triple is 3 bytes - that is 193 bytes reduced to 102 bytes.

\end{example}




\paragraph{How much space do we need?}

Each triple in the encoding includes $d \le W$, $l \le L$ and a character. For ASCII, the character takes $8$ bits. In total, we need
\[
    \log_2 ( W + 1 ) + \log_2 (L + 1) + 8
\]
bits to encode a triple.

Typical values are $W = 2^{16} - 1 = 65535$, and $L = 2^8 - 1 = 255$, so we need $16+8+8$ bits per triple, i.e. $4$ bytes. This much can be wasteful, especially if the value of $l$ is very low ($l=0$ means that this is a new character for instance).


\subsection{Applications of LZ77}

\paragraph{LZSS} Lempel-Ziv-Storer-Szymanski (\Define{LZSS}) is a popular variant of LZ77 introduced in 1982. The main improvement is that it includes a flag to distinguish between new characters and tokens. That way, a new character does not need to be encoded as a full token, and tokens only have two fields instead of three.

\paragraph{DEFLATE} \Define{Deflate} is a lossless compression technique that combines LZSS and Huffman coding. The key idea is that Lempel-Ziv removes structural redundancy from the data, but its output still has some statistical redundancy; the latter is then removed by Huffman coding.

Deflate is everywhere: in \texttt{gzip}, in the \Important{ZIP} file format, in \Important{PNG}, etc. (Technically, ZIP allows for many different compression techniques, but Deflate is the one that's used most of the time.)

\paragraph{LZMA} The Lempel–Ziv–Markov chain algorithm (\Define{LZMA}) was developed for 7z. Its description is out of the scope of these lectures.

\subsection{See further}

\paragraph{Variants} There are many variants of LZ77, e.g. LZX, LZRW1, LZRW4. Have a look!

\paragraph{VCDIFF} \Define{File differencing} refers to any method that compresses the differences between two files (say the source and the target files). The term \Important{delta compression} is also used. VCDIFF is a method for file differencing based on LZ77. The basic idea is very simple:
\begin{enumerate}
    \item append the target file to the source file to make one massive file

    \item use LZ77 to compress that massive file

    \item only save the part relating to the target file of the output of LZ77.
\end{enumerate}
The implementation is more involved; see \cite{KMMV02}. In general, delta compression is an important problem that is still the subject of ongoing research.


\subsection{Exercises}

\begin{exercise}
Decode the following string encoded with LZ77.
\begin{align*}
&(0,0,r)(0,0,i)(0,0,n)(0,0,g)(0,0, ) \\
& (0,0,a)(2,1,r)(7,4,o)(7,2,o)(0,0,s)(9,1,e)(3,1, )\\
& (16,2,p)(9,1,c)(0,0,k)(9,1,t)(7,1,f)(0,0,u)(0,0,l)(1,1, )\\
& (11,1,f)(15,3,s)(24,5,t)(6,1,s)(0,0,h)(11,1,o)(8,9,w)(20,1, )\\
& (11,1,l)(33,2,f)(5,4,d)(15,1,w)(0,0,n)
\end{align*}
\end{exercise}


\begin{exercise}
Watch this youtube video on the repetitiveness of pop music: \href{https://www.youtube.com/watch?v=_tjFwcmHy5M}{\textcolor{blue}{Pop Music is Stuck on Repeat}}. Can you guess which is the most repetitive song in the history of the Billboard Hot 100?
\end{exercise}

\begin{exercise}
Write your own LZ77 encoder (in Python, Java, or any other language). Can you find famous pieces of fiction that compresses massively, or hardly at all?
\end{exercise}








\section{Lempel-Ziv II: LZ78}
\label{sec:04}


\subsection{LZ78}

\paragraph{Basic idea} The \Define{LZ78} method does not use any search buffer, look-ahead buffer, or sliding window. Instead, it simply keeps a dictionary of previously encountered strings. The dictionary starts with the empty string at position zero and its size is only limited by the memory size.

The encoder outputs \Important{two-field tokens} (instead of three-field tokens in LZ77). Each token simply corresponds to a new string in the dictionary: it is of the form
\[
    (i, x),
\]
where $i$ is the position of the longest match in the dictionary and $x$ is the final character of the string.

Nothing is ever deleted from the dictionary:
\begin{itemize}
    \item Advantage over LZ77: future strings will be compressed even if they only match strings in the distant past;

    \item Drawback: the dictionary can become very large!
\end{itemize}

\paragraph{Example} Once again, it is best explained via a simple example. Say we want to compress
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily%\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}

The tokens are then (in order!):\\
~\\
\begin{tabular}{l|l|l}
     Dictionary position    & String    & Token  \\
     \hline
     0                      &  $\epsilon$         &        \\
     1                        & s           &  (0, s)      \\
     2                   & i          & (0,i)       \\
     3                   &  r         & (0,r)       \\
     4                   &  \textvisiblespace         & (0,\textvisiblespace)       \\
     5                   &  si         &    (1,i)    \\
     6                   &  d         & (0,d)       \\
     7                   &  \textvisiblespace e         &  (4,e)      \\
     8                   &  a         & (0,a)       \\
     9                   &  st         &    (1,t)    \\
     10                   & m          &    (0,m)    \\
     11                   & an          &   (8,n)     \\
     12                   & \textvisiblespace ea          & (7,a)       \\
     13                   & sil          &  (5,l)      \\
     14                   & y          &    (0,y)
\end{tabular}
~\\
And the compressed output is the list of tokens
\begin{quote}
    (0,s) (0,i) (0,r) (0,\textvisiblespace) (1,i) (0,d) (4,e) (0,a) (1,t) (0,m) (8,n) (7,a) (5,l) (0,y)
\end{quote}

Once again, the decoder sees these tokens as ``instructions.'' But following these instructions means searching in the dictionary. A useful data structure for the dictionary is a \Important{tree}, where the root is the empty string and a new string is added to the tree as a child of the string it refers to on its token. Such a tree is called a \Define{trie}.

\begin{tikzpicture}
    \node (0) at (3,0) {$\epsilon$};

    \node (1) at (0,-1) {s};
    \node (2) at (1,-1) {i};
    \node (3) at (2,-1) {r};
    \node (4) at (3,-1) {\textvisiblespace};
    \node (5) at (0,-2) {si};
    \node (6) at (4,-1) {d};
    \node (7) at (3,-2) {\textvisiblespace e};
    \node (8) at (5,-1) {a};
    \node (9) at (1,-2) {st};
    \node (10) at (6,-1) {m};
    \node (11) at (5,-2) {an};
    \node (12) at (3,-3) {\textvisiblespace ea};
    \node (13) at (0,-3) {sil};
    \node (14) at (7,-1) {y};

    \draw (0) -- (1);
    \draw (0) -- (2);
    \draw (0) -- (3);
    \draw (0) -- (4);
    \draw (1) -- (5);
    \draw (0) -- (6);
    \draw (4) -- (7);
    \draw (0) -- (8);
    \draw (1) -- (9);
    \draw (0) -- (10);
    \draw (8) -- (11);
    \draw (7) -- (12);
    \draw (5) -- (13);
    \draw (0) -- (14);


\end{tikzpicture}



\subsection{LZW}


\paragraph{Basic idea} Lempel-Ziv-Welch (\Define{LZW}) is a variant of LZ78, with two main differences.
\begin{enumerate}
    \item The dictionary is \Important{initialised with all possible characters}. If we are compressing an ASCII file, then positions 0 to 255 are filled at initialisation.

    \item The tokens only have \Important{one field}! Since we always work with at least one character (that can always be found in the dictionary), there is no need to output the next character.
\end{enumerate}

Let us go back to our example:
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily%\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}

The dictionary is initialised with all 256 ASCII characters in positions 0 to 255, e.g. a is in position 97, b in 98, s in 115, z in 122. The first character in the string is s (in the dictionary at position 115). Since si does not appear in the dictionary, we add si to the dictionary at 256, and we continue with the character i. Again, since ir is not in the dictionary, we add ir at 257 and continue with the character r.

The dictionary (omitting positions 0 to 255) and the tokens look like this:\\
~\\
\begin{tabular}{l|l|l|l}
     Position & String & Token & What the token encodes \\
     \hline
     256 & si & 115 & s\\
     257 & ir & 105 & i\\
     258 & r\textvisiblespace & 114 & r\\
     259 & \textvisiblespace a & 32 & \textvisiblespace\\
     260 & sid & 256 & si\\
     261 & d\textvisiblespace & 100 & d\\
     262 & \textvisiblespace e & 32 & \textvisiblespace\\
     263 & ea & 101 & e\\
     264 & as & 97 & a\\
     265 & st & 115 & s\\
     266 & tm & 116 & t\\
     267 & ma & 109 & m\\
     268 & an & 97 & a\\
     269 & n\textvisiblespace & 110 & n\\
     270 & \textvisiblespace ea & 262 & \textvisiblespace e\\
     271 & asi & 264 & as\\
     272 & il & 105 & i\\
     273 & ly & 108 & l\\
        &   & 121 & y
\end{tabular}
~\\~\\
The output is then
\[
    115, 105, 114, 32, 256, 100, 32, 101, 97, 115, 116, 109, 97, 110, 262, 264, 105, 108, 121
\]

The dictionary can once again be stored as a tree, but the implementation is more complex than for LZ78. A thorough description is given in \cite[3.13.2]{Sal04}.


\subsection{Applications of LZW}

\paragraph{GIF} The ubiquitous Graphics Interchange Format (\Define{GIF}) uses a variation of LZW. It uses a dynamic, growing dictionary. It starts with the number of bits per pixel $b$: $b=2$ for monochromatic images, $b=8$ for an image with $256$ colours of shades of grey. The dictionary starts with $2^{b+1}$ entries and is doubled in size every time it fills up until it reaches $2^{12} = 4,096$ entries. At that point, the encoder may want to start a new dictionary!

GIF is not actually that good at image compression because it is unidimensional. It scans the image row after row, so it can detect similarities within a row but has trouble dealing with similarities across rows instead.

\paragraph{Limitations} One major issue of using LZW (e.g. for GIF), is that LZW is \Important{patented}. In response to that, the Portable Network Graphics format was created in the mid-90s (finalised in 96). It is based on DEFLATE (and hence LZSS) instead.

Another application of LZW was the Unix shell compression utility \texttt{compress}, that was used in the 80s. However, it was superseded by \texttt{gzip}, which typically outperforms it in terms of compression ratio.

\subsection{See further}

\paragraph{Variants} LZ78 and LZW also have a few variants, notably LZMW, LZAP and LZY. Have another look!

\paragraph{Kolmogorov complexity} The principle of Lempel-Ziv encoding is to construct a list of instructions to the decoder of the form ``Copy that string (and add that character).'' But what if we allowed any sort of instructions?

The \Define{Kolmogorov complexity} is a concept that predates Lempel-Ziv. It aims at evaluating the ``intrinsic'' complexity of a binary string. Simply put, the Kolmogorov complexity of a string $x$ w.r.t. a Turing machine $U$, denoted $K_U(x)$, is the shortest length of a program for $U$ that prints out $x$ and halts. Obviously, $K_U(x)$ is not computable. But still, we can say a lot about the Kolmogorov complexity of a random string: it's about the length of the string. Therefore, almost any string is incompressible! The study of Kolmogorov complexity and associated concepts (e.g. Solomonoff's universal probability or Chaitin's Omega number) is very intriguing but outside the scope of this course.



 \subsection{Exercises}

\begin{exercise}
Encode the string
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}
with LZ78: give the dictionary table, the trie, and the output.

Encode the same string with LZW.
\end{exercise}


\begin{exercise}
Select a few (small) images, and compare the file sizes for those when saved as .bmp, .gif and .png.
\end{exercise}










\section{Context-based compression}
\label{sec:05}



\subsection{Context}

\paragraph{Context-based compression} Statistical compression (mainly for text, but not only) can be based on two properties. The first property is the frequency of symbols: the model assigns probabilities to the symbol according to their frequency in the document.

The second one is the \Important{context}. In practice, the context a symbol consists of the $N$ symbols preceding it (note that we cannot use symbols succeeding it, as the decoder typically does not know them yet!). Context-based compression then uses the context of a symbol to \Important{predict} it (i.e. to assign it a probability).

For instance, let's use a context of only one character. The letter h occurs in typical English text only about 5\% of the time. However, if the current symbol is t, then there is a much higher probability (around 30\%) that the next symbol will be h, since the digram th is very common in English. Note that the prediction is about assigning probabilities, not trying to figure out the next symbol exactly (which is impossible).

\paragraph{Static v Adaptive contexts} A static context-based modeler always uses the same probabilities, which are stored in some large table. Those probabilities are usually obtained by crawling through many documents (say typical English texts). There are issues with that approach, notably the fact that this might assign zero probabilities to some strings.

An adaptive context-based modeler also maintains tables of probabilities of all the possible digrams (or trigrams, or even longer $n$-grams). But this time the tables are updated all the time as more data are input, which adapts the probabilities to the particular data being compressed. \Important{Adaptive context-based compression} might be slower, but typically results in better compression.



\paragraph{Context length} One may think at first that the larger the number $N$ of symbols in the context, the better the compression. However, this might not be the case:
\begin{enumerate}
    \item A large $N$ requires to write the first $N$ symbols in plain text, which might hurt the overall compression.

    \item If $N$ is too large, then there are simply too many contexts, which makes storing, reading off, and writing on the table of probabilities infeasible.

    \item A very long context contains information about the nature of old data. It is not uncommon to have files where different parts have different symbol distributions.
\end{enumerate}
Therefore, in practice relatively small contexts are used in practice (for text compression, traditional methods use no more than 10 characters).


\subsection{PPM}

\paragraph{Basic idea} Prediction by Partial Matching (\Define{PPM}) is based on an encoder that keeps a statistical model of the text. It starts with an order-$N$ context. It searches its data structure for an occurrence of the current context $C$ followed by the next symbol $S$. If it finds no such occurrence, if decreases the order of the context to $N-1$ and tries again (the new context $C'$ is the final $N-1$ characters of $C$). It keeps \Important{shortening the context} until it is successful.

The encoder reads the next symbol $S$ from the input stream, looks at the current order-$N$ context $C$ (the last $N$ symbols read), and based on the previous input data, computes the probability $P$ that $S$ will appear following $C$. The encoder then calls an adaptive arithmetic encoder to encode $S$ with probability $P$. If the probability $P$ is zero, the PPM encoder tries with a smaller context; it reduces the context until $P \ne 0$. What if the symbol $S$ has not been seen yet (and hence, even with order-$0$ context, we still have $P = 0$)? Then the PPM encoder enters \Define{order-$(-1)$ context}, where the probability of $S$ is simply $1/$(size of alphabet).

\begin{example} \label{example:ppm_contexts}
Let us look at the contexts and frequency counts for the following string with $11$ symbols:
\begin{quote}
    xyzzxyxyzzx
\end{quote}

\begin{tabular}{r l | r l | r l | r l | r l}
    Order 4 & & Order 3 & & Order 2 & & Order 1 & & Order 0 &   \\
    \hline
    xyzz $\to$ x & 2 & xyz $\to$ z & 2 & xy $\to$ z & 2 & x $\to$ y & 3 & x & 4\\
    yzzx $\to$ y & 1 & yzz $\to$ x & 2 & xy $\to$ x & 1 & y $\to$ z & 2 & y & 3\\
    zzxy $\to$ x & 1 & zzx $\to$ y & 1 & yz $\to$ z & 2 & y $\to$ x & 1 & z & 4\\
    zxyx $\to$ y & 1 & zxy $\to$ x & 1 & zz $\to$ x & 2 & z $\to$ z & 2 &  & \\
    xyxy $\to$ z & 1 & xyx $\to$ y & 1 & zx $\to$ y & 1 & z $\to$ x & 2 &  & \\
    yxyz $\to$ z & 1 & yxy $\to$ z & 1 & yx $\to$ y & 1 & & &  &
\end{tabular}
\end{example}


Now, how does the encoder tell the decoder which order context it is currently using (and hence what the decoder should be using too)? The answer is to have a dedicated \Important{escape symbol}, which we'll denote esc, which should be output whenever the context size is decreased. Since this is a new character, we should also assign a probability for the escape symbol for every encountered context. There are various ways (heuristics) of assigning such probabilities. Here, we will use the so-called Method A, where the escape symbol is assigned a frequency of 1.


We are now in position to give a more explicit example. Encoding a full sequence is actually quite tedious to explain so we'll only encode a few characters. We use contexts of order at most 2. Let us consider
\begin{quote}
    this\textvisiblespace is\textvisiblespace the\textvisiblespace tithe
\end{quote}
The first few symbols are not very interesting, so let us skip forward. Let's assume we have already encoded ``this\textvisiblespace is'' and we wish to encode the next character \textvisiblespace.

We assume the word length for arithmetic coding is six bits (we used four decimal digits in our example in Lecture \ref{sec:02}). For the sake of simplicity, we have $Low = 0$ and hence $L^* = 000000$ and $High = 1$ hence $H^* = 111111$. (As we shall see, the low and high values may vary over time.)

Here is what the table of contexts looks like\\
~\\
\begin{tabular}{lr | lr | lr}
    Order 2 &           & Order 1 & & Order 0   \\
    \hline
    th $\to$ i      & 1 & t $\to$ h & 1 & t & 1\\
    th $\to$ esc    & 1 & t $\to$ esc & 1 & h & 1 \\
    hi $\to$ s      & 1 & h $\to$ i & 1 & i & 2\\
    hi $\to$ esc    & 1 & h $\to$ esc & 1 & s & 2 \\
    \Important{is $\to$ \textvisiblespace}      & \Important{1} & i $\to$ s & 2 & \textvisiblespace & 1\\
    \Important{is $\to$ esc}    & \Important{1} & i $\to$ esc & 1 & esc & 1 \\
    s\textvisiblespace $\to$ i      & 1 & \textvisiblespace $\to$ i & 1 \\
    s\textvisiblespace $\to$ esc    & 1 & \textvisiblespace $\to$ esc & 1 \\
    \textvisiblespace i $\to$ s      & 1 & s $\to$ \textvisiblespace & 1 \\
    \textvisiblespace i $\to$ esc    & 1 & s $\to$ esc & 1
\end{tabular}
~\\
~\\
The  second-order context is ``\Important{is}''. We use characters in the order of the table: the first row gives the first interval and so on. In this context, the probability of the space sign ``\textvisiblespace'' and the probability of the escape symbol esc are both equal to 1/2, and
\[
    L(\text{\textvisiblespace}) = 0, H(\text{\textvisiblespace}) = 1/2 = L(esc), H(esc) = 1.
\]
    The update equations for the new $Low$ and $High$ are
\begin{align*}
    Low &\gets Low + (High - Low) L(x) = 0,\\
    L^* &\gets 000000,\\
    High & \gets Low + (High - Low) H(x) = 1/2,\\
    H^* &\gets 011111.
\end{align*}
Since the first (most significant) bit of $L^*$ and $H^*$ coincide, we shift that bit out and shift $0$ into $L^*$ and shift $1$ into $H^*$. So we obtain:
\begin{enumerate}
    \item Encoded sequence for ``\textvisiblespace'': $0$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}

The table of contexts now becomes:\\
~\\
\begin{tabular}{lr | lr | lr}
    Order 2 &           & Order 1 & & Order 0   \\
    \hline
    th $\to$ i      & 1 & t $\to$ h & 1 & \Structure{t} & \Structure{1}\\
    th $\to$ esc    & 1 & t $\to$ esc & 1 & h & 1 \\
    hi $\to$ s      & 1 & h $\to$ i & 1 & i & 2\\
    hi $\to$ esc    & 1 & h $\to$ esc & 1 & s & 2 \\
    is $\to$ \textvisiblespace      & 2 & i $\to$ s & 2 & \textvisiblespace & 2\\
    is $\to$ esc    & 1 & i $\to$ esc & 1 & esc & 1 \\
    \Structure{s\textvisiblespace $\to$ i}      & \Structure{1} & \Structure{\textvisiblespace $\to$ i} & \Structure{1} \\
    \Structure{s\textvisiblespace $\to$ esc}    & \Structure{1} & \Structure{\textvisiblespace $\to$ esc} & \Structure{1} \\
    \textvisiblespace i $\to$ s      & 1 & s $\to$ \textvisiblespace & 2 \\
    \textvisiblespace i $\to$ esc    & 1 & s $\to$ esc & 1
\end{tabular}
~\\
~\\
The next symbol is ``t''. The second-order context is ``\Structure{s\textvisiblespace }''. Since ``t'' has zero frequency in this context, we need to encode the escape symbol. By a similar argument as above, we obtain
\begin{enumerate}
    \item Encoded escape symbol sequence: $1$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
We need to look at the first-order context, which is ``\Structure{\textvisiblespace}''. Again, ``t'' does not appear with this context, so we encode another escape symbol. We obtain
\begin{enumerate}
    \item Encoded escape symbol sequence: $1$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
We need to look at the zero-th order context. This time, ``t'' has already appeared, and is assigned the interval $[0, 1/9 )$. We then have
\begin{align*}
    Low &\gets Low + (High - Low) L(x) = 0,\\
    L^* &\gets 000000,\\
    High & \gets Low + (High - Low) H(x) = 1/9,\\
    H^* &\gets 000111.
\end{align*}
Since the three leftmost bits are equal, we shift them out. We finally obtain
\begin{enumerate}
    \item Encoded sequence: for ``t'': $000$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
So, to encode ``\textvisiblespace t'', we have transmitted $011000$.

Note that there would be a slight difference in practice: to keep everything integral, we would use $High = 63$, $Low = 0$ and perform an update of the form
\begin{align*}
    Low &\gets Low + \left\lfloor (High - Low + 1) \frac{ 1 }{ 9 }  \right\rfloor  = 0 = 000000\\
    High & \gets High + \left\lfloor (High - Low + 1) \frac{ 1 }{ 9 }  \right\rfloor - 1  = 6 = 000110
\end{align*}
Then we would have: Higher bound $H^* = 110111$.



\paragraph{Methods B and C} Two other main ways of assigning frequencies to the escape symbols aim to make the escape symbol more probable, which typically reduces the size of the resulting sequence for that symbol. The main idea is that if a context is followed by many different characters, then you are likely to encounter yet another character following that same context. For instance, think of the context ``s'' in English, which can be followed by virtually any other letter. \Important{Methods B and C} give the escape symbol a count equal to the number of symbols following the context; Method B then subtracts the count of every other symbol by one, while Method C does not amend those.


\subsection{See further}

\paragraph{RAR} The main application of PPM is in the Roshal Archive (\Define{RAR}) file format.


\paragraph{Context mixing} In \Define{context mixing}, the next-symbol predictions of two or more statistical models are combined to yield a prediction that is often more accurate than any of the individual predictions. The \Important{PAQ} series of data compression programs use context mixing; they are the cutting edge in lossless compression in terms of compression ratio (at the expense of speed and memory usage) \cite{KF11}.

Note that the problem of mixing different contexts is a very challenging issue in machine learning; that could be a very interesting topic for a project...


\paragraph{BWT} The Burrows-Wheeler transform (\Define{BWT}) is a very clever way of converting a list of symbols into one that is much more structured. You only need a little more information to make sure that the transform does not lose any information. By structured, we mean that it is ``almost sorted.'' After the transform, one can use very simple techniques to efficiently encode the structured list. Unfortunately, BWT-based compression requires to scan and to manipulate the whole message, which is an important drawback compared to the adaptive PPM.


\subsection{Exercises}


\begin{exercise} \label{exercise:ppm_contexts}
Update the table in Example \ref{example:ppm_contexts} if the following character is x.
\end{exercise}

\begin{exercise}
Finish the encoding of the sequence ``this\textvisiblespace is\textvisiblespace the\textvisiblespace tithe''. To update $High$ and $Low$, you may use our simple technique based on rational numbers, or use the version with integers instead.
\end{exercise}




\section{Transform coding I: Mathematical background}
\label{sec:06}



\subsection{Fourier series and Fourier transform}

\paragraph{Fourier series}
The representation of periodic functions in terms of a series of sines and cosines was first used by Fourier in 1812. This idea has spread like wildfire across mathematics and its applications and is used in a ridiculous range of areas under different guises.

In this section, we want to give an introduction to Fourier series. We shall be rather informal: we will not prove many of the claimed results, and we will not worry about the precise assumptions we make.

Let $f$ be a periodic function of period $T$, i.e.
\[
    f(t) = f(t + nT) \quad \forall t \in \R, n \in \Z.
\]
Then we can write $f(t)$ as
\[
    f(t) = \sum_{k=-\infty}^\infty c_k e^{\i k \omega t},
\]
where $\i = \sqrt{-1}$ and
\[
    \omega = \frac{2 \pi}{ T }.
\]
Equivalently, we have
\[
    f(t) = a_0 + \sum_{k=1}^\infty a_k \cos ( k \omega t ) + \sum_{k=1}^\infty b_k \sin ( k \omega t ).
\]
In other words, the periodic functions of period $T$ form a vector space, and $\{ e^{\i n \omega t} \}$ forms a basis. In fact, this basis is orthonormal w.r.t. the inner product
\[
    \langle f(t), g(t) \rangle = \frac{1}{T} \int_0^T f(t) g^*(t) \ dt,
\]
where $g^*(t)$ denotes the complex conjugate of $g$. The coefficients can then be obtained as follows:
\begin{equation} \label{eq:fourier}
    c_k = \langle f(t), e^{\i k \omega t} \rangle = \frac{1}{T} \int_0^T f(t) e^{-\i k \omega t} dt.
\end{equation}

The intuition behind this representation is as follows. Suppose we have a periodic signal $f$. Then its $c_k$ Fourier coefficients decompose the signal into basic fluctuating signals; each $e^{\i k \omega t}$ fluctuates at a frequency of $k \omega/(2 \pi)$. As such, these coefficients give us a measure of the different amounts of fluctuation in the signal.

\paragraph{Discrete Fourier transform}
We want to handle signals that are not periodic, but instead limited in time. Suppose we have a signal $f(t)$ that is limited in time (say from $t=0$ to $t = t_1$). Then we can extend it to a periodic signal by doing
\[
    f_P(t) = \sum_{n \in \Z} f(t - nT),
\]
where $T > t_1$. This is the so-called periodic extension of $f$.

Moreover, we are dealing with discrete signals: instead of $f(t)$, we are considering $\{f_0, f_1, \dots, f_{N-1} \}$. We can discretise Equation \eqref{eq:fourier} as follows:
\[
    F_k = \frac{1}{\sqrt{N}} \sum_{n=0}^{N-1}  f_n e^{-\i \frac{2 \pi k n }{N}} \quad k = 0, 1, \dots, N-1.
\]
The coefficients $F_k$ are called the discrete Fourier transform (\Define{DFT}) of $f$. We can recover the signal from its DFT by
\[
    f_n = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} F_k e^{\i \frac{2 \pi k n }{N}} \quad n = 0, 1, \dots, N-1.
\]

\paragraph{Matrix representation of DFT}
Let $f = (f_0, f_1, \dots, f_{N-1})$ and let $F = (F_0, \dots, F_{n-1})$ be its DFT. Then we have
\[
    F = {\bf A} f,
\]
where ${\bf A}$ is an $N \times N$ matrix with coefficients
\[
    a_{i,j} = \frac{1}{\sqrt{N}} e^{- \i \frac{2\pi}{N} ij}.
\]
For example, with $N = 4$ we obtain
\[
    {\bf A} = \frac{1}{\sqrt{4}} \begin{pmatrix}
    1 & 1 & 1 & 1\\
    1 & -\i & -1 & \i\\
    1 & -1 & 1 & -1\\
    1 & \i & -1 & -\i
    \end{pmatrix}
\]

\paragraph{Unitary and orthogonal transforms}
The matrix ${\bf A}$ from the DFT is \Define{unitary}: it satisfies
\[
    {\bf A}^\dagger = {\bf A}^{-1},
\]
where ${\bf A}^\dagger$ is its conjugate transpose: $a^\dagger_{i,j} = a_{j,i}^*$. It is easily shown that the following are equivalent for a matrix ${\bf U}$:
\begin{enumerate}
    \item ${\bf U}$ is unitary;

    \item ${\bf U}$ preserves the inner product, i.e.
    \[
        \langle x,y \rangle = \langle {\bf U} x, {\bf U}y \rangle;
    \]

    \item ${\bf U}$ preserves the norm, i.e.
    \[
         \| x \|^2 = \sum_{i=0}^{N-1} |x_i|^2 = \langle x,x \rangle = \langle {\bf U} x, {\bf U}x \rangle = \sum_{i=0}^{N-1} |({\bf U}x)_i|^2 = \| {\bf U} x \|^2.
    \]
\end{enumerate}

We will not actually use the DFT and instead we will restrict ourselves to real matrices. A real unitary matrix is called \Define{orthogonal}. So an orthogonal matrix is a real matrix ${\bf A}$ such that
\[
    {\bf A}^{-1} = {\bf A}^\top.
\]



\subsection{Two-dimensional transforms}

\paragraph{Two-dimensional data}
In general, in one dimension, we could apply any orthogonal transform as such:
\[
    \theta = {\bf A}x, \quad x = {\bf A}^\top \theta.
\]
We will use transforms for two-dimensional data (small blocks of pixels). How are we going to apply a one-dimensional transform to two-dimensional data? The answer is actually easy: we apply the transform column-wise and row-wise.

\paragraph{Matrix form} More succinctly, let ${\bf X}$ be an $N \times N$ matrix with entries $\{x_{i,j} : i,j = 0, \dots, N-1\}$. We then perform
\[
    \Theta = {\bf A} {\bf X} {\bf A}^\top.
\]
($\Theta$ is another $N \times N$ matrix.) Multiplying on the left by ${\bf A}$ performs the transform column-wise, while multiplying on the right by ${\bf A}^\top$ performs the transform row-wise. By associativity,
\[
    \Theta = ({\bf A} {\bf X}) {\bf A}^\top = {\bf A} ( {\bf X} {\bf A}^\top ),
\]
and hence the order does not matter!

Note that the inverse is straightforward:
\[
    {\bf X} = {\bf A}^\top \Theta {\bf A}.
\]

\paragraph{Basis matrices}
Let ${\bf X}$ be an $N \times N$ matrix. For all $i,j \in \{0, \dots, N-1\}$, let ${\bf E}_{i,j}$ be the matrix with a single $1$ in position $(i,j)$ and $0$ everywhere else. Then those matrices form a basis for the vector space of all $N \times N$ matrices, and we have the decomposition
\[
    {\bf X} = \sum_{i=0}^{N-1} \sum_{j=0}^{N-1} x_{i,j} {\bf E}_{i,j}.
\]
(Trivial, isn't it?)

Any two-dimensional orthogonal transform also yields a similar decomposition. We still denote $\Theta = {\bf A} {\bf X} {\bf A}^\top$, and we denote the entries of $\Theta$ as $\theta_{i,j}$. We then have
\begin{align*}
    {\bf X} &= {\bf A}^\top \Theta {\bf A}\\
            &= {\bf A}^\top \left( \sum_{i,j} \theta_{i,j} {\bf E}_{i,j} \right) {\bf A}\\
            &= \sum_{i,j} \theta_{i,j} {\bf A}_{i,j},
\end{align*}
where the matrices ${\bf A}_{i,j} := {\bf A}^\top {\bf E}_{i,j} {\bf A}$ are the \Define{basis matrices} of the transform. More concretely, denote the $k$-th row of ${\bf A}$ as $a_k$, then
\[
    {\bf A}_{i,j} = a_i^\top a_j.
\]

For instance, let
\[
    {\bf A} =  \frac{1}{ \sqrt{2} }
    \begin{pmatrix}
    1 & 1\\
    1 & -1
    \end{pmatrix}.
\]
Then the four basis matrices are given by
\begin{alignat*}{3}
    {\bf A}_{0,0} &= \frac{1}{2}
    \begin{pmatrix}
    1 & 1\\
    1 & 1
    \end{pmatrix} &
    \qquad & &
    {\bf A}_{0,1} &= \frac{1}{2}
    \begin{pmatrix}
    1 & -1\\
    1 & -1
    \end{pmatrix} \\
    {\bf A}_{1,0} &= \frac{1}{2}
    \begin{pmatrix}
    1 & 1\\
    -1 & -1
    \end{pmatrix} &
    \qquad & &
    {\bf A}_{1,1} &= \frac{1}{2}
    \begin{pmatrix}
    1 & -1\\
    -1 & 1
    \end{pmatrix}.
\end{alignat*}




\subsection{Modus operandi of transform coding}


\paragraph{Norm and energy}
The square of the norm of a vector can be viewed as its \Important{energy}. Orthogonal transforms preserve the energy (as they are unitary matrices). The efficacy of a transform depends on how much energy compaction is provided by the transform. Intuitively, the transform helps to accumulate the energy on a few symbols; those symbols should be kept, while the other ones can be discarded at little loss.

One measure of the energy compaction offered by a transform is the \Define{transform coding gain}, defined as follows (for simplicity, we define it for one-dimensional data). Let $\sigma_i^2$ be the variance of the transformed coefficient $\theta_i$ for $i=0, \dots, N-1$, then the transform coding gain is the ration of the arithmetic mean of variances over their geometric mean:
\[
    G_{\mathrm{TC}} := \frac{ \frac{1}{N} \sum_{i=0}^{N-1} \sigma_i^2 }{ \left(\sum_{i=0}^{N-1} \sigma_i^2 \right)^{\frac{1}{N}} }.
\]
(The derivation of this gain is out of the scope of this course.)


\paragraph{Three steps of transform coding}
Transform coding consists of three steps.
\begin{description}
\item[Step 1: Transform] First, the data is split into blocks of size $N$. Each block is mapped into a transform sequence using a reversible transform (usually orthogonal).

\item[Step 2: Quantization] Secondly, the transformed sequence is quantized. The quantization strategy depends on three main factors:
\begin{enumerate}
    \item the desired average bit rate

    \item the statistics of the various elements of the transformed sequence

    \item the effect of distortion in the transformed coefficients on the reconstructed sequence.
\end{enumerate}
Quantization is an important problem with some nice maths behind it, but we will skip the details here.

\item[Step 3: Encoding] Thirdly, the quantized value is encoded using some binary encoding technique, e.g. Huffman or arithmetic coding.
\end{description}









\subsection{See further}

\paragraph{FFT} The Fast Fourier Transform (\Define{FFT}) is, as its name suggests, a very efficient way of computing the DFT via divide and conquer. It was due to Cooley and Tuckey in 1965\dots even though Gauss had already discovered it in 1805!


\paragraph{Karhunen-Lo\`eve transform}
The \Define{KL} transform is the one that maximises the transform coding gain. However, it is based on the actual data and is impractical for our purposes.













\subsection{Exercises}

\begin{exercise}
Walsh-Hadamard transform. For all $N$ a power of $2$, the discrete Walsh-Hadamard transform (\Define{DWHT}) is defined as follows. For $N = 1$, let ${\bf H}_1 = (1)$. For $N \ge 1$, let
\[
    {\bf H}_{2N} = C \begin{pmatrix}
    {\bf H}_N & {\bf H}_N\\
    {\bf H}_N & - {\bf H}_N
    \end{pmatrix}.
\]
(This transform comes under different names; to make matters worse, those matrices are sometimes called Sylvester matrices.)
\begin{enumerate}
    \item Verify that the Discrete Walsh-Hadamard Transform is indeed orthogonal for the right choice of constant $C$.

    \item Give ${\bf H}_2$, ${\bf H}_4$ and ${\bf H}_8$.

    \item Give all sixteen basis matrices for $N = 4$.

    \item Find a closed form formula for the $(i,j)$ entry of ${\bf H}_N$.
\end{enumerate}
\end{exercise}


\begin{exercise}
Give the sixteen basis matrices for the DFT with $N = 4$.
\end{exercise}

\begin{exercise}
Consider the data
\[
    {\bf X} = \begin{pmatrix}
    4 & 3 & 2 & 1\\
    3 & 2 & 1 & 1\\
    2 & 1 & 1 & 0\\
    1 & 1 & 0 & 1
    \end{pmatrix}.
\]
\begin{enumerate}
    \item Compute its two-dimensional DFT.

    \item Compute its two-dimensional DWHT.
\end{enumerate}
\end{exercise}
































































\subsection{Exercises}

\begin{exercise} \label{exercise:arithmetic}
With the same source as in Example \ref{example:arithmetic}, encode the message $m = a_2a_3a_1a_1a_3$.
\end{exercise}



\begin{exercise} \label{exercise:swiss_miss}
Work out a decoder for the four-digit implementation of arithmetic coding, and decode the output of the SWISS MISS example.
\end{exercise}






% \subsection{Answers}








\section{Wavelet coding II: JPEG 2000}
\label{sec:09}


\subsection{Objectives of JPEG 2000}

\begin{itemize}
    \item High compression efficiency. Bitrates of less than 0.25 bpp are expected for highly detailed grayscale images.

\item The ability to handle large images, up to $2^{32} \times 2^{32}$ pixels (the original JPEG can handle images of up to $2^{16} \times 2^{16}$).

\item Progressive image transmission. The proposed standard can decompress an image progressively by SNR, resolution, color component, or region of interest.

\item Easy, fast access to various points in the compressed stream.
The decoder can pan/zoom the image while decompressing only parts of it. The decoder can rotate and crop the image while decompressing it.

\item Error resilience. Error-correcting codes can be included in the compressed stream, to improve transmission reliability in noisy environments.
\end{itemize}


\begin{center}
    \includegraphics[width=10cm]{DataCompression/Jpeg2000.png}
\end{center}



\subsection{Operations before wavelet transform coding}


A colour image consists of three colour components (typically RGB). First of all, we perform a DC level shifting on each component. If the pixel values range in the interval $[0, 2^k - 1]$, then we subtract $2^k - 1$ to each pixel value. Then we transform the components by means of either a reversible component transform (\Important{RCT}) or an irreversible component transform (\Important{ICT}). Both are very similar to the YCbCr conversion used in JPEG. The RCT is used for \Important{lossless compression}; the ICT is used for \Important{lossy compression}. Each transformed component is then compressed separately.

More precisely, let $I_0, I_1, I_2$ denote the three colour components of a certain pixel (after DC shifting). Then the RCT is the triple $(Y_0, Y_1, Y_2)$ given by
\begin{align*}
    Y_0 &= \left\lfloor \frac{I_0 + 2 I_1 + I_2}{4} \right\rfloor,\\
    Y_1 &= I_2 - I_1,\\
    Y_2 &= I_0 - I_1.
\end{align*}
The ICT is given by
\begin{align*}
    Y_0 &= 0.299 I_0 + 0.587 I_1 + 0.144 I_2,\\
    Y_1 &= -0.16875 I_0 - 0.33126 I_1 + 0.5 I_2,\\
    Y_2 &= 0.5 I_0 - 0.41869 I_1 - 0.08131 I_2.
\end{align*}

\paragraph{Tiling}
Each transformed colour component of the image is partitioned into rectangular, nonoverlapping \Important{tiles}. Since the colour components may have different resolutions, they may use different tile sizes. Tiles may have any size, up to the size of the entire image (i.e. one tile). All the tiles of a given colour component have the same size, except those at the edges. Each tile is compressed individually.




\subsection{The wavelets used}

JPEG 200 used two different wavelets, one for lossy compression and the other for lossless compression. The details are given below, but of course you needn't remember them.

We now denote a row of extended pixels in a tile by $P(k), P(k+1), \dots , P(m)$. Since the pixels have been extended, index values below $k$ and above $m$ can be used. The LeGall-Tabatabai 5/3 (\Define{LGT 5/3}) transform computes wavelet coefficients $C(i)$ by executing the following two steps successively:
\begin{alignat*}{4}
    C(2i + 1) &= P(2i + 1) - \left\lfloor \frac{P(2i) + P(2i + 2)}{2} \right\rfloor &\quad& k - 1 \le 2i + 1 < m+ 1 &\quad& \text{step 1}\\
    C(2i) &= P(2i) + \left\lfloor \frac{ C(2i - 1) + C(2i + 1) + 2 }{4} \right\rfloor &\quad&  k \le 2i < m+ 1 &\quad& \text{step 2}.
\end{alignat*}
The Cohen–Daubechies–Feauveau 9/7 (\Define{CDF 9/7}) wavelet transform is computed by executing the following six steps successively:
\begin{alignat*}{4}
C(2i + 1) &= P(2i + 1) +  \alpha [P(2i) + P(2i + 2)], &\quad & k-3 \le 2i+1 < m+3 & \quad& \text{step 1}\\
C(2i) &= P(2i) + \beta[C(2i - 1) + C(2i + 1)], &\quad& k-2 \le 2i < m+2 &\quad& \text{step 2}\\
C(2i + 1) &= C(2i + 1) + \gamma[C(2i) + C(2i + 2)], &\quad& k-1 \le 2i+1 < m+1 &\quad& \text{step 3}\\
C(2i) &= C(2i) + \delta[C(2i - 1) + C(2i + 1)], &\quad& k \le 2i < m &\quad& \text{step 4}\\
C(2i + 1) &= -K C(2i + 1), &\quad& k \le 2i + 1 < m &\quad& \text{step 5}\\
C(2i) &=  \frac{1}{K} C(2i), &\quad& k \le 2i < m &\quad& \text{step 6}
\end{alignat*}
where the five constants (wavelet filter coefficients) used by JPEG 2000 are given by
$\alpha = -1.586134342$, $\beta = -0.052980118$, $\gamma = 0.882911075$, $\delta = 0.443506852$, and $K = 1.230174105$.



\subsection{JPEG 2000 encoding}

In this section, we give a brief overview of how JPEG 2000 encodes an image. We shall skip many details.

\paragraph{Basic idea}
The basic idea is similar to transform coding: transform, quantise, and encode. In JPEG, this idea was applied to every 8x8 pixel block. On the other hand, JPEG 2000 uses a sophisticated means of partitioning the image, using tiles, precincts and code-blocks. Also, the binary encoding in JPEG 2000 is much more sophisticated than the Huffman coding used in JPEG.

\paragraph{Wavelet transform}
The one-dimensional wavelet transforms described above are applied $L$ times, where $L$ is a parameter (either user-controlled or set by the encoder), and are interleaved on rows and columns to form $L$ resolutions of subbands. Resolution $L - 1$ is the original image, and resolution $0$ is the lowest-frequency subband.

\paragraph{Quantization}
Each subband can have a different quantization step size. Each
wavelet coefficient in the subband is divided by the quantization step size and the result is truncated. The quantization step size may be determined iteratively in order to achieve a target bitrate (i.e., the compression factor may be specified in advance by the user) or in order to achieve a predetermined level of image quality.

\paragraph{Precincts and Code-Blocks}

Consider a tile in a colour component. The original pixels are wavelet transformed, resulting in subbands of $L$ resolution levels. A grid of rectangles known as \Define{precincts} is now imposed on the entire image. The origin of the precinct grid is anchored at the top-left corner of the image and the dimensions of a precinct (its width and height) are powers of $2$. Notice that subband boundaries are generally not identical to precinct boundaries.

\begin{center}
    \includegraphics[width=10cm]{DataCompression/precincts.png}
\end{center}

We now examine the three subbands of a certain resolution and pick three precincts located in the same regions in the three subbands (the three gray rectangles in the figure above). These three precincts constitute a \Define{precinct partition}. The grid of precincts is now divided into a finer grid of \Define{code-blocks}, which are the basic units to be arithmetically coded.

In our example, each precinct is subdivided into 15 code-blocks; therefore, a precinct partition consists of 45 code-blocks. Think of the tiles, precincts, and code-blocks as coarse, medium, and fine partitions of the image, respectively. Partitioning the image into smaller and smaller units helps in creating memory-efficient implementations, streaming, and allowing easy access to many points in the bitstream.  However, most implementations may ignore this partitioning and just use one tile, one precinct and one code-block for the image.

\paragraph{Entropy coding}
The wavelet coefficients of a given code-block are then encoded. The method used by JPEG 2000 is called Embedded Block Coding with Optimized Truncation (\Important{EBCOT}). It is a context-based compression algorithm, based on the arithmetic encoder MQ (a variant of QM). We shall skip this part entirely.



\paragraph{Packets and layers}

After all the bits of all the coefficients of all the code-blocks of a precinct
partition have been encoded into a short bitstream, a header is added to that bitstream, thereby turning it into a \Important{packet}. A packet can be considered a
quality increment for one level of resolution at a certain spatial location.

A \Important{layer} is a set of packets, one from each precinct partition of each resolution level. Thus, a layer is a quality increment for the entire
image at full resolution.

\paragraph{Progressive Transmission}
This is an important feature of JPEG 2000. The
standard provides four ways of progressively transmitting and decoding an image: by
resolution, quality, spatial location, and component. Progression is achieved simply
by storing the packets in a specific order in the bitstream.

For example, quality  progression can be achieved by arranging the packets in layers, within each layer by component, within each component by resolution level, and within each resolution level by precinct partition. Resolution progression is achieved when the packets are arranged by precinct partition (innermost nesting), layer, image component, and resolution level (outermost nesting).

When an image is encoded, the packets are placed in the bitstream in a certain order, corresponding to a certain progression. If a user or an application require a different progression (and thus a different order of the packets), it should be easy to read the bitstream, identify the packets, and rearrange them. This process is known as \Important{parsing}, and is an easy task because of the many markers embedded in the bitstream. Thus, the bitstream can be parsed without having to decode
any of it.




\subsection{See further}

\paragraph{DjVu}
\Important{DjVu} is a wavelet-based compression algorithm and file format used for digitized books. Using wavelets makes sense for such pictures, Indeed, JPEG is designed for continuous-tone images (little variations); other techniques were designed for discrete-tone images (e.g. FABD); but digitized books have both features.

\paragraph{Fingerprint images}
The Wavelet Scalar Quantization algorithm (\Define{WSQ}) is a compression algorithm used for gray-scale fingerprint images. It has become a standard for the exchange and storage of fingerprint images. This method is preferred over JPEG because at the same compression ratios WSQ doesn't present the ``blocking artifacts'' and loss of fine-scale features that are not acceptable for identification in financial environments and criminal justice.

\subsection{Exercises}

\begin{exercise}
Write a program that visualises the Haar, LGT 5/3 and CDF 9/7 wavelet pyramid image decomposition.
\end{exercise}



























































\subsection{Exercises}

\begin{exercise} \label{exercise:arithmetic}
With the same source as in Example \ref{example:arithmetic}, encode the message $m = a_2a_3a_1a_1a_3$.
\end{exercise}



\begin{exercise} \label{exercise:swiss_miss}
Work out a decoder for the four-digit implementation of arithmetic coding, and decode the output of the SWISS MISS example.
\end{exercise}






% \subsection{Answers}








\section{Wavelet coding II: JPEG 2000}
\label{sec:09}


\subsection{Objectives of JPEG 2000}

\begin{itemize}
    \item High compression efficiency. Bitrates of less than 0.25 bpp are expected for highly detailed grayscale images.

\item The ability to handle large images, up to $2^{32} \times 2^{32}$ pixels (the original JPEG can handle images of up to $2^{16} \times 2^{16}$).

\item Progressive image transmission. The proposed standard can decompress an image progressively by SNR, resolution, color component, or region of interest.

\item Easy, fast access to various points in the compressed stream.
The decoder can pan/zoom the image while decompressing only parts of it. The decoder can rotate and crop the image while decompressing it.

\item Error resilience. Error-correcting codes can be included in the compressed stream, to improve transmission reliability in noisy environments.
\end{itemize}


\begin{center}
    \includegraphics[width=10cm]{DataCompression/Jpeg2000.png}
\end{center}



\subsection{Operations before wavelet transform coding}


A colour image consists of three colour components (typically RGB). First of all, we perform a DC level shifting on each component. If the pixel values range in the interval $[0, 2^k - 1]$, then we subtract $2^k - 1$ to each pixel value. Then we transform the components by means of either a reversible component transform (\Important{RCT}) or an irreversible component transform (\Important{ICT}). Both are very similar to the YCbCr conversion used in JPEG. The RCT is used for \Important{lossless compression}; the ICT is used for \Important{lossy compression}. Each transformed component is then compressed separately.

More precisely, let $I_0, I_1, I_2$ denote the three colour components of a certain pixel (after DC shifting). Then the RCT is the triple $(Y_0, Y_1, Y_2)$ given by
\begin{align*}
    Y_0 &= \left\lfloor \frac{I_0 + 2 I_1 + I_2}{4} \right\rfloor,\\
    Y_1 &= I_2 - I_1,\\
    Y_2 &= I_0 - I_1.
\end{align*}
The ICT is given by
\begin{align*}
    Y_0 &= 0.299 I_0 + 0.587 I_1 + 0.144 I_2,\\
    Y_1 &= -0.16875 I_0 - 0.33126 I_1 + 0.5 I_2,\\
    Y_2 &= 0.5 I_0 - 0.41869 I_1 - 0.08131 I_2.
\end{align*}

\paragraph{Tiling}
Each transformed colour component of the image is partitioned into rectangular, nonoverlapping \Important{tiles}. Since the colour components may have different resolutions, they may use different tile sizes. Tiles may have any size, up to the size of the entire image (i.e. one tile). All the tiles of a given colour component have the same size, except those at the edges. Each tile is compressed individually.




\subsection{The wavelets used}

JPEG 200 used two different wavelets, one for lossy compression and the other for lossless compression. The details are given below, but of course you needn't remember them.

We now denote a row of extended pixels in a tile by $P(k), P(k+1), \dots , P(m)$. Since the pixels have been extended, index values below $k$ and above $m$ can be used. The LeGall-Tabatabai 5/3 (\Define{LGT 5/3}) transform computes wavelet coefficients $C(i)$ by executing the following two steps successively:
\begin{alignat*}{4}
    C(2i + 1) &= P(2i + 1) - \left\lfloor \frac{P(2i) + P(2i + 2)}{2} \right\rfloor &\quad& k - 1 \le 2i + 1 < m+ 1 &\quad& \text{step 1}\\
    C(2i) &= P(2i) + \left\lfloor \frac{ C(2i - 1) + C(2i + 1) + 2 }{4} \right\rfloor &\quad&  k \le 2i < m+ 1 &\quad& \text{step 2}.
\end{alignat*}
The Cohen–Daubechies–Feauveau 9/7 (\Define{CDF 9/7}) wavelet transform is computed by executing the following six steps successively:
\begin{alignat*}{4}
C(2i + 1) &= P(2i + 1) +  \alpha [P(2i) + P(2i + 2)], &\quad & k-3 \le 2i+1 < m+3 & \quad& \text{step 1}\\
C(2i) &= P(2i) + \beta[C(2i - 1) + C(2i + 1)], &\quad& k-2 \le 2i < m+2 &\quad& \text{step 2}\\
C(2i + 1) &= C(2i + 1) + \gamma[C(2i) + C(2i + 2)], &\quad& k-1 \le 2i+1 < m+1 &\quad& \text{step 3}\\
C(2i) &= C(2i) + \delta[C(2i - 1) + C(2i + 1)], &\quad& k \le 2i < m &\quad& \text{step 4}\\
C(2i + 1) &= -K C(2i + 1), &\quad& k \le 2i + 1 < m &\quad& \text{step 5}\\
C(2i) &=  \frac{1}{K} C(2i), &\quad& k \le 2i < m &\quad& \text{step 6}
\end{alignat*}
where the five constants (wavelet filter coefficients) used by JPEG 2000 are given by
$\alpha = -1.586134342$, $\beta = -0.052980118$, $\gamma = 0.882911075$, $\delta = 0.443506852$, and $K = 1.230174105$.



\subsection{JPEG 2000 encoding}

In this section, we give a brief overview of how JPEG 2000 encodes an image. We shall skip many details.

\paragraph{Basic idea}
The basic idea is similar to transform coding: transform, quantise, and encode. In JPEG, this idea was applied to every 8x8 pixel block. On the other hand, JPEG 2000 uses a sophisticated means of partitioning the image, using tiles, precincts and code-blocks. Also, the binary encoding in JPEG 2000 is much more sophisticated than the Huffman coding used in JPEG.

\paragraph{Wavelet transform}
The one-dimensional wavelet transforms described above are applied $L$ times, where $L$ is a parameter (either user-controlled or set by the encoder), and are interleaved on rows and columns to form $L$ resolutions of subbands. Resolution $L - 1$ is the original image, and resolution $0$ is the lowest-frequency subband.

\paragraph{Quantization}
Each subband can have a different quantization step size. Each
wavelet coefficient in the subband is divided by the quantization step size and the result is truncated. The quantization step size may be determined iteratively in order to achieve a target bitrate (i.e., the compression factor may be specified in advance by the user) or in order to achieve a predetermined level of image quality.

\paragraph{Precincts and Code-Blocks}

Consider a tile in a colour component. The original pixels are wavelet transformed, resulting in subbands of $L$ resolution levels. A grid of rectangles known as \Define{precincts} is now imposed on the entire image. The origin of the precinct grid is anchored at the top-left corner of the image and the dimensions of a precinct (its width and height) are powers of $2$. Notice that subband boundaries are generally not identical to precinct boundaries.

\begin{center}
    \includegraphics[width=10cm]{DataCompression/precincts.png}
\end{center}

We now examine the three subbands of a certain resolution and pick three precincts located in the same regions in the three subbands (the three gray rectangles in the figure above). These three precincts constitute a \Define{precinct partition}. The grid of precincts is now divided into a finer grid of \Define{code-blocks}, which are the basic units to be arithmetically coded.

In our example, each precinct is subdivided into 15 code-blocks; therefore, a precinct partition consists of 45 code-blocks. Think of the tiles, precincts, and code-blocks as coarse, medium, and fine partitions of the image, respectively. Partitioning the image into smaller and smaller units helps in creating memory-efficient implementations, streaming, and allowing easy access to many points in the bitstream.  However, most implementations may ignore this partitioning and just use one tile, one precinct and one code-block for the image.

\paragraph{Entropy coding}
The wavelet coefficients of a given code-block are then encoded. The method used by JPEG 2000 is called Embedded Block Coding with Optimized Truncation (\Important{EBCOT}). It is a context-based compression algorithm, based on the arithmetic encoder MQ (a variant of QM). We shall skip this part entirely.



\paragraph{Packets and layers}

After all the bits of all the coefficients of all the code-blocks of a precinct
partition have been encoded into a short bitstream, a header is added to that bitstream, thereby turning it into a \Important{packet}. A packet can be considered a
quality increment for one level of resolution at a certain spatial location.

A \Important{layer} is a set of packets, one from each precinct partition of each resolution level. Thus, a layer is a quality increment for the entire
image at full resolution.

\paragraph{Progressive Transmission}
This is an important feature of JPEG 2000. The
standard provides four ways of progressively transmitting and decoding an image: by
resolution, quality, spatial location, and component. Progression is achieved simply
by storing the packets in a specific order in the bitstream.

For example, quality  progression can be achieved by arranging the packets in layers, within each layer by component, within each component by resolution level, and within each resolution level by precinct partition. Resolution progression is achieved when the packets are arranged by precinct partition (innermost nesting), layer, image component, and resolution level (outermost nesting).

When an image is encoded, the packets are placed in the bitstream in a certain order, corresponding to a certain progression. If a user or an application require a different progression (and thus a different order of the packets), it should be easy to read the bitstream, identify the packets, and rearrange them. This process is known as \Important{parsing}, and is an easy task because of the many markers embedded in the bitstream. Thus, the bitstream can be parsed without having to decode
any of it.




\subsection{See further}

\paragraph{DjVu}
\Important{DjVu} is a wavelet-based compression algorithm and file format used for digitized books. Using wavelets makes sense for such pictures, Indeed, JPEG is designed for continuous-tone images (little variations); other techniques were designed for discrete-tone images (e.g. FABD); but digitized books have both features.

\paragraph{Fingerprint images}
The Wavelet Scalar Quantization algorithm (\Define{WSQ}) is a compression algorithm used for gray-scale fingerprint images. It has become a standard for the exchange and storage of fingerprint images. This method is preferred over JPEG because at the same compression ratios WSQ doesn't present the ``blocking artifacts'' and loss of fine-scale features that are not acceptable for identification in financial environments and criminal justice.

\subsection{Exercises}

\begin{exercise}
Write a program that visualises the Haar, LGT 5/3 and CDF 9/7 wavelet pyramid image decomposition.
\end{exercise}




\section{Wavelet coding I: Mathematical background}
\label{sec:08}


\subsection{The Continuous Wavelet Transform}

\paragraph{Mathematical definitions}
The continuous wavelet transform (\Define{CWT}) of a function $f(t)$ involves a \Important{mother wavelet} $\psi(t)$. The mother wavelet is scaled by a factor $a$ and translated by $b$ as such:
\[
    \psi_{a,b}(t) = \frac{1}{\sqrt{|a|}} \psi \left( \frac{t-b}{a} \right).
\]
We view the $\psi_{a,b}$ functions as a basis, and we naturally compute the inner product
\[
    W(a,b) := \langle f(t), \psi_{a,b}(t)  \rangle = \int_{-\infty}^\infty f(t) \psi^*_{a,b}(t) \ dt.
\]
The mother wavelet needs to satisfy three properties.
\begin{enumerate}
    \item It has zero average:
    \[
        \int_{-\infty}^\infty \psi(t) \ dt = 0.
    \]

    \item It has finite energy:
    \[
        \int_{-\infty}^\infty |\psi(t)|^2 \ dt < \infty.
    \]

    \item The admissibility condition. Let
    \begin{align*}
        \Psi(\omega) &:= \int_{-\infty}^\infty \psi(t) \e^{-\i \omega t} \ dt,\\
        C &:= \int_{-\infty}^\infty \frac{ |\Psi(\omega)^2| }{ |\omega| } \ d\omega.
    \end{align*}
    Then the admissibility condition requires that $0 < C < \infty$. This technical condition ensures that the inverse CWT exists. You needn't remember the details.
\end{enumerate}

\paragraph{Intuition}
The CWT provides a \Important{time-frequency representation} of a signal. Let us consider the signal $f(t) = \sin (t)$ as a basic example.

Firstly, note that due to the finite energy condition, the mother wavelet amplitude decreases rapidly when $t$ tends to plus or minus infinity. A good example is the \Define{Mexican hat} wavelet:

\begin{center}
\includegraphics[width=10cm]{DataCompression/mexican.png}
\end{center}

The Mexican hat wavelet is similar to having one oscillation.


For a fixed $a$, the set $\{ \psi_{a,b} : b \in \R \}$ is a sequence of the same function translated over time. If the Mexican hat is placed in phase with the sinusoid, then the inner product will be a large positive number; if it is in opposite phase it will be a large negative number. So it will create oscillations.

On the other hand, for a fixed $b$, the set $\{ \psi_{a,b} : a \in \R \}$ is a sequence of the same function stretched and squeezed. For the Mexican hat, it means changing the frequency of its one oscillation. If the frequency matches that of the sinusoid, it will oscillate a lot. If the frequencies do not match, the oscillations will have a much lower amplitude.

This yields the following time-frequency diagram.

\begin{center}
\includegraphics[width=10cm]{DataCompression/time-frequency.png}
\end{center}

\subsection{The Haar transform}

Once again, we will be dealing with two-dimensional, discrete time signals. Let us consider the simplest wavelet in discrete form: the \Important{Haar transform}. It is based on the \Define{Haar wavelet}
\[
    \psi(t) = \begin{cases}
    1 & \text{if } 0 \le t < 1/2,\\
    -1 & \text{if } 1/2 \le t < 1,\\
    0 & \text{otherwise.}
    \end{cases}
\]
It also needs the \Important{scaling function}
\[
    \phi(t) = \begin{cases}
    1 & \text{if } 0 \le t < 1,\\
    0 & \text{otherwise}
    \end{cases}
\]
to take the DC term into account.

\begin{center}
\includegraphics[width=10cm]{DataCompression/haar_wavelet.png}
\end{center}


Leaving out some details, here's how we can discretise this. Let $N = 2^n$, then the Haar matrix ${\bf H}_N$ is defined recursively as follows. Firstly,
\[
    {\bf H}_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
    1 & 1\\
    1 & -1
    \end{pmatrix}.
\]
Then
\[
    {\bf H}_{2N} = \frac{1}{\sqrt{2}} \begin{pmatrix}
    {\bf H}_N  \otimes (1,1)\\
    {\bf I}_N \otimes (1,-1)
    \end{pmatrix},
\]
where ${\bf I}_N$ is the identity matrix and $\otimes$ denotes the Kronecker product.

Note: the Kronecker product of two matrices ${\bf A}$ and ${\bf B}$ is (informally) defined as follows. Say ${\bf A}$ is $m \times n$ and ${\bf B}$ is $r \times s$, then ${\bf K} = {\bf A} \otimes {\bf B}$ is an $mr \times ns$ matrix, where every entry $a_{i,j}$ of ${\bf A}$ has been replaced by the whole matrix $a_{i,j}{\bf B}$.

For instance, we have
\begin{align*}
    {\bf H}_4 &= \frac{1}{\sqrt{2}} \begin{pmatrix}
     \begin{pmatrix}
    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
    \end{pmatrix} \otimes (1,1)\\
    \begin{pmatrix}
    1 & 0\\
    0 & 1
    \end{pmatrix} \otimes (1, -1)
    \end{pmatrix}\\
    &= \frac{1}{\sqrt{2}} \begin{pmatrix}
    \frac{1}{\sqrt{2}} \cdot (1,1) & \frac{1}{\sqrt{2}} \cdot (1,1)\\
    \frac{1}{\sqrt{2}} \cdot (1,1) & -\frac{1}{\sqrt{2}} \cdot (1,1)\\
    1 \cdot (1, -1) & 0 \cdot (1, -1)\\
    0 \cdot (1, -1) & 1 \cdot (1, -1)
    \end{pmatrix}\\
    &= \frac{1}{ \sqrt{2} } \begin{pmatrix}
    \frac{1}{\sqrt{2}}           & \frac{1}{\sqrt{2}}         & \frac{1}{\sqrt{2}}         & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}           & \frac{1}{\sqrt{2}}         & -\frac{1}{\sqrt{2}}        & -\frac{1}{\sqrt{2}}\\
    1    & -1 & 0         & 0\\
    0           & 0         & 1  & -1
    \end{pmatrix}.
\end{align*}


We then have
\begin{align*}
    {\bf H}_4 &= \frac{1}{ \sqrt{4} } \begin{pmatrix}
    1           & 1         & 1         & 1\\
    1           & 1         & -1        & -1\\
    \sqrt{2}    & -\sqrt{2} & 0         & 0\\
    0           & 0         & \sqrt{2}  & -\sqrt{2}
    \end{pmatrix}\\
    %
    {\bf H}_8 &= \frac{1}{ \sqrt{8} } \begin{pmatrix}
    1           & 1         & 1         & 1         & 1         & 1         & 1         & 1\\
    1           & 1         & 1         & 1         & -1        & -1        & -1        & -1\\
    \sqrt{2}    & \sqrt{2}  & -\sqrt{2} & -\sqrt{2} & 0         & 0         & 0         & 0\\
    0           & 0         & 0         & 0         & \sqrt{2}  & \sqrt{2}  & -\sqrt{2} & -\sqrt{2}\\
    2           & -2        & 0         & 0         & 0         & 0         & 0         &  0\\
    0           & 0         & 2         & -2        & 0         & 0         & 0         &  0\\
    0           & 0         & 0         & 0         & 2         & -2        & 0         &  0\\
    0           & 0         & 0         & 0         & 0         & 0         & 2         &  -2
    \end{pmatrix}.
\end{align*}
(Yes, it is the same as for the DWHT for $N = 2$. But it's different for $N = 4$ and after that!)

The Haar matrix can be decomposed into ``computing averages and differences'' as follows. For all $N$ powers of $2$, let
\[
    \Delta_N = \frac{1}{\sqrt{N}} \begin{pmatrix}
    1 & 1 & 0 & 0 & \dots & 0 & 0\\
    0 & 0 & 1 & 1 & \dots & 0 & 0\\
    \vdots & \ddots & \dots & \dots & \dots & 1 & 1\\
    1 & -1 & 0 & 0 & \dots & 0 & 0\\
    0 & 0 & 1 & -1 & \dots & 0 & 0\\
    \vdots & \ddots & \dots & \dots & \dots & 1 & -1
    \end{pmatrix},
\]
so that
\begin{align*}
    \Delta_2 &= \frac{1}{\sqrt{2}} \end{pmatrix}.
\end{align*}

We can then decompose, e.g. ${\bf H}_8$ as follows:
\begin{align*}
    {\bf H}_8 &= \left(\begin{array}{c|c}
    \Delta_2 & {\bf 0}\\
    \hline
    {\bf 0} & {\bf I}_6
    \end{array} \right)
    \left(\begin{array}{c|c}
    \Delta_4 & {\bf 0}\\
    \hline
    {\bf 0} & {\bf I}_4
    \end{array} \right)
    \left(\begin{array}{c|c}
    \multicolumn{2}{c}{\Delta_8}
    \end{array} \right)\\
    %
    {\bf H}_8 &=
        \left( \begin{array}{cc|}

    \end{array} \right),
\end{align*}
where $c = \frac{1}{\sqrt{2}}$, $b = \frac{1}{\sqrt{4}}$, and $a = \frac{1}{\sqrt{8}}$.

This product should be read from right to left: ${\bf H}_8 = {\bf C}{\bf B}{\bf A}$. First, ${\bf A}$ computes the four averages of adjacent of points, and keeps their differences in order to remain reversible. Second, ${\bf B}$ does the same as ${\bf A}$, but only for the four averages (and leaves the differences untouched), thus creating two more differences. Finally, ${\bf C}$ computes the average of the remaining two averages, to get one final average and seven other differences.

We associate with each iteration a quantity called \Define{resolution}, which is defined as the number of remaining averages at the end of the iteration. The resolutions after each of the three iterations above are $4(= 2^2)$, $2(= 2^1)$, and $1(= 2^0)$.

We can think of the averages as a coarse resolution representation of
the original image, and of the details as the data needed to reconstruct the original image from this coarse resolution. If the pixels of the image are correlated, the coarse representation will resemble the original pixels, while the details will be small.

Here are the basis matrices of the Haar transform for $N = 8$.

\begin{center}
\includegraphics[width=10cm]{DataCompression/haar.png}
\end{center}

\paragraph{The Haar transform in 2D}
The simplest way of applying the Haar transform in 2D is the standard image wavelet transform, where we simply apply the 1D-transform row-wise and then column-wise. That is, we do
\[
    \Theta = {\bf H} ({\bf X}{\bf H}^\top).
\]

Another, much more common, technique is the \Important{pyramid} image wavelet transform. The idea is to decompose the Haar matrix as a chain of ``averages and differences'' computations and alternate row and column operations.

\begin{center}
\includegraphics[width=10cm]{DataCompression/pyramid.png}
\end{center}


Typically, the averages (that end up in the top left hand region) have large values, while the differences (in the three other regions) tend to have small values. Those regions are called \Define{subbands}. Subbands actually reflect different geometrical artifacts of the image:
\begin{itemize}
    \item  the upper-right subband (usually referred to as LH) corresponds to vertical artifacts;

    \item the lower-left subband (usually referred to as HL) corresponds to horizontal artifacts;

    \item the lower-right subband (usually referred to as HH) corresponds to diagonal artifacts.
\end{itemize}

Below is a typical image decomposition using the pyramid decomposition.

\begin{center}
    \includegraphics[width=10cm]{DataCompression/Lenna_Haar.png}
\end{center}

%\subsection{Image decompositions}

\subsection{See further}

\paragraph{Filter banks}
The most common way of implementing the Discrete Wavelet Transform (not just for Haar, but for any wavelet) is via the use of \Important{filter banks}. Here's a rapid explanation of the intuition. We can view ${\bf H}_2$ as using two filters:
\[
    {\bf H}_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
    1 & 1\\
    1 & -1
    \end{pmatrix},
\]
the first row corresponds to a lowpass filter, that only takes the DC term (low frequency); the second row corresponds to a highpass filter, that only takes the high frequency term. In general, the ``averages and differences'' computation can be viewed as applying two simple filters on different parts of the data; those filters are then placed in series to compute the whole transform.

\paragraph{Various image decompositions}
Even though the pyramid image decomposition is by far the most common, many different image decompositions have been proposed, e.g.  pyramid, line, , () wavelet packet transform, full wavelet decomposition, etc.



\paragraph{Other wavelets}
We will see two other wavelets in the next lecture. But many other wavelets have been proposed: Meyer, Morlet, Shannon, the large family of  wavelets, that of , etc. Their definition can range from easy (e.g. Morlet) to highly involved (Daubechies).



\subsection{Exercises}

\begin{exercise}
What is the number of nonzero entries in ${\bf H}_N$?
\end{exercise}

\begin{exercise}
What is the inverse of ${\bf H}_N$? Is that matrix orthogonal?
\end{exercise}

\begin{exercise}
Compute the two-dimensional Haar transform of the ${\bf X}$ data from Lecture \ref{sec:07} using the pyramid decomposition.
\end{exercise}




\section{Wavelet coding I: Mathematical background}
\label{sec:08}


\subsection{The Continuous Wavelet Transform}

\paragraph{Mathematical definitions}
The continuous wavelet transform (\Define{CWT}) of a function $f(t)$ involves a \Important{mother wavelet} $\psi(t)$. The mother wavelet is scaled by a factor $a$ and translated by $b$ as such:
\[
    \psi_{a,b}(t) = \frac{1}{\sqrt{|a|}} \psi \left( \frac{t-b}{a} \right).
\]
We view the $\psi_{a,b}$ functions as a basis, and we naturally compute the inner product
\[
    W(a,b) := \langle f(t), \psi_{a,b}(t)  \rangle = \int_{-\infty}^\infty f(t) \psi^*_{a,b}(t) \ dt.
\]
The mother wavelet needs to satisfy three properties.
\begin{enumerate}
    \item It has zero average:
    \[
        \int_{-\infty}^\infty \psi(t) \ dt = 0.
    \]

    \item It has finite energy:
    \[
        \int_{-\infty}^\infty |\psi(t)|^2 \ dt < \infty.
    \]

    \item The admissibility condition. Let
    \begin{align*}
        \Psi(\omega) &:= \int_{-\infty}^\infty \psi(t) \e^{-\i \omega t} \ dt,\\
        C &:= \int_{-\infty}^\infty \frac{ |\Psi(\omega)^2| }{ |\omega| } \ d\omega.
    \end{align*}
    Then the admissibility condition requires that $0 < C < \infty$. This technical condition ensures that the inverse CWT exists. You needn't remember the details.
\end{enumerate}

\paragraph{Intuition}
The CWT provides a \Important{time-frequency representation} of a signal. Let us consider the signal $f(t) = \sin (t)$ as a basic example.

Firstly, note that due to the finite energy condition, the mother wavelet amplitude decreases rapidly when $t$ tends to plus or minus infinity. A good example is the \Define{Mexican hat} wavelet:

\begin{center}
\includegraphics[width=10cm]{DataCompression/mexican.png}
\end{center}

The Mexican hat wavelet is similar to having one oscillation.


For a fixed $a$, the set $\{ \psi_{a,b} : b \in \R \}$ is a sequence of the same function translated over time. If the Mexican hat is placed in phase with the sinusoid, then the inner product will be a large positive number; if it is in opposite phase it will be a large negative number. So it will create oscillations.

On the other hand, for a fixed $b$, the set $\{ \psi_{a,b} : a \in \R \}$ is a sequence of the same function stretched and squeezed. For the Mexican hat, it means changing the frequency of its one oscillation. If the frequency matches that of the sinusoid, it will oscillate a lot. If the frequencies do not match, the oscillations will have a much lower amplitude.

This yields the following time-frequency diagram.

\begin{center}
\includegraphics[width=10cm]{DataCompression/time-frequency.png}
\end{center}

\subsection{The Haar transform}

Once again, we will be dealing with two-dimensional, discrete time signals. Let us consider the simplest wavelet in discrete form: the \Important{Haar transform}. It is based on the \Define{Haar wavelet}
\[
    \psi(t) = \begin{cases}
    1 & \text{if } 0 \le t < 1/2,\\
    -1 & \text{if } 1/2 \le t < 1,\\
    0 & \text{otherwise.}
    \end{cases}
\]
It also needs the \Important{scaling function}
\[
    \phi(t) = \begin{cases}
    1 & \text{if } 0 \le t < 1,\\
    0 & \text{otherwise}
    \end{cases}
\]
to take the DC term into account.

\begin{center}
\includegraphics[width=10cm]{DataCompression/haar_wavelet.png}
\end{center}


Leaving out some details, here's how we can discretise this. Let $N = 2^n$, then the Haar matrix ${\bf H}_N$ is defined recursively as follows. Firstly,
\[
    {\bf H}_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
    1 & 1\\
    1 & -1
    \end{pmatrix}.
\]
Then
\[
    {\bf H}_{2N} = \frac{1}{\sqrt{2}} \begin{pmatrix}
    {\bf H}_N  \otimes (1,1)\\
    {\bf I}_N \otimes (1,-1)
    \end{pmatrix},
\]
where ${\bf I}_N$ is the identity matrix and $\otimes$ denotes the Kronecker product.

Note: the Kronecker product of two matrices ${\bf A}$ and ${\bf B}$ is (informally) defined as follows. Say ${\bf A}$ is $m \times n$ and ${\bf B}$ is $r \times s$, then ${\bf K} = {\bf A} \otimes {\bf B}$ is an $mr \times ns$ matrix, where every entry $a_{i,j}$ of ${\bf A}$ has been replaced by the whole matrix $a_{i,j}{\bf B}$.

For instance, we have
\begin{align*}
    {\bf H}_4 &= \frac{1}{\sqrt{2}} \begin{pmatrix}
     \begin{pmatrix}
    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
    \end{pmatrix} \otimes (1,1)\\
    \begin{pmatrix}
    1 & 0\\
    0 & 1
    \end{pmatrix} \otimes (1, -1)
    \end{pmatrix}\\
    &= \frac{1}{\sqrt{2}} \begin{pmatrix}
    \frac{1}{\sqrt{2}} \cdot (1,1) & \frac{1}{\sqrt{2}} \cdot (1,1)\\
    \frac{1}{\sqrt{2}} \cdot (1,1) & -\frac{1}{\sqrt{2}} \cdot (1,1)\\
    1 \cdot (1, -1) & 0 \cdot (1, -1)\\
    0 \cdot (1, -1) & 1 \cdot (1, -1)
    \end{pmatrix}\\
    &= \frac{1}{ \sqrt{2} } \begin{pmatrix}
    \frac{1}{\sqrt{2}}           & \frac{1}{\sqrt{2}}         & \frac{1}{\sqrt{2}}         & \frac{1}{\sqrt{2}}\\
    \frac{1}{\sqrt{2}}           & \frac{1}{\sqrt{2}}         & -\frac{1}{\sqrt{2}}        & -\frac{1}{\sqrt{2}}\\
    1    & -1 & 0         & 0\\
    0           & 0         & 1  & -1
    \end{pmatrix}.
\end{align*}


We then have
\begin{align*}
    {\bf H}_4 &= \frac{1}{ \sqrt{4} } \begin{pmatrix}
    1           & 1         & 1         & 1\\
    1           & 1         & -1        & -1\\
    \sqrt{2}    & -\sqrt{2} & 0         & 0\\
    0           & 0         & \sqrt{2}  & -\sqrt{2}
    \end{pmatrix}\\
    %
    {\bf H}_8 &= \frac{1}{ \sqrt{8} } \begin{pmatrix}
    1           & 1         & 1         & 1         & 1         & 1         & 1         & 1\\
    1           & 1         & 1         & 1         & -1        & -1        & -1        & -1\\
    \sqrt{2}    & \sqrt{2}  & -\sqrt{2} & -\sqrt{2} & 0         & 0         & 0         & 0\\
    0           & 0         & 0         & 0         & \sqrt{2}  & \sqrt{2}  & -\sqrt{2} & -\sqrt{2}\\
    2           & -2        & 0         & 0         & 0         & 0         & 0         &  0\\
    0           & 0         & 2         & -2        & 0         & 0         & 0         &  0\\
    0           & 0         & 0         & 0         & 2         & -2        & 0         &  0\\
    0           & 0         & 0         & 0         & 0         & 0         & 2         &  -2
    \end{pmatrix}.
\end{align*}
(Yes, it is the same as for the DWHT for $N = 2$. But it's different for $N = 4$ and after that!)

The Haar matrix can be decomposed into ``computing averages and differences'' as follows. For all $N$ powers of $2$, let
\[
    \Delta_N = \frac{1}{\sqrt{N}} \begin{pmatrix}
    1 & 1 & 0 & 0 & \dots & 0 & 0\\
    0 & 0 & 1 & 1 & \dots & 0 & 0\\
    \vdots & \ddots & \dots & \dots & \dots & 1 & 1\\
    1 & -1 & 0 & 0 & \dots & 0 & 0\\
    0 & 0 & 1 & -1 & \dots & 0 & 0\\
    \vdots & \ddots & \dots & \dots & \dots & 1 & -1
    \end{pmatrix},
\]
so that
\begin{align*}
    \Delta_2 &= \frac{1}{\sqrt{2}} \end{pmatrix}.
\end{align*}

We can then decompose, e.g. ${\bf H}_8$ as follows:
\begin{align*}
    {\bf H}_8 &= \left(\begin{array}{c|c}
    \Delta_2 & {\bf 0}\\
    \hline
    {\bf 0} & {\bf I}_6
    \end{array} \right)
    \left(\begin{array}{c|c}
    \Delta_4 & {\bf 0}\\
    \hline
    {\bf 0} & {\bf I}_4
    \end{array} \right)
    \left(\begin{array}{c|c}
    \multicolumn{2}{c}{\Delta_8}
    \end{array} \right)\\
    %
    {\bf H}_8 &=
        \left( \begin{array}{cc|}

    \end{array} \right),
\end{align*}
where $c = \frac{1}{\sqrt{2}}$, $b = \frac{1}{\sqrt{4}}$, and $a = \frac{1}{\sqrt{8}}$.

This product should be read from right to left: ${\bf H}_8 = {\bf C}{\bf B}{\bf A}$. First, ${\bf A}$ computes the four averages of adjacent of points, and keeps their differences in order to remain reversible. Second, ${\bf B}$ does the same as ${\bf A}$, but only for the four averages (and leaves the differences untouched), thus creating two more differences. Finally, ${\bf C}$ computes the average of the remaining two averages, to get one final average and seven other differences.

We associate with each iteration a quantity called \Define{resolution}, which is defined as the number of remaining averages at the end of the iteration. The resolutions after each of the three iterations above are $4(= 2^2)$, $2(= 2^1)$, and $1(= 2^0)$.

We can think of the averages as a coarse resolution representation of
the original image, and of the details as the data needed to reconstruct the original image from this coarse resolution. If the pixels of the image are correlated, the coarse representation will resemble the original pixels, while the details will be small.

Here are the basis matrices of the Haar transform for $N = 8$.

\begin{center}
\includegraphics[width=10cm]{DataCompression/haar.png}
\end{center}

\paragraph{The Haar transform in 2D}
The simplest way of applying the Haar transform in 2D is the standard image wavelet transform, where we simply apply the 1D-transform row-wise and then column-wise. That is, we do
\[
    \Theta = {\bf H} ({\bf X}{\bf H}^\top).
\]

Another, much more common, technique is the \Important{pyramid} image wavelet transform. The idea is to decompose the Haar matrix as a chain of ``averages and differences'' computations and alternate row and column operations.

\begin{center}
\includegraphics[width=10cm]{DataCompression/pyramid.png}
\end{center}


Typically, the averages (that end up in the top left hand region) have large values, while the differences (in the three other regions) tend to have small values. Those regions are called \Define{subbands}. Subbands actually reflect different geometrical artifacts of the image:
\begin{itemize}
    \item  the upper-right subband (usually referred to as LH) corresponds to vertical artifacts;

    \item the lower-left subband (usually referred to as HL) corresponds to horizontal artifacts;

    \item the lower-right subband (usually referred to as HH) corresponds to diagonal artifacts.
\end{itemize}

Below is a typical image decomposition using the pyramid decomposition.

\begin{center}
    \includegraphics[width=10cm]{DataCompression/Lenna_Haar.png}
\end{center}

%\subsection{Image decompositions}

\subsection{See further}

\paragraph{Filter banks}
The most common way of implementing the Discrete Wavelet Transform (not just for Haar, but for any wavelet) is via the use of \Important{filter banks}. Here's a rapid explanation of the intuition. We can view ${\bf H}_2$ as using two filters:
\[
    {\bf H}_2 = \frac{1}{\sqrt{2}} \begin{pmatrix}
    1 & 1\\
    1 & -1
    \end{pmatrix},
\]
the first row corresponds to a lowpass filter, that only takes the DC term (low frequency); the second row corresponds to a highpass filter, that only takes the high frequency term. In general, the ``averages and differences'' computation can be viewed as applying two simple filters on different parts of the data; those filters are then placed in series to compute the whole transform.

\paragraph{Various image decompositions}
Even though the pyramid image decomposition is by far the most common, many different image decompositions have been proposed, e.g.  pyramid, line, , () wavelet packet transform, full wavelet decomposition, etc.



\paragraph{Other wavelets}
We will see two other wavelets in the next lecture. But many other wavelets have been proposed: Meyer, Morlet, Shannon, the large family of  wavelets, that of , etc. Their definition can range from easy (e.g. Morlet) to highly involved (Daubechies).



\subsection{Exercises}

\begin{exercise}
What is the number of nonzero entries in ${\bf H}_N$?
\end{exercise}

\begin{exercise}
What is the inverse of ${\bf H}_N$? Is that matrix orthogonal?
\end{exercise}

\begin{exercise}
Compute the two-dimensional Haar transform of the ${\bf X}$ data from Lecture \ref{sec:07} using the pyramid decomposition.
\end{exercise}



\section{Transform coding II: JPEG}
\label{sec:07}


\subsection{Operations before transform coding}

\paragraph{Color space transformation}
First, the image should be converted from RGB into a different color space called \Important{$Y'C_BC_R$} (a.k.a. YCbCr). It has three components $Y'$, $C_B$ and $C_R$: the $Y'$ component represents the \Important{brightness} of a pixel, and the $C_B$ and $C_R$ components represent the \Important{chrominance} (split into blue and red components).

Mathematically, let $R, B, G$ be the red, green and blue signals, which all take a value between $0$ and $1$. Then
\begin{align*}
    Y' &= K_R R + K_G G + K_B B,\\
    C_B &= \frac {1}{2} \frac {B-Y'}{1-K_B},\\
    C_R &=\frac {1}{2} \frac {R-Y'}{1-K_R},
\end{align*}
where $K_R$, $K_G$, and $K_B$ are constants that satisfy $K_R + K_G +K_B = 1$. For instance,
\begin{align*}
    K_R &= 0.299,\\
    K_G &= 0.587,\\
    K_B &=0.114.
\end{align*}

The YCbCr colour space conversion allows greater compression without a significant effect on perceptual image quality (or greater perceptual image quality for the same compression). The compression is more efficient because the brightness information, which is more important to the eventual perceptual quality of the image, is confined to a single channel. This more closely corresponds to the perception of color in the human visual system.


\includegraphics[height=10cm]{DataCompression/YCbCr.jpg}


\paragraph{Downsampling}
Humans can see considerably more fine detail in the brightness of an image (the Y' component) than in the hue and color saturation of an image (the Cb and Cr components). As such, the next step is \Important{chroma downsampling}, which reduces the spatial resolution of the Cb and Cr components.



\paragraph{Block splitting}
JPEG splits an image into $8 \times 8$ blocks of pixels and applies transform coding to each block. We will focus on one channel and see the data as an $8 \times 8$ matrix of integers ${\bf X}$. We will use a running example with
\[
    {\bf X} = \begin{pmatrix}
    124 & 125 & 122 & 120 & 122 & 119 & 117 & 118\\
    121 & 121 & 120 & 119 & 119 & 120 & 120 & 118\\
    126 & 124 & 123 & 122 & 121 & 121 & 120 & 120\\
    124 & 124 & 125 & 125 & 126 & 125 & 124 & 124\\
    127 & 127 & 128 & 129 & 130 & 128 & 127 & 125\\
    143 & 142 & 143 & 142 & 140 & 139 & 139 & 139\\
    150 & 148 & 152 & 152 & 152 & 152 & 150 & 151\\
    156 & 159 & 158 & 155 & 158 & 158 & 157 & 156
    \end{pmatrix}.
\]


\subsection{Step 1: Transform}

\paragraph{DCT}
The discrete cosine transform (\Define{DCT}) is similar to the DFT, but only takes the cosines into consideration. More precisely, the transform matrix is ${\bf C}$, where
\[
    c_{i,j} = C_i \sqrt{ \frac{1}{N} } \cos \left( \frac{(2j+1)i \pi}{2 N} \right),
\]
where $C_0 = 1$ and $C_i = 2$ for all $i \in \{1, \dots, N-1\}$. The basis matrices for the DCT ($N=8$) are given below.

\begin{center}
\includegraphics[width=10cm]{DataCompression/DCT-8x8.png}
\end{center}

Technically, JPEG does not apply the DCT to ${\bf X}$ directly. Instead, it subtracts $128$ to each value (so that for instance, the top left value is now -4) before applying the DCT. Following our running example, we obtain
\[
    \Theta = \begin{pmatrix}
    39.88 & 6.56 & -2.24 & 1.22 & -0.37 & -1.08 & 0.79 & 1.13\\
    -102.43 & 4.56 & 2.26 & 1.12 & 0.35 & -0.63 & -1.05 & -0.48\\
    37.77 & 1.31 & 1.77 & 0.25 & -1.50 & -2.21 & -0.10 & 0.23\\
    -5.67 & 2.24 & -1.32 & -0.81 & 1.41 & 0.22 & -0.13 & 0.17\\
    -3.37 & -0.74 & -1.75 & 0.77 & -0.62 & -2.65 & -1.30 & 0.76\\
    5.98 & -0.13 & -0.45 & -0.77 & 1.99 & -0.26 & 1.46 & 0.00\\
    3.97 & 5.52 & 2.39 & -0.55 & -0.051 & -0.84 & -0.52 & -0.13\\
    -3.43 & 0.51 & -1.07 & 0.87 & 0.96 & 0.09 & 0.33 & 0.01
    \end{pmatrix}.
\]



\paragraph{DCT over DFT} Why use the DCT instead of the DFT? Let's use the one-dimensional case to explain this. The DFT has a major problem: it ``presumes'' that the signal $(f_0, \dots, f_{N-1})$ has period $N$, since it uses a basis a functions that are periodic of period $N$. But there could be a large \Important{discontinuity} in our signal: $f_0$ may differ from $f_{N-1}$. The DFT needs accounts for that discontinuity by adding a term of high frequency and in turn modifying all the lower frequency terms: that can ruin everything. On the other hand, the DCT ``presumes'' that the signal has period $2N$ by effectively working on a new signal $(f_0, \dots, f_{N-1}, f_{N-1}, f_{N-2}, \dots, f_0)$. That signal does not have such discontinuity anymore. See below for an example; note that there are four main kinds of DCT and that JPEG uses DCT-II.

\begin{center}
\includegraphics[width=10cm]{DataCompression/DCT.png}
\end{center}




\subsection{Step 2: Quantization}

\paragraph{Zigzag scan}

The basis matrices of the DCT represent different fluctuations of frequency increasing with both $i$ and $j$. In particular, the $\theta_{0,0}$ coefficient is referred to as the \Define{DC} coefficient as it corresponds to zero frequency and is proportional to the average value of $x_{i,j}$. The other coefficients are referred to as the \Define{AC} coefficients.  The coefficients are sorted according to a \Important{zigzag scan}, displayed below.

\begin{center}
\includegraphics[width=6cm]{DataCompression/JPEG_ZigZag.png}
\end{center}


\paragraph{Quantization}
The JPEG algorithm uses so-called ``uniform midtread quantization.'' The quantization steps are organized in a quantization table; an example is given in the following matrix.
\[
    {\bf Q} = \begin{pmatrix}
    16 & 11 & 10 & 16 & 24 & 40 & 51 & 61\\
    12 & 12 & 14 & 19 & 26 & 58 & 60 & 55\\
    14 & 13 & 16 & 24 & 40 & 57 & 69 & 56\\
    14 & 17 & 22 & 29 & 51 & 87 & 80 & 62\\
    18 & 22 & 37 & 56 & 68 & 109 & 103 & 77\\
    24 & 35 & 55 & 64 & 81 & 104 & 113 & 92\\
    49 & 64 & 78 & 87 & 103 & 121 & 120 & 101\\
    72 & 92 & 95 & 98 & 112 & 100 & 103 & 99
    \end{pmatrix}
\]
These values are actually determined by a quality coefficient specified by the user.

The quantized value of $\theta_{i,j}$ is
\[
    l_{i.j} = \left\lfloor \frac{ \theta_{i,j} }{ Q_{i,j} } \right\rceil,
\]
where $\lfloor x \rceil$ denotes the integer nearest to $x$: $\lfloor x \rceil = \lfloor x + 0.5 \rfloor$.


We can see that the step size increases as we move from the DC coefficient to higher frequency coefficients. Therefore, more quantization error will be introduced at higher-frequency levels. This is because quantization errors in the DC and lower AC coefficients are more easily detectable by the human visual system than quantization errors in the higher AC frequencies.

Following our example, we obtain the following quantized coefficients
\[
    {\bf L} = \begin{pmatrix}
    2 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
    -9 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    3 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{pmatrix}
\]

All coefficients with magnitude less than half the corresponding step size will be set to zero. Because the step sizes at the end of the zigzag scan are larger, we typically see a \Important{long run of zeroes} toward the end of the scan. The entire run of zeroes at the end of the scan can be code by an end of block (\Important{EOB}) code after the last nonzero label.


Reconstruction at the decoder's end is straightforward: the reconstructed value $\hat{\theta}_{i,j}$ is given by
\[
    \hat{\theta}_{i,j} = l_{i,j} Q_{i,j}.
\]
In our example, we obtain
\[
    \hat{\Theta} = \begin{pmatrix}
    32 & 11 & 0 & 0 & 0 & 0 & 0 & 0\\
    -108 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    42 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{pmatrix}.
\]
The reconstructed data is then
\[
    \hat{\bf X} = \begin{pmatrix}
    123 & 122 & 122 & 121 & 120 & 120 & 119 & 119\\
    121 & 121 & 121 & 120 & 119 & 118 & 118 & 118\\
    121 & 121 & 120 & 119 & 119 & 118 & 117 & 117\\
    124 & 124 & 123 & 122 & 122 & 121 & 120 & 120\\
    130 & 130 & 129 & 129 & 128 & 128 & 128 & 127\\
    141 & 141 & 140 & 140 & 139 & 138 & 138 & 137\\
    152 & 152 & 151 & 151 & 150 & 149 & 149 & 148\\
    159 & 159 & 158 & 157 & 157 & 156 & 155 & 155
    \end{pmatrix}.
\]

\subsection{Step 3: Encoding}

There are two distinct forms of encoding, one for the DC coefficient and one for the AC coefficients.

\paragraph{Encoding the DC coefficient}
The DC coefficient is directly related to the average value of the data in the block. As such, it typically does not change massively from one block to the next. We then use \Important{differential encoding}, where we encode the difference between the current block and the previous one.

Values of the change are grouped in \Important{Categories}. Intuitively, the $n$-th category contains all changes that need $n$ bits to be written. Formally,
\begin{enumerate}
    \item for $n=0$, the category $C_0$ is $C_0 = \{0\}$;

    \item  then for all $1 \le n \le 15$, the category $C_n$ is defined as
    \[
        C_n = \{ -(2^n - 1), \dots, -2^{n-1} \} \cup \{ 2^{n-1}, \dots, 2^n - 1 \};
    \]
    (hence $C_1 = \{-1,1\}$, $C_2 = \{-3, -2, 2, 3\}$, and so on)

    \item finally, for $n=16$, $C_{16} = 2^{15}$.
\end{enumerate}
The category numbers are encoded using \Important{Huffman coding}; we then use $n$ bits to specify a particular value in category $n$. For instance, if the difference is $6$, then we would send the Huffman codeword for category $3$ and then $3$ bits to specify that the value is $6$.


\paragraph{Encoding the AC coefficients}
The AC coefficients also use the grouping into categories but they are encoded according to a different Huffman code. The key idea is that we are likely to encounter long runs of zeros in the zigzag scan (even before the EOB), so we want to encode a whole run of zeros at a time. Therefore, we not only encode the category number $C$, but also the number $Z$ of zero-valued labels since the last nonzero label. The pair $C/Z$ forms a pointer to a predetermined Huffman code.

For instance, suppose the value is $-2$ (Category $C = 2$) and there have been $Z = 5$ zero-valued labels prior to this label in the zigzag scan. Then we would send the Huffman codeword for $2/5$ and then send $2$ more bits to identify the value $2$.







\subsection{See further}


\paragraph{JFIF} JPEG is the algorithm to compress images; the JPEG File Interchange Format (\Define{JFIF}) is the most common file format for storing JPEG encoded images. It contains all the necessary supplementary information, such as the image size.

\paragraph{Run Length Encoding} The idea of encoding lengths of runs (used in the encoding of AC coefficients) is actually a very basic form of compression, called Run Length Encoding (\Define{RLE}). It is the base of the \Important{Bitmap} image format.

\paragraph{QM coder} The \Important{QM} coder is a binary adaptive arithmetic encoder. It can be used as an alternative to Huffman coding. Using QM may yield a higher compression rate but at a cost of higher complexity; as such it is seldom used. The QM coder has variants, notably the MQ coder and the M coder.



\subsection{Exercises}

\begin{exercise} \label{exercise:transform_coder}
Write a program simulating an $8 \times 8$ transform coder and decoder without quantization. This should work for the DCT, DFT and DWHT. You may pre-compute and store the transform matrices.
\end{exercise}

\begin{exercise}
Using the program for Exercise \ref{exercise:transform_coder}, perform a very crude quantization: only keep the first $M$ coefficients in the zigzag scan,  where $M$ is a parameter that can be set from $1$ to $N$. Compare the quality of the reconstruction for DFT, DCT, and DWHT. Try to use continuous tone image blocks or random image blocks.
\end{exercise}





\section{Transform coding I: Mathematical background}
\label{sec:06}



\subsection{Fourier series and Fourier transform}

\paragraph{Fourier series}
The representation of periodic functions in terms of a series of sines and cosines was first used by Fourier in 1812. This idea has spread like wildfire across mathematics and its applications and is used in a ridiculous range of areas under different guises.

In this section, we want to give an introduction to Fourier series. We shall be rather informal: we will not prove many of the claimed results, and we will not worry about the precise assumptions we make.

Let $f$ be a periodic function of period $T$, i.e.
\[
    f(t) = f(t + nT) \quad \forall t \in \R, n \in \Z.
\]
Then we can write $f(t)$ as
\[
    f(t) = \sum_{k=-\infty}^\infty c_k e^{\i k \omega t},
\]
where $\i = \sqrt{-1}$ and
\[
    \omega = \frac{2 \pi}{ T }.
\]
Equivalently, we have
\[
    f(t) = a_0 + \sum_{k=1}^\infty a_k \cos ( k \omega t ) + \sum_{k=1}^\infty b_k \sin ( k \omega t ).
\]
In other words, the periodic functions of period $T$ form a vector space, and $\{ e^{\i n \omega t} \}$ forms a basis. In fact, this basis is orthonormal w.r.t. the inner product
\[
    \langle f(t), g(t) \rangle = \frac{1}{T} \int_0^T f(t) g^*(t) \ dt,
\]
where $g^*(t)$ denotes the complex conjugate of $g$. The coefficients can then be obtained as follows:
\begin{equation} \label{eq:fourier}
    c_k = \langle f(t), e^{\i k \omega t} \rangle = \frac{1}{T} \int_0^T f(t) e^{-\i k \omega t} dt.
\end{equation}

The intuition behind this representation is as follows. Suppose we have a periodic signal $f$. Then its $c_k$ Fourier coefficients decompose the signal into basic fluctuating signals; each $e^{\i k \omega t}$ fluctuates at a frequency of $k \omega/(2 \pi)$. As such, these coefficients give us a measure of the different amounts of fluctuation in the signal.

\paragraph{Discrete Fourier transform}
We want to handle signals that are not periodic, but instead limited in time. Suppose we have a signal $f(t)$ that is limited in time (say from $t=0$ to $t = t_1$). Then we can extend it to a periodic signal by doing
\[
    f_P(t) = \sum_{n \in \Z} f(t - nT),
\]
where $T > t_1$. This is the so-called periodic extension of $f$.

Moreover, we are dealing with discrete signals: instead of $f(t)$, we are considering $\{f_0, f_1, \dots, f_{N-1} \}$. We can discretise Equation \eqref{eq:fourier} as follows:
\[
    F_k = \frac{1}{\sqrt{N}} \sum_{n=0}^{N-1}  f_n e^{-\i \frac{2 \pi k n }{N}} \quad k = 0, 1, \dots, N-1.
\]
The coefficients $F_k$ are called the discrete Fourier transform (\Define{DFT}) of $f$. We can recover the signal from its DFT by
\[
    f_n = \frac{1}{\sqrt{N}} \sum_{k=0}^{N-1} F_k e^{\i \frac{2 \pi k n }{N}} \quad n = 0, 1, \dots, N-1.
\]

\paragraph{Matrix representation of DFT}
Let $f = (f_0, f_1, \dots, f_{N-1})$ and let $F = (F_0, \dots, F_{n-1})$ be its DFT. Then we have
\[
    F = {\bf A} f,
\]
where ${\bf A}$ is an $N \times N$ matrix with coefficients
\[
    a_{i,j} = \frac{1}{\sqrt{N}} e^{- \i \frac{2\pi}{N} ij}.
\]
For example, with $N = 4$ we obtain
\[
    {\bf A} = \frac{1}{\sqrt{4}} \begin{pmatrix}
    1 & 1 & 1 & 1\\
    1 & -\i & -1 & \i\\
    1 & -1 & 1 & -1\\
    1 & \i & -1 & -\i
    \end{pmatrix}
\]

\paragraph{Unitary and orthogonal transforms}
The matrix ${\bf A}$ from the DFT is \Define{unitary}: it satisfies
\[
    {\bf A}^\dagger = {\bf A}^{-1},
\]
where ${\bf A}^\dagger$ is its conjugate transpose: $a^\dagger_{i,j} = a_{j,i}^*$. It is easily shown that the following are equivalent for a matrix ${\bf U}$:
\begin{enumerate}
    \item ${\bf U}$ is unitary;

    \item ${\bf U}$ preserves the inner product, i.e.
    \[
        \langle x,y \rangle = \langle {\bf U} x, {\bf U}y \rangle;
    \]

    \item ${\bf U}$ preserves the norm, i.e.
    \[
         \| x \|^2 = \sum_{i=0}^{N-1} |x_i|^2 = \langle x,x \rangle = \langle {\bf U} x, {\bf U}x \rangle = \sum_{i=0}^{N-1} |({\bf U}x)_i|^2 = \| {\bf U} x \|^2.
    \]
\end{enumerate}

We will not actually use the DFT and instead we will restrict ourselves to real matrices. A real unitary matrix is called \Define{orthogonal}. So an orthogonal matrix is a real matrix ${\bf A}$ such that
\[
    {\bf A}^{-1} = {\bf A}^\top.
\]



\subsection{Two-dimensional transforms}

\paragraph{Two-dimensional data}
In general, in one dimension, we could apply any orthogonal transform as such:
\[
    \theta = {\bf A}x, \quad x = {\bf A}^\top \theta.
\]
We will use transforms for two-dimensional data (small blocks of pixels). How are we going to apply a one-dimensional transform to two-dimensional data? The answer is actually easy: we apply the transform column-wise and row-wise.

\paragraph{Matrix form} More succinctly, let ${\bf X}$ be an $N \times N$ matrix with entries $\{x_{i,j} : i,j = 0, \dots, N-1\}$. We then perform
\[
    \Theta = {\bf A} {\bf X} {\bf A}^\top.
\]
($\Theta$ is another $N \times N$ matrix.) Multiplying on the left by ${\bf A}$ performs the transform column-wise, while multiplying on the right by ${\bf A}^\top$ performs the transform row-wise. By associativity,
\[
    \Theta = ({\bf A} {\bf X}) {\bf A}^\top = {\bf A} ( {\bf X} {\bf A}^\top ),
\]
and hence the order does not matter!

Note that the inverse is straightforward:
\[
    {\bf X} = {\bf A}^\top \Theta {\bf A}.
\]

\paragraph{Basis matrices}
Let ${\bf X}$ be an $N \times N$ matrix. For all $i,j \in \{0, \dots, N-1\}$, let ${\bf E}_{i,j}$ be the matrix with a single $1$ in position $(i,j)$ and $0$ everywhere else. Then those matrices form a basis for the vector space of all $N \times N$ matrices, and we have the decomposition
\[
    {\bf X} = \sum_{i=0}^{N-1} \sum_{j=0}^{N-1} x_{i,j} {\bf E}_{i,j}.
\]
(Trivial, isn't it?)

Any two-dimensional orthogonal transform also yields a similar decomposition. We still denote $\Theta = {\bf A} {\bf X} {\bf A}^\top$, and we denote the entries of $\Theta$ as $\theta_{i,j}$. We then have
\begin{align*}
    {\bf X} &= {\bf A}^\top \Theta {\bf A}\\
            &= {\bf A}^\top \left( \sum_{i,j} \theta_{i,j} {\bf E}_{i,j} \right) {\bf A}\\
            &= \sum_{i,j} \theta_{i,j} {\bf A}_{i,j},
\end{align*}
where the matrices ${\bf A}_{i,j} := {\bf A}^\top {\bf E}_{i,j} {\bf A}$ are the \Define{basis matrices} of the transform. More concretely, denote the $k$-th row of ${\bf A}$ as $a_k$, then
\[
    {\bf A}_{i,j} = a_i^\top a_j.
\]

For instance, let
\[
    {\bf A} =  \frac{1}{ \sqrt{2} }
    \begin{pmatrix}
    1 & 1\\
    1 & -1
    \end{pmatrix}.
\]
Then the four basis matrices are given by
\begin{alignat*}{3}
    {\bf A}_{0,0} &= \frac{1}{2}
    \begin{pmatrix}
    1 & 1\\
    1 & 1
    \end{pmatrix} &
    \qquad & &
    {\bf A}_{0,1} &= \frac{1}{2}
    \begin{pmatrix}
    1 & -1\\
    1 & -1
    \end{pmatrix} \\
    {\bf A}_{1,0} &= \frac{1}{2}
    \begin{pmatrix}
    1 & 1\\
    -1 & -1
    \end{pmatrix} &
    \qquad & &
    {\bf A}_{1,1} &= \frac{1}{2}
    \begin{pmatrix}
    1 & -1\\
    -1 & 1
    \end{pmatrix}.
\end{alignat*}




\subsection{Modus operandi of transform coding}


\paragraph{Norm and energy}
The square of the norm of a vector can be viewed as its \Important{energy}. Orthogonal transforms preserve the energy (as they are unitary matrices). The efficacy of a transform depends on how much energy compaction is provided by the transform. Intuitively, the transform helps to accumulate the energy on a few symbols; those symbols should be kept, while the other ones can be discarded at little loss.

One measure of the energy compaction offered by a transform is the \Define{transform coding gain}, defined as follows (for simplicity, we define it for one-dimensional data). Let $\sigma_i^2$ be the variance of the transformed coefficient $\theta_i$ for $i=0, \dots, N-1$, then the transform coding gain is the ration of the arithmetic mean of variances over their geometric mean:
\[
    G_{\mathrm{TC}} := \frac{ \frac{1}{N} \sum_{i=0}^{N-1} \sigma_i^2 }{ \left(\sum_{i=0}^{N-1} \sigma_i^2 \right)^{\frac{1}{N}} }.
\]
(The derivation of this gain is out of the scope of this course.)


\paragraph{Three steps of transform coding}
Transform coding consists of three steps.
\begin{description}
\item[Step 1: Transform] First, the data is split into blocks of size $N$. Each block is mapped into a transform sequence using a reversible transform (usually orthogonal).

\item[Step 2: Quantization] Secondly, the transformed sequence is quantized. The quantization strategy depends on three main factors:
\begin{enumerate}
    \item the desired average bit rate

    \item the statistics of the various elements of the transformed sequence

    \item the effect of distortion in the transformed coefficients on the reconstructed sequence.
\end{enumerate}
Quantization is an important problem with some nice maths behind it, but we will skip the details here.

\item[Step 3: Encoding] Thirdly, the quantized value is encoded using some binary encoding technique, e.g. Huffman or arithmetic coding.
\end{description}









\subsection{See further}

\paragraph{FFT} The Fast Fourier Transform (\Define{FFT}) is, as its name suggests, a very efficient way of computing the DFT via divide and conquer. It was due to Cooley and Tuckey in 1965\dots even though Gauss had already discovered it in 1805!


\paragraph{Karhunen-Lo\`eve transform}
The \Define{KL} transform is the one that maximises the transform coding gain. However, it is based on the actual data and is impractical for our purposes.













\subsection{Exercises}

\begin{exercise}
Walsh-Hadamard transform. For all $N$ a power of $2$, the discrete Walsh-Hadamard transform (\Define{DWHT}) is defined as follows. For $N = 1$, let ${\bf H}_1 = (1)$. For $N \ge 1$, let
\[
    {\bf H}_{2N} = C \begin{pmatrix}
    {\bf H}_N & {\bf H}_N\\
    {\bf H}_N & - {\bf H}_N
    \end{pmatrix}.
\]
(This transform comes under different names; to make matters worse, those matrices are sometimes called Sylvester matrices.)
\begin{enumerate}
    \item Verify that the Discrete Walsh-Hadamard Transform is indeed orthogonal for the right choice of constant $C$.

    \item Give ${\bf H}_2$, ${\bf H}_4$ and ${\bf H}_8$.

    \item Give all sixteen basis matrices for $N = 4$.

    \item Find a closed form formula for the $(i,j)$ entry of ${\bf H}_N$.
\end{enumerate}
\end{exercise}


\begin{exercise}
Give the sixteen basis matrices for the DFT with $N = 4$.
\end{exercise}

\begin{exercise}
Consider the data
\[
    {\bf X} = \begin{pmatrix}
    4 & 3 & 2 & 1\\
    3 & 2 & 1 & 1\\
    2 & 1 & 1 & 0\\
    1 & 1 & 0 & 1
    \end{pmatrix}.
\]
\begin{enumerate}
    \item Compute its two-dimensional DFT.

    \item Compute its two-dimensional DWHT.
\end{enumerate}
\end{exercise}







\section{Context-based compression}
\label{sec:05}



\subsection{Context}

\paragraph{Context-based compression} Statistical compression (mainly for text, but not only) can be based on two properties. The first property is the frequency of symbols: the model assigns probabilities to the symbol according to their frequency in the document.

The second one is the \Important{context}. In practice, the context a symbol consists of the $N$ symbols preceding it (note that we cannot use symbols succeeding it, as the decoder typically does not know them yet!). Context-based compression then uses the context of a symbol to \Important{predict} it (i.e. to assign it a probability).

For instance, let's use a context of only one character. The letter h occurs in typical English text only about 5\% of the time. However, if the current symbol is t, then there is a much higher probability (around 30\%) that the next symbol will be h, since the digram th is very common in English. Note that the prediction is about assigning probabilities, not trying to figure out the next symbol exactly (which is impossible).

\paragraph{Static v Adaptive contexts} A static context-based modeler always uses the same probabilities, which are stored in some large table. Those probabilities are usually obtained by crawling through many documents (say typical English texts). There are issues with that approach, notably the fact that this might assign zero probabilities to some strings.

An adaptive context-based modeler also maintains tables of probabilities of all the possible digrams (or trigrams, or even longer $n$-grams). But this time the tables are updated all the time as more data are input, which adapts the probabilities to the particular data being compressed. \Important{Adaptive context-based compression} might be slower, but typically results in better compression.



\paragraph{Context length} One may think at first that the larger the number $N$ of symbols in the context, the better the compression. However, this might not be the case:
\begin{enumerate}
    \item A large $N$ requires to write the first $N$ symbols in plain text, which might hurt the overall compression.

    \item If $N$ is too large, then there are simply too many contexts, which makes storing, reading off, and writing on the table of probabilities infeasible.

    \item A very long context contains information about the nature of old data. It is not uncommon to have files where different parts have different symbol distributions.
\end{enumerate}
Therefore, in practice relatively small contexts are used in practice (for text compression, traditional methods use no more than 10 characters).


\subsection{PPM}

\paragraph{Basic idea} Prediction by Partial Matching (\Define{PPM}) is based on an encoder that keeps a statistical model of the text. It starts with an order-$N$ context. It searches its data structure for an occurrence of the current context $C$ followed by the next symbol $S$. If it finds no such occurrence, if decreases the order of the context to $N-1$ and tries again (the new context $C'$ is the final $N-1$ characters of $C$). It keeps \Important{shortening the context} until it is successful.

The encoder reads the next symbol $S$ from the input stream, looks at the current order-$N$ context $C$ (the last $N$ symbols read), and based on the previous input data, computes the probability $P$ that $S$ will appear following $C$. The encoder then calls an adaptive arithmetic encoder to encode $S$ with probability $P$. If the probability $P$ is zero, the PPM encoder tries with a smaller context; it reduces the context until $P \ne 0$. What if the symbol $S$ has not been seen yet (and hence, even with order-$0$ context, we still have $P = 0$)? Then the PPM encoder enters \Define{order-$(-1)$ context}, where the probability of $S$ is simply $1/$(size of alphabet).

\begin{example} \label{example:ppm_contexts}
Let us look at the contexts and frequency counts for the following string with $11$ symbols:
\begin{quote}
    xyzzxyxyzzx
\end{quote}

\begin{tabular}{r l | r l | r l | r l | r l}
    Order 4 & & Order 3 & & Order 2 & & Order 1 & & Order 0 &   \\
    \hline
    xyzz $\to$ x & 2 & xyz $\to$ z & 2 & xy $\to$ z & 2 & x $\to$ y & 3 & x & 4\\
    yzzx $\to$ y & 1 & yzz $\to$ x & 2 & xy $\to$ x & 1 & y $\to$ z & 2 & y & 3\\
    zzxy $\to$ x & 1 & zzx $\to$ y & 1 & yz $\to$ z & 2 & y $\to$ x & 1 & z & 4\\
    zxyx $\to$ y & 1 & zxy $\to$ x & 1 & zz $\to$ x & 2 & z $\to$ z & 2 &  & \\
    xyxy $\to$ z & 1 & xyx $\to$ y & 1 & zx $\to$ y & 1 & z $\to$ x & 2 &  & \\
    yxyz $\to$ z & 1 & yxy $\to$ z & 1 & yx $\to$ y & 1 & & &  &
\end{tabular}
\end{example}


Now, how does the encoder tell the decoder which order context it is currently using (and hence what the decoder should be using too)? The answer is to have a dedicated \Important{escape symbol}, which we'll denote esc, which should be output whenever the context size is decreased. Since this is a new character, we should also assign a probability for the escape symbol for every encountered context. There are various ways (heuristics) of assigning such probabilities. Here, we will use the so-called Method A, where the escape symbol is assigned a frequency of 1.


We are now in position to give a more explicit example. Encoding a full sequence is actually quite tedious to explain so we'll only encode a few characters. We use contexts of order at most 2. Let us consider
\begin{quote}
    this\textvisiblespace is\textvisiblespace the\textvisiblespace tithe
\end{quote}
The first few symbols are not very interesting, so let us skip forward. Let's assume we have already encoded ``this\textvisiblespace is'' and we wish to encode the next character \textvisiblespace.

We assume the word length for arithmetic coding is six bits (we used four decimal digits in our example in Lecture \ref{sec:02}). For the sake of simplicity, we have $Low = 0$ and hence $L^* = 000000$ and $High = 1$ hence $H^* = 111111$. (As we shall see, the low and high values may vary over time.)

Here is what the table of contexts looks like\\
~\\
\begin{tabular}{lr | lr | lr}
    Order 2 &           & Order 1 & & Order 0   \\
    \hline
    th $\to$ i      & 1 & t $\to$ h & 1 & t & 1\\
    th $\to$ esc    & 1 & t $\to$ esc & 1 & h & 1 \\
    hi $\to$ s      & 1 & h $\to$ i & 1 & i & 2\\
    hi $\to$ esc    & 1 & h $\to$ esc & 1 & s & 2 \\
    \Important{is $\to$ \textvisiblespace}      & \Important{1} & i $\to$ s & 2 & \textvisiblespace & 1\\
    \Important{is $\to$ esc}    & \Important{1} & i $\to$ esc & 1 & esc & 1 \\
    s\textvisiblespace $\to$ i      & 1 & \textvisiblespace $\to$ i & 1 \\
    s\textvisiblespace $\to$ esc    & 1 & \textvisiblespace $\to$ esc & 1 \\
    \textvisiblespace i $\to$ s      & 1 & s $\to$ \textvisiblespace & 1 \\
    \textvisiblespace i $\to$ esc    & 1 & s $\to$ esc & 1
\end{tabular}
~\\
~\\
The  second-order context is ``\Important{is}''. We use characters in the order of the table: the first row gives the first interval and so on. In this context, the probability of the space sign ``\textvisiblespace'' and the probability of the escape symbol esc are both equal to 1/2, and
\[
    L(\text{\textvisiblespace}) = 0, H(\text{\textvisiblespace}) = 1/2 = L(esc), H(esc) = 1.
\]
    The update equations for the new $Low$ and $High$ are
\begin{align*}
    Low &\gets Low + (High - Low) L(x) = 0,\\
    L^* &\gets 000000,\\
    High & \gets Low + (High - Low) H(x) = 1/2,\\
    H^* &\gets 011111.
\end{align*}
Since the first (most significant) bit of $L^*$ and $H^*$ coincide, we shift that bit out and shift $0$ into $L^*$ and shift $1$ into $H^*$. So we obtain:
\begin{enumerate}
    \item Encoded sequence for ``\textvisiblespace'': $0$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}

The table of contexts now becomes:\\
~\\
\begin{tabular}{lr | lr | lr}
    Order 2 &           & Order 1 & & Order 0   \\
    \hline
    th $\to$ i      & 1 & t $\to$ h & 1 & \Structure{t} & \Structure{1}\\
    th $\to$ esc    & 1 & t $\to$ esc & 1 & h & 1 \\
    hi $\to$ s      & 1 & h $\to$ i & 1 & i & 2\\
    hi $\to$ esc    & 1 & h $\to$ esc & 1 & s & 2 \\
    is $\to$ \textvisiblespace      & 2 & i $\to$ s & 2 & \textvisiblespace & 2\\
    is $\to$ esc    & 1 & i $\to$ esc & 1 & esc & 1 \\
    \Structure{s\textvisiblespace $\to$ i}      & \Structure{1} & \Structure{\textvisiblespace $\to$ i} & \Structure{1} \\
    \Structure{s\textvisiblespace $\to$ esc}    & \Structure{1} & \Structure{\textvisiblespace $\to$ esc} & \Structure{1} \\
    \textvisiblespace i $\to$ s      & 1 & s $\to$ \textvisiblespace & 2 \\
    \textvisiblespace i $\to$ esc    & 1 & s $\to$ esc & 1
\end{tabular}
~\\
~\\
The next symbol is ``t''. The second-order context is ``\Structure{s\textvisiblespace }''. Since ``t'' has zero frequency in this context, we need to encode the escape symbol. By a similar argument as above, we obtain
\begin{enumerate}
    \item Encoded escape symbol sequence: $1$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
We need to look at the first-order context, which is ``\Structure{\textvisiblespace}''. Again, ``t'' does not appear with this context, so we encode another escape symbol. We obtain
\begin{enumerate}
    \item Encoded escape symbol sequence: $1$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
We need to look at the zero-th order context. This time, ``t'' has already appeared, and is assigned the interval $[0, 1/9 )$. We then have
\begin{align*}
    Low &\gets Low + (High - Low) L(x) = 0,\\
    L^* &\gets 000000,\\
    High & \gets Low + (High - Low) H(x) = 1/9,\\
    H^* &\gets 000111.
\end{align*}
Since the three leftmost bits are equal, we shift them out. We finally obtain
\begin{enumerate}
    \item Encoded sequence: for ``t'': $000$,

    \item Lower bound $L^* = 000000$,

    \item Higher bound $H^* = 111111$.
\end{enumerate}
So, to encode ``\textvisiblespace t'', we have transmitted $011000$.

Note that there would be a slight difference in practice: to keep everything integral, we would use $High = 63$, $Low = 0$ and perform an update of the form
\begin{align*}
    Low &\gets Low + \left\lfloor (High - Low + 1) \frac{ 1 }{ 9 }  \right\rfloor  = 0 = 000000\\
    High & \gets High + \left\lfloor (High - Low + 1) \frac{ 1 }{ 9 }  \right\rfloor - 1  = 6 = 000110
\end{align*}
Then we would have: Higher bound $H^* = 110111$.



\paragraph{Methods B and C} Two other main ways of assigning frequencies to the escape symbols aim to make the escape symbol more probable, which typically reduces the size of the resulting sequence for that symbol. The main idea is that if a context is followed by many different characters, then you are likely to encounter yet another character following that same context. For instance, think of the context ``s'' in English, which can be followed by virtually any other letter. \Important{Methods B and C} give the escape symbol a count equal to the number of symbols following the context; Method B then subtracts the count of every other symbol by one, while Method C does not amend those.


\subsection{See further}

\paragraph{RAR} The main application of PPM is in the Roshal Archive (\Define{RAR}) file format.


\paragraph{Context mixing} In \Define{context mixing}, the next-symbol predictions of two or more statistical models are combined to yield a prediction that is often more accurate than any of the individual predictions. The \Important{PAQ} series of data compression programs use context mixing; they are the cutting edge in lossless compression in terms of compression ratio (at the expense of speed and memory usage) \cite{KF11}.

Note that the problem of mixing different contexts is a very challenging issue in machine learning; that could be a very interesting topic for a project...


\paragraph{BWT} The Burrows-Wheeler transform (\Define{BWT}) is a very clever way of converting a list of symbols into one that is much more structured. You only need a little more information to make sure that the transform does not lose any information. By structured, we mean that it is ``almost sorted.'' After the transform, one can use very simple techniques to efficiently encode the structured list. Unfortunately, BWT-based compression requires to scan and to manipulate the whole message, which is an important drawback compared to the adaptive PPM.


\subsection{Exercises}


\begin{exercise} \label{exercise:ppm_contexts}
Update the table in Example \ref{example:ppm_contexts} if the following character is x.
\end{exercise}

\begin{exercise}
Finish the encoding of the sequence ``this\textvisiblespace is\textvisiblespace the\textvisiblespace tithe''. To update $High$ and $Low$, you may use our simple technique based on rational numbers, or use the version with integers instead.
\end{exercise}




\section{Lempel-Ziv II: LZ78}
\label{sec:04}


\subsection{LZ78}

\paragraph{Basic idea} The \Define{LZ78} method does not use any search buffer, look-ahead buffer, or sliding window. Instead, it simply keeps a dictionary of previously encountered strings. The dictionary starts with the empty string at position zero and its size is only limited by the memory size.

The encoder outputs \Important{two-field tokens} (instead of three-field tokens in LZ77). Each token simply corresponds to a new string in the dictionary: it is of the form
\[
    (i, x),
\]
where $i$ is the position of the longest match in the dictionary and $x$ is the final character of the string.

Nothing is ever deleted from the dictionary:
\begin{itemize}
    \item Advantage over LZ77: future strings will be compressed even if they only match strings in the distant past;

    \item Drawback: the dictionary can become very large!
\end{itemize}

\paragraph{Example} Once again, it is best explained via a simple example. Say we want to compress
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily%\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}

The tokens are then (in order!):\\
~\\
\begin{tabular}{l|l|l}
     Dictionary position    & String    & Token  \\
     \hline
     0                      &  $\epsilon$         &        \\
     1                        & s           &  (0, s)      \\
     2                   & i          & (0,i)       \\
     3                   &  r         & (0,r)       \\
     4                   &  \textvisiblespace         & (0,\textvisiblespace)       \\
     5                   &  si         &    (1,i)    \\
     6                   &  d         & (0,d)       \\
     7                   &  \textvisiblespace e         &  (4,e)      \\
     8                   &  a         & (0,a)       \\
     9                   &  st         &    (1,t)    \\
     10                   & m          &    (0,m)    \\
     11                   & an          &   (8,n)     \\
     12                   & \textvisiblespace ea          & (7,a)       \\
     13                   & sil          &  (5,l)      \\
     14                   & y          &    (0,y)
\end{tabular}
~\\
And the compressed output is the list of tokens
\begin{quote}
    (0,s) (0,i) (0,r) (0,\textvisiblespace) (1,i) (0,d) (4,e) (0,a) (1,t) (0,m) (8,n) (7,a) (5,l) (0,y)
\end{quote}

Once again, the decoder sees these tokens as ``instructions.'' But following these instructions means searching in the dictionary. A useful data structure for the dictionary is a \Important{tree}, where the root is the empty string and a new string is added to the tree as a child of the string it refers to on its token. Such a tree is called a \Define{trie}.

\begin{tikzpicture}
    \node (0) at (3,0) {$\epsilon$};

    \node (1) at (0,-1) {s};
    \node (2) at (1,-1) {i};
    \node (3) at (2,-1) {r};
    \node (4) at (3,-1) {\textvisiblespace};
    \node (5) at (0,-2) {si};
    \node (6) at (4,-1) {d};
    \node (7) at (3,-2) {\textvisiblespace e};
    \node (8) at (5,-1) {a};
    \node (9) at (1,-2) {st};
    \node (10) at (6,-1) {m};
    \node (11) at (5,-2) {an};
    \node (12) at (3,-3) {\textvisiblespace ea};
    \node (13) at (0,-3) {sil};
    \node (14) at (7,-1) {y};

    \draw (0) -- (1);
    \draw (0) -- (2);
    \draw (0) -- (3);
    \draw (0) -- (4);
    \draw (1) -- (5);
    \draw (0) -- (6);
    \draw (4) -- (7);
    \draw (0) -- (8);
    \draw (1) -- (9);
    \draw (0) -- (10);
    \draw (8) -- (11);
    \draw (7) -- (12);
    \draw (5) -- (13);
    \draw (0) -- (14);


\end{tikzpicture}



\subsection{LZW}


\paragraph{Basic idea} Lempel-Ziv-Welch (\Define{LZW}) is a variant of LZ78, with two main differences.
\begin{enumerate}
    \item The dictionary is \Important{initialised with all possible characters}. If we are compressing an ASCII file, then positions 0 to 255 are filled at initialisation.

    \item The tokens only have \Important{one field}! Since we always work with at least one character (that can always be found in the dictionary), there is no need to output the next character.
\end{enumerate}

Let us go back to our example:
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily%\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}

The dictionary is initialised with all 256 ASCII characters in positions 0 to 255, e.g. a is in position 97, b in 98, s in 115, z in 122. The first character in the string is s (in the dictionary at position 115). Since si does not appear in the dictionary, we add si to the dictionary at 256, and we continue with the character i. Again, since ir is not in the dictionary, we add ir at 257 and continue with the character r.

The dictionary (omitting positions 0 to 255) and the tokens look like this:\\
~\\
\begin{tabular}{l|l|l|l}
     Position & String & Token & What the token encodes \\
     \hline
     256 & si & 115 & s\\
     257 & ir & 105 & i\\
     258 & r\textvisiblespace & 114 & r\\
     259 & \textvisiblespace a & 32 & \textvisiblespace\\
     260 & sid & 256 & si\\
     261 & d\textvisiblespace & 100 & d\\
     262 & \textvisiblespace e & 32 & \textvisiblespace\\
     263 & ea & 101 & e\\
     264 & as & 97 & a\\
     265 & st & 115 & s\\
     266 & tm & 116 & t\\
     267 & ma & 109 & m\\
     268 & an & 97 & a\\
     269 & n\textvisiblespace & 110 & n\\
     270 & \textvisiblespace ea & 262 & \textvisiblespace e\\
     271 & asi & 264 & as\\
     272 & il & 105 & i\\
     273 & ly & 108 & l\\
        &   & 121 & y
\end{tabular}
~\\~\\
The output is then
\[
    115, 105, 114, 32, 256, 100, 32, 101, 97, 115, 116, 109, 97, 110, 262, 264, 105, 108, 121
\]

The dictionary can once again be stored as a tree, but the implementation is more complex than for LZ78. A thorough description is given in \cite[3.13.2]{Sal04}.


\subsection{Applications of LZW}

\paragraph{GIF} The ubiquitous Graphics Interchange Format (\Define{GIF}) uses a variation of LZW. It uses a dynamic, growing dictionary. It starts with the number of bits per pixel $b$: $b=2$ for monochromatic images, $b=8$ for an image with $256$ colours of shades of grey. The dictionary starts with $2^{b+1}$ entries and is doubled in size every time it fills up until it reaches $2^{12} = 4,096$ entries. At that point, the encoder may want to start a new dictionary!

GIF is not actually that good at image compression because it is unidimensional. It scans the image row after row, so it can detect similarities within a row but has trouble dealing with similarities across rows instead.

\paragraph{Limitations} One major issue of using LZW (e.g. for GIF), is that LZW is \Important{patented}. In response to that, the Portable Network Graphics format was created in the mid-90s (finalised in 96). It is based on DEFLATE (and hence LZSS) instead.

Another application of LZW was the Unix shell compression utility \texttt{compress}, that was used in the 80s. However, it was superseded by \texttt{gzip}, which typically outperforms it in terms of compression ratio.

\subsection{See further}

\paragraph{Variants} LZ78 and LZW also have a few variants, notably LZMW, LZAP and LZY. Have another look!

\paragraph{Kolmogorov complexity} The principle of Lempel-Ziv encoding is to construct a list of instructions to the decoder of the form ``Copy that string (and add that character).'' But what if we allowed any sort of instructions?

The \Define{Kolmogorov complexity} is a concept that predates Lempel-Ziv. It aims at evaluating the ``intrinsic'' complexity of a binary string. Simply put, the Kolmogorov complexity of a string $x$ w.r.t. a Turing machine $U$, denoted $K_U(x)$, is the shortest length of a program for $U$ that prints out $x$ and halts. Obviously, $K_U(x)$ is not computable. But still, we can say a lot about the Kolmogorov complexity of a random string: it's about the length of the string. Therefore, almost any string is incompressible! The study of Kolmogorov complexity and associated concepts (e.g. Solomonoff's universal probability or Chaitin's Omega number) is very intriguing but outside the scope of this course.



 \subsection{Exercises}

\begin{exercise}
Encode the string
\begin{quote}
    sir\textvisiblespace sid\textvisiblespace eastman\textvisiblespace easily\textvisiblespace teases\textvisiblespace sea\textvisiblespace sick\textvisiblespace seals
\end{quote}
with LZ78: give the dictionary table, the trie, and the output.

Encode the same string with LZW.
\end{exercise}


\begin{exercise}
Select a few (small) images, and compare the file sizes for those when saved as .bmp, .gif and .png.
\end{exercise}


\section{Lempel-Ziv I: LZ77}
\label{sec:03}


\subsection{Limitations of statistical compression}

In Lectures \ref{sec:01} and \ref{sec:02} we looked at compact codes for data being emitted by a memoryless source - a random process.

This week we look at encoding a fixed file of data efficiently. Rather than having estimates for the probabilities of each symbol, we can look at the whole message and determine the frequency of each symbol. Compact codes for memoryless sources are guaranteed to be optimal on average, but we may not have an average message. If the encoding is not determined in advance (as can be done for known sources), but is message dependent, then we must transmit the code as well as the encoded message.

Recall, memoryless sources emit each symbol independently of any previous symbols. There is no ‘pattern’ to the data beyond the frequency of each symbol. For instance, consider the message
\[
    abbaeadcaadccbaabaaa
\]
(20 characters). The letter frequencies are a:10/20, b:4/20, c:3/20, d:2/20, e:1/20. These agree exactly with the probabilities in Exercise \ref{exercise:huffman}. Using the Huffman code $abbaeadcaadccbaabaaa$ becomes
\[
    100001011110110010110110010010001100111
\]
(39 characters, as expected - 1.95 bits on average).

It is not obvious that we can do better here, and in general for randomly chosen messages with these frequencies we simply can't! But what about:
\[
    aaaaaaaaaabbbbcccdde
\]
or:
\[
    ababababacacacadadae ?
\]
Again, Huffman coding would yield 39 characters. Clearly, those messages have more than just statistical redundancy; they also have a form of structural redundancy to which statistical methods such as Huffman coding are oblivious.





\paragraph{Source modelling}

A lot of work was done in the early days of text compression to model natural languages and to understand their redundancy. The first work is Shannon's statistical analysis of English text \cite{Sha51}, and it has been significantly refined over the years (see Cover and King \cite{CK78} for a survey of techniques).

Using a completely different approach, Zipf \cite{Zip49} exhibited a remarkable variety of hyperbolic laws in social sciences; in particular the distribution of words in a natural language approximately satisfies the beautiful law described below. Suppose a natural language has $N$ words, sorted in non-increasing frequency ($p(1) \ge p(2) \ge \dots p(r) \dots \ge p(N)$). Then the probability of the word at the $r$-th rank is
\[
    p(r) = \frac{ \mu }{ r },
\]
with
\[
    \mu \approx \frac{ 1 }{ \log_e N + \gamma },
\]
where $\gamma = 0.577\dots$ is the Euler-Mascheroni constant. Finer models have been proposed, e.g. by Mandelbrot \cite{Man52}.

We have already looked at modelling English as a sequence of random letters with frequencies. This is called the \Define{first-order model} of English. Random text from this model (plus space) would look like:
\begin{quote}
    \texttt{ocroh hli rgwr nmielwis eu ll nbnesebya th eei alhenhttpa oobttva nah brl}
\end{quote}



We could do better by regarding English not as 26 letters, but as $26^2$ pairs of letters (\Define{digrams}), E.g. AB QU ZA QZ. If we analyse the frequencies of digrams, we can choose the next letter based upon the previous letter and the digram frequencies. E.g. Q will almost certainly be followed by U, T is most likely to be followed by H. This is called the \Define{second-order model} of English.  Random text from this model (plus space) would look like:
\begin{quote}
    \texttt{on ie antsoutinys are t inctore st be s deamy achin d ilonasive tucoowe at teasonare fuso tizin andy tobe seace ctisbe}
\end{quote}

Random text from the third-order model of English would look like:
\begin{quote}
\texttt{in no ist lat whey cratict froure birs grocid pondenome of demonstures of the reptagin is regoactiona of cre}
\end{quote}
There are finer and finer models of the English language, some based on $n$-gram frequencies, other (more accurate), based on frequencies of sequences of words. Examples of text generated from $12$-gram model (for letters) and $6$-gram model (for text) can be found in \cite[Chapter 4]{BCW90}.

One could then consider using Huffman coding (or any other statistical technique) with finer and finer models. There are two major issues with this approach.
\begin{enumerate}
    \item The alphabet of the source $X$ explodes! If we consider just the $4$-gram model (for letters), then the alphabet is of size $26^4 = 456,976$. In general, the alphabet size grows exponentially with $n$ for $n$-grams.

    \item The model is only appropriate for a particular sort of text. The model for English is inappropriate for German or French, let alone Greek, Russian or Chinese. So that strategy is not easily portable.
\end{enumerate}



\subsection{Lempel-Ziv}

The main idea of dictionary based compression is to construct a table (dictionary) of commonly used subsequences and refer to this to build the coded message. The main idea behind Lempel-Ziv (LZ77) is to use the message itself as a dictionary.

The LZ77 encoding algorithm works as follows. The encoding scans the message from first to last character. For implementation purposes, it uses a sliding window, of size $W$ and a look-ahead buffer of size $L$. Consider the message $m_1 \dots m_n$. When encoding at character $i$, look for the largest $l$ such that the first $l$ characters of the look-ahead buffer match $l$ consecutive characters in the sliding window, i.e.
\[
    m_i \dots m_{i+l-1} = m_{i-d} \dots m_{i-d+l-1}
\]
where   $d \le W$ and $l \le L$. Append the coded message with $(d,l,m_{i+l})$. Resume encoding at character $i+l+1$.

The L77 decoding algorithm reads a list of triplets $(d,l,m_{i+l})$, which it interprets as the instruction:
\begin{quote}
Print out $m_{i-d} \dots m_{i-d+l-1} m_{i+l}$ (the $l$ successive characters of $m$ starting from $d$ positions ago, and then $m_{i+l}$).
\end{quote}


\begin{example}
Encoding the sequence
\[
    ABRACADABRA
\]
using LZ77 (with say infinite $W$ and $L$) yields
\[
(0,0,A) \quad
(0,0,B) \quad
(0,0,R) \quad
(3,1,C) \quad
(2,1,D) \quad
(7,4,-)
\]
\end{example}

\begin{example}
A longer example now:
\begin{quote}
    Peter Piper picked a peck of pickled peppers;\\
    A peck of pickled peppers Peter Piper picked;\\
    If Peter Piper picked a peck of pickled peppers,\\
    Where's the peck of pickled peppers Peter Piper picked?
\end{quote}

We obtain:
\begin{align*}
&(0,0,P) (0,0,e)(0,0,t)(2,1,r)(0,0, )(6,1,i)(0,0,p)(6,3,p)(6,1,c)(0,0,k)(7,1,d)\\
&(7,1,a) (9,2,e)(9,2, )(0,0,o)(0,0,f)(17,5,l)(18,3,p)(4,1,p)(32,3,s)(0,0,;)\\
&(0,0,A)(26,24, )(71,18,;)(0,0,I)(38,2,P)(93,43,,)\\
&(0,0,W)(0,0,h)(6,2,e)(0,0,')(75,2,t)(8,2, ) (103,42,?)
\end{align*}


193 characters encoded as 34 triples.
If each triple is 3 bytes - that is 193 bytes reduced to 102 bytes.

\end{example}




\paragraph{How much space do we need?}

Each triple in the encoding includes $d \le W$, $l \le L$ and a character. For ASCII, the character takes $8$ bits. In total, we need
\[
    \log_2 ( W + 1 ) + \log_2 (L + 1) + 8
\]
bits to encode a triple.

Typical values are $W = 2^{16} - 1 = 65535$, and $L = 2^8 - 1 = 255$, so we need $16+8+8$ bits per triple, i.e. $4$ bytes. This much can be wasteful, especially if the value of $l$ is very low ($l=0$ means that this is a new character for instance).


\subsection{Applications of LZ77}

\paragraph{LZSS} Lempel-Ziv-Storer-Szymanski (\Define{LZSS}) is a popular variant of LZ77 introduced in 1982. The main improvement is that it includes a flag to distinguish between new characters and tokens. That way, a new character does not need to be encoded as a full token, and tokens only have two fields instead of three.

\paragraph{DEFLATE} \Define{Deflate} is a lossless compression technique that combines LZSS and Huffman coding. The key idea is that Lempel-Ziv removes structural redundancy from the data, but its output still has some statistical redundancy; the latter is then removed by Huffman coding.

Deflate is everywhere: in \texttt{gzip}, in the \Important{ZIP} file format, in \Important{PNG}, etc. (Technically, ZIP allows for many different compression techniques, but Deflate is the one that's used most of the time.)

\paragraph{LZMA} The Lempel–Ziv–Markov chain algorithm (\Define{LZMA}) was developed for 7z. Its description is out of the scope of these lectures.

\subsection{See further}

\paragraph{Variants} There are many variants of LZ77, e.g. LZX, LZRW1, LZRW4. Have a look!

\paragraph{VCDIFF} \Define{File differencing} refers to any method that compresses the differences between two files (say the source and the target files). The term \Important{delta compression} is also used. VCDIFF is a method for file differencing based on LZ77. The basic idea is very simple:
\begin{enumerate}
    \item append the target file to the source file to make one massive file

    \item use LZ77 to compress that massive file

    \item only save the part relating to the target file of the output of LZ77.
\end{enumerate}
The implementation is more involved; see \cite{KMMV02}. In general, delta compression is an important problem that is still the subject of ongoing research.


\subsection{Exercises}

\begin{exercise}
Decode the following string encoded with LZ77.
\begin{align*}
&(0,0,r)(0,0,i)(0,0,n)(0,0,g)(0,0, ) \\
& (0,0,a)(2,1,r)(7,4,o)(7,2,o)(0,0,s)(9,1,e)(3,1, )\\
& (16,2,p)(9,1,c)(0,0,k)(9,1,t)(7,1,f)(0,0,u)(0,0,l)(1,1, )\\
& (11,1,f)(15,3,s)(24,5,t)(6,1,s)(0,0,h)(11,1,o)(8,9,w)(20,1, )\\
& (11,1,l)(33,2,f)(5,4,d)(15,1,w)(0,0,n)
\end{align*}
\end{exercise}


\begin{exercise}
Watch this youtube video on the repetitiveness of pop music: \href{https://www.youtube.com/watch?v=_tjFwcmHy5M}{\textcolor{blue}{Pop Music is Stuck on Repeat}}. Can you guess which is the most repetitive song in the history of the Billboard Hot 100?
\end{exercise}

\begin{exercise}
Write your own LZ77 encoder (in Python, Java, or any other language). Can you find famous pieces of fiction that compresses massively, or hardly at all?
\end{exercise}



\section{Statistical compression I: Huffman coding}
\label{sec:01}

\subsection{Prefix codes}

\paragraph{Memoryless sources}

We have some data that we wish to encode. It could be anything: Spoken English, Data from a digital camera sensor, DNA string, etc.

We model our data as coming from a memoryless source $X$. We imagine that symbols are emitted at random according to the probability distribution of $X$. In other words, we view our data as a random string $X_1, X_2, \dots$ over some alphabet $\mathcal{X}$. Our memoryless assumption is that those form a sequence of independent identically distributed (i.i.d.) random variables: $X_i \sim X$ for all $i$.

More concretely, for any $x \in \mathcal{X}$ and any $i$, the probability
\[
    \probability(X_i = x)
\]
is independent of $i$, and of all previous or future emitted symbols.

Note that this is not always a valid assumption. We will look into source modelling into more detail in the next lectures.



\paragraph{The coding problem}


We have a source emitting symbols in $\mathcal{X} = \{x_1, \dots, x_n\}$ with respective probabilities  $\{p_1, \dots, p_n\}$.


Question: If  $\mathcal{D}$ is an alphabet of  $D$ code symbols, how can we encode the source symbols using code words (finite strings of code symbols) as economically as possible?

Formally: a \Define{source code} is a map $C : \mathcal{X} \to \mathcal{D}^*$
where  $\mathcal{D}^*$     is the set of all finite strings of symbols in  $\mathcal{D}$.

The words $C(x)$ are called the \Define{codewords}, and the integers $|C(x)|$ (the length of $C(x)$) are the \Define{word lengths}.


We can extend the code to messages as follows. A \Define{message} is any finite string of source symbols $m = m_1 \dots m_k \in \mathcal{X}^*$ and its encoding is the obvious concatenation
\[
    C(m) = C(m_1) C(m_2) \dots C(m_k).
\]



\paragraph{Prefix codes}


A code $C$ is \Define{uniquely decodable} (a.k.a. uniquely decipherable) if every finite string in $\mathcal{D}^*$ is the image of at most one message.

A prefix of a word $w = w_1 \dots w_k \in \mathcal{D}^*$ is any word of the form $w_1 \dots w_l$ for some $0 \le l \le k$ (for $l=0$, we obtain the empty word). A code is \Define{prefix} (a.k.a. instantaneous or prefix-free) if there are no two distinct source symbols $x, y \in \mathcal{X}$ such that $C(x)$ is a prefix of  $C(y)$         .

\begin{theorem}
A prefix code is uniquely decodable.
\end{theorem}

\begin{proof}
Let $C$ be a prefix code, and let $w = C(m)$ for some message $m = m_1 \dots m_k \in \mathcal{X}^*$. We give a decoding algorithm which, given $w$, determines $m$. Let $w = w_1 \dots w_l$.

Let $i$ be the smallest integer such that $w_1 \dots w_i$ is a codeword, say $w_1 \dots w_i = C(x)$. Then the $m_1 = x$. Indeed, if $m_1 = y \ne x$, then $C(x)$ is a prefix of $C(y)$, which is a contradiction. Then repeat this step, beginning with $w_{i+1}$ and hence determining $m_2$, and so on until $w$ is empty.
\end{proof}

\begin{example}
Let $\mathcal{X} = \{a, b, c, d, e\}$, $\mathcal{D} = \{0,1\}$ and
\begin{align*}
    C(a) &= 01\\
    C(b) &= 100\\
    C(c) &= 101\\
    C(d) &= 1101\\
    C(e) &= 1111.
\end{align*}

Suppose we need to decode the word $C(m) = w = 10010111011111100101$. We proceed as follows. We read the word until we reach a codeword:
\begin{align*}
    w_1 &= 1\\
    w_1 w_2 &= 10\\
    w_1 w_2 w_3 &= 100 = C(b).
\end{align*}
Therefore $m_1 = b$. We continue until we reach a codeword:
\begin{align*}
    w_4 &= 1\\
    w_4 w_5 &= 10\\
    w_4 w_5 w_6 &= 101 = C(c).
\end{align*}
Therefore $m_2 = c$. And so on... Exercise \ref{exercise:prefix} asks you to finish this simple example.
\end{example}




\subsection{Huffman codes}


\paragraph{Compact codes}

Our main aim is to design codes where the typical length of messages is reduced dramatically. The basic idea is to assign short codewords to more frequent symbols and longer codewords to less frequent ones.


More formally, the \Define{average length} (a.k.a. expected length) of the code is
\[
    L(C) = \expectation( |C(X)| ) = \sum_{x \in \mathcal{X}} |C(x)| \probability(X = x).
\]

A code is \Define{compact} (for a given source $X$) if it is uniquely decodable and it minimises the average length of codewords over all uniquely decodable codes.


\begin{theorem}
A uniquely decodable code with prescribed word lengths exists if and only if a prefix code with the same word lengths exists.
\end{theorem}

We shall prove this result in Lecture \ref{sec:35}.

\begin{corollary}
For any source $X$, there is a compact prefix code for $X$.
\end{corollary}


\paragraph{Binary Huffman code}


The key is to construct a tree where the leaves correspond to the symbols in  $\mathcal{X}$ and the paths from the root to the leaves give the codewords.

The tree is constructed iteratively. Suppose $\mathcal{X} = \{x_1, \dots, x_n\}$ with $p_1 \ge p_2 \ge  \dots \ge p_{n-1} \ge p_n$. Then merge $x_{n-1}$ and $x_n$ into a new symbol, say $x_{n-1, n}$ with probability $p_{n-1} + p_n$, and let $x_{n-1}$ and $x_n$ be the children of $x_{n-1, n}$ on the tree. Label the edges from $x_{n-1, n}$ to its children as $0$ and $1$, respectively. Repeat for the new source $X^{(1)} = \{x_1, \dots, x_{n-2}, x_{n-1, n} \}$ (making sure to order the symbols in non-decreasing probability). Repeat until the final source $X^{(n-1)}$ only has one symbol left with probability $1$; that symbol is the root of the tree.

Once the tree is built, read off the labels on the path from the root to a leaf to get the corresponding codeword.

\begin{example} \label{example:huffman}
Let $X$ with respective probabilities $a: 0.4, b: 0.2, c: 0.15, d:0.15, e:0.1$.

\begin{tikzpicture}[xscale=2]
    %\node (a0) at (0,4) {$a$}; %0.4
    %\node (b0) at (0,3) {$b$}; %0.2
    %\node (c0) at (0,2) {$c$}; %0.15
    \node (d0) at (1,2) {$d$}; %0.15
    \node (e0) at (1,1) {$e$}; %0.1

    %\node (a1) at (1,4) {$a$}; %0.4
    %\node (de1) at (1,3) {$de$}; %0.25
    \node (b1) at (1,4) {$b$}; %0.2
    \node (c1) at (1,3) {$c$}; %0.15

    %\node (a2) at (2,4) {$a$}; %0.4
    \node (bc2) at (2,4) {$bc$}; %0.35
    \node (de2) at (2,2) {$de$}; %0.25

    \node (bcde3) at (3,4) {$bcde$}; %0.6
    \node (a3) at (3,3) {$a$}; %0.4

    \node (abcde4) at (4,4) {$abcde$}; %1

\draw (de2) -- node[above] {$0$} ++(d0);
\draw (de2) -- node[below] {$1$} ++(e0);

\draw (bc2) -- node[above] {$0$} ++(b1);
\draw (bc2) -- node[below] {$1$} ++(c1);

\draw (bcde3) -- node[above] {$0$} ++(bc2);
\draw (bcde3) -- node[below] {$1$} ++(de2);

\draw (abcde4) -- node[above] {$0$} ++(bcde3);
\draw (abcde4) -- node[below] {$1$} ++(a3);


\end{tikzpicture}

The code is then
\begin{align*}
    C(a) &= 1\\
    C(b) &= 000\\
    C(c) &= 001\\
    C(d) &= 010\\
    C(e) &= 011
\end{align*}
The average length is then $2.2$ bits per symbol.
\end{example}


Note that there is no need for general tie-breaking rules. Indeed, different merges may yield different codes, and maybe even different code lengths, but always the same expected length. Similarly, the assignment of $0$ or $1$ does not change the code lengths.



\paragraph{Huffman codes are compact} Clearly, Huffman codes are prefix. The proof that they are compact is by induction on the number of symbols and omitted. It can be found in \cite[Section 5.8]{CT06}.


\paragraph{Non-binary Huffman codes}

Huffman codes can be extended to non-binary alphabets: If we have an alphabet of $D$ characters, we group the $D$ least likely symbols at each stage of reducing the source. When expanding the code we append each of the $D$ characters to one of the least likely symbols’ codewords.

We must end up with exactly $D$ symbols in the final source, so we may need to pad the original source up to $D+k(D-1)$ by adding symbols of probability $0$.

\subsection{See further}

\paragraph{Codes and Automata} The mathematical theory of uniquely decodable codes is reviewed in \cite{BPR10}, where they are simply referred to as codes. The language generated by a prefix code can be recognised by very a simple deterministic finite automaton; in fact, the relation between codes and automata is very deep and explored throughout the book. Note that this book hardly talks about data compression!

\paragraph{Canonical Huffman codes} As we shall see in Exercise \ref{exercise:huffman_unequal_lengths}, there can be several different Huffman trees for the same source. However, there is always a so-called canonical Huffman tree (and hence code) with a special shape that can be easily computed; see \cite[3.2.2]{Say12}. We shall encounter a similar idea in Lecture \ref{sec:35}.

\paragraph{Adaptive Huffman coding} Huffman coding is based on a source $X$ with given probabilities. In general, the probability of an element is computed by its relative frequency in the message; for instance, if the message has 100 characters, 34 of them are ``e'', then the probability of ``e'' is 34\%. Computing those probabilities then requires scanning the whole document before building the tree. Adaptive Huffman coding, on the other hand, builds the Huffman tree as the document is scanned, making small updates (if any) each time a new character is scanned.

\subsection{Exercises}

\begin{exercise} \label{exercise:not_prefix}
Let $\mathcal{X} = \{x_1, \dots, x_q\}$ for $q \ge 2$. Give a binary code $C : \mathcal{X} \to \{0,1\}^*$ that is uniquely decodable but neither prefix nor suffix.
\end{exercise}

\begin{exercise} \label{exercise:prefix}
Finish the example of decoding a prefix code.
\end{exercise}

\begin{exercise} \label{exercise:decoding_prefix}
How could you make the decoding algorithm of prefix codes more efficient? Would you use that modification for decoding Huffman codes? How would you include the decision problem: given $w \in \mathcal{D}^*$, determine whether $w$ is a codeword.
\end{exercise}

\begin{exercise} \label{exercise:huffman}
Construct a binary Huffman code for $X$ with probabilities $0.5, 0.2, 0.15, 0.1, 0.05$. What is the average length, and how does it compare with the one in Example \ref{example:huffman}?
\end{exercise}

\begin{exercise} \label{exercise:huffman_unequal_lengths}
Let $X$ have probabilities $(1/3, 1/3, 1/4, 1/{12})$. Show that, depending on how you merge, the binary Huffman coding procedure may lead to different code lengths, namely $(2,2,2,2)$ or $(1,2,3,3)$. Verify that the average length remains the same, though.
\end{exercise}
